{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Causal graph"
      ],
      "metadata": {
        "id": "uSlJcP_Bm7XP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class CausalGraph:\n",
        "    def __init__(self, V, path=[], unobserved_edges=[]):\n",
        "        self.v = list(V)\n",
        "        self.set_v = set(V)\n",
        "        self.fn = {v: set() for v in V}  # First neighborhood\n",
        "        self.sn = {v: set() for v in V}  # Second neighborhood\n",
        "        self.on = {v: set() for v in V}  # Out of neighborhood\n",
        "        self.p = set(map(tuple, map(sorted, path)))  # Path to First neighborhood\n",
        "        self.ue = set(map(tuple, map(sorted, unobserved_edges)))  # Unobserved edges\n",
        "\n",
        "        for v1, v2 in path:\n",
        "            self.fn[v1].add(v2)\n",
        "            self.fn[v2].add(v1)\n",
        "            self.p.add(tuple(sorted((v1, v2))))\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.v)\n",
        "\n",
        "    def categorize_neighbors(self,target_node):\n",
        "        # centrality = {v: len(self.fn[v]) for v in self.v}\n",
        "        # target_node = max(centrality, key=centrality.get)\n",
        "        if target_node not in self.set_v:\n",
        "            return\n",
        "\n",
        "        one_hop_neighbors = self.fn[target_node]\n",
        "        two_hop_neighbors = set()\n",
        "\n",
        "        for neighbor in one_hop_neighbors:\n",
        "            two_hop_neighbors |= self.fn[neighbor]\n",
        "\n",
        "        two_hop_neighbors -= one_hop_neighbors\n",
        "        two_hop_neighbors.discard(target_node)\n",
        "        out_of_neighborhood = self.set_v - (one_hop_neighbors | two_hop_neighbors | {target_node})\n",
        "\n",
        "        self.sn[target_node] = two_hop_neighbors\n",
        "        self.on[target_node] = out_of_neighborhood\n",
        "        return target_node, one_hop_neighbors, two_hop_neighbors, out_of_neighborhood\n",
        "\n",
        "    def plot(self):\n",
        "        G = nx.Graph()\n",
        "        G.add_nodes_from(self.v)\n",
        "        G.add_edges_from(self.p)\n",
        "\n",
        "        pos = nx.spring_layout(G)\n",
        "        nx.draw(G, pos, with_labels=True, node_size=200, font_size=10, font_weight='bold', node_color=\"lightblue\", edge_color=\"grey\")\n",
        "        plt.savefig('causal.png')\n",
        "        plt.show()\n",
        "\n",
        "    def sort(self):\n",
        "        sorted_nodes = sorted(list(self.set_v))\n",
        "        return sorted_nodes\n",
        "\n",
        "    def graph_search(self,cg, v1, v2=None, edge_type=\"path\",target_node = None):\n",
        "        assert edge_type in [\"path\", \"unobserved\"]\n",
        "        assert v1 in cg.set_v\n",
        "        assert v2 in cg.set_v or v2 is None\n",
        "\n",
        "        target, one_hop_neighbors, two_hop_neighbors, out_of_neighborhood = cg.categorize_neighbors(target_node)\n",
        "\n",
        "        q = deque([v1])\n",
        "        seen = {v1}\n",
        "        while len(q) > 0:\n",
        "            cur = q.popleft()\n",
        "            cur_fn = cg.fn[cur]\n",
        "            cur_sn = cg.sn[target_node]\n",
        "            cur_on = cg.on[target_node]\n",
        "\n",
        "            cur_neighbors = cur_fn if edge_type == \"path\" else (cur_sn | cur_on)\n",
        "\n",
        "            for neighbor in cur_neighbors:\n",
        "                if neighbor not in seen:\n",
        "                    if v2 is not None:\n",
        "                        if (neighbor == v2 and edge_type == \"path\" and neighbor in one_hop_neighbors) or \\\n",
        "                                (neighbor == v2 and edge_type == \"unobserved\" and neighbor in (\n",
        "                                        two_hop_neighbors | out_of_neighborhood)):\n",
        "                            return True\n",
        "                    seen.add(neighbor)\n",
        "                    q.append(neighbor)\n",
        "\n",
        "        if v2 is None:\n",
        "            return seen\n",
        "\n",
        "        return False\n",
        "\n",
        "    def generate_binary_values(self, cg, num_samples):\n",
        "        dataset = []\n",
        "        for _ in range(num_samples):\n",
        "            binary_values = {node: random.choice([0, 1]) for node in cg}\n",
        "            dataset.append(binary_values)\n",
        "        return dataset\n",
        "\n",
        "    def calculate_probabilities(self, dataset):\n",
        "        node_counts = {node: 0 for node in self.v}\n",
        "        total_samples = len(dataset)\n",
        "\n",
        "        for i in dataset:\n",
        "            for node, value in i.items():\n",
        "                if value == 1:\n",
        "                    node_counts[node] += 1\n",
        "\n",
        "        node_probabilities = {node: count / total_samples for node, count in node_counts.items()}\n",
        "        return node_probabilities\n",
        "\n",
        "    def calculate_joint_probabilities(self, dataset):\n",
        "        joint_counts = {(node_i, node_j): 0 for node_i in self.v for node_j in self.v if node_i != node_j}\n",
        "        total_samples = len(dataset)\n",
        "\n",
        "        for sample in dataset:\n",
        "            for node_i, node_j in joint_counts.keys():\n",
        "                if sample[node_i] == 1 and sample[node_j] == 1:\n",
        "                    joint_counts[(node_i, node_j)] += 1\n",
        "\n",
        "        joint_probabilities = {}\n",
        "        min_prob = 1  # initialize the min_prob to 1\n",
        "\n",
        "        # First, calculate the probabilities for the existing links\n",
        "        for (node_i, node_j), count in joint_counts.items():\n",
        "            if (node_i, node_j) in self.p or (node_j, node_i) in self.p:\n",
        "                prob = count / total_samples\n",
        "                joint_probabilities[(node_i, node_j)] = prob\n",
        "                joint_probabilities[(node_j, node_i)] = prob  # update for bidirectional link\n",
        "                if prob < min_prob:\n",
        "                    min_prob = prob  # update the minimum probability\n",
        "\n",
        "        # Now, calculate the probabilities for the non-existing links using the Gumbel distribution\n",
        "        for (node_i, node_j), count in joint_counts.items():\n",
        "            if (node_i, node_j) not in self.p and (node_j, node_i) not in self.p:\n",
        "                # generate a random value from a Gumbel distribution\n",
        "                gumbel_noise = np.random.gumbel()\n",
        "                # rescale the gumbel noise to be in [0, min_prob)\n",
        "                # scaled_gumbel_noise = min_prob * (gumbel_noise - np.min(gumbel_noise)) / (np.max(gumbel_noise) - np.min(gumbel_noise))\n",
        "                joint_probabilities[(node_i, node_j)] = gumbel_noise\n",
        "                joint_probabilities[(node_j, node_i)] = gumbel_noise  # update for bidirectional link\n",
        "\n",
        "        return joint_probabilities\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    cg = CausalGraph(['A', 'B', 'C', 'D'], [('A', 'B'), ('A', 'C'), ('B', 'D')])\n",
        "    datasets = cg.generate_binary_values(cg,num_samples=10)\n",
        "    p_v = cg.calculate_probabilities(datasets)\n",
        "    print(datasets)\n",
        "    p_v_joint = cg.calculate_joint_probabilities(datasets)\n",
        "    print(p_v)\n",
        "    print(p_v_joint)\n",
        "    print(cg.p)\n",
        "\n",
        "    target_node, one_hop_neighbors, two_hop_neighbors, out_of_neighborhood = cg.categorize_neighbors(target_node = cg.sort()[0])\n",
        "    print(f\"Target node: {target_node}\")\n",
        "    print(f\"1-hop neighbors of A: {one_hop_neighbors}\")\n",
        "    print(f\"2-hop neighbors of A: {two_hop_neighbors}\")\n",
        "    print(f\"Out of neighborhood of A: {out_of_neighborhood}\")\n",
        "    # Example usage of graph_search\n",
        "    result1 = cg.graph_search(cg, 'A', 'D', edge_type=\"path\",target_node = 'A')\n",
        "    print(f\"Is there a path from A to D? {result1}\")\n",
        "    result2 = cg.graph_search(cg, 'A', 'D', edge_type=\"unobserved\",target_node = 'A')\n",
        "    print(f\"Can there be an unobserved path from A to D? {result2}\")\n",
        "    result3 = cg.graph_search(cg, 'A', edge_type=\"path\",target_node = 'A')\n",
        "    print(f\"All nodes reachable from A via paths: {result3}\")\n",
        "    cg.plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 736
        },
        "id": "vxp7ciGUm8E3",
        "outputId": "01f56a64-6a28-4533-ed43-5d5e3c403c57"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'A': 1, 'B': 1, 'C': 1, 'D': 1}, {'A': 0, 'B': 0, 'C': 1, 'D': 1}, {'A': 0, 'B': 1, 'C': 1, 'D': 0}, {'A': 1, 'B': 1, 'C': 1, 'D': 1}, {'A': 1, 'B': 1, 'C': 0, 'D': 1}, {'A': 1, 'B': 1, 'C': 0, 'D': 1}, {'A': 1, 'B': 1, 'C': 0, 'D': 0}, {'A': 1, 'B': 0, 'C': 0, 'D': 0}, {'A': 1, 'B': 1, 'C': 0, 'D': 1}, {'A': 0, 'B': 0, 'C': 0, 'D': 1}]\n",
            "{'A': 0.7, 'B': 0.7, 'C': 0.4, 'D': 0.7}\n",
            "{('A', 'B'): 0.6, ('B', 'A'): 0.6, ('A', 'C'): 0.2, ('C', 'A'): 0.2, ('B', 'D'): 0.5, ('D', 'B'): 0.5, ('A', 'D'): 1.444523479018019, ('D', 'A'): 1.444523479018019, ('B', 'C'): 0.38319999220088613, ('C', 'B'): 0.38319999220088613, ('C', 'D'): 2.699330310025533, ('D', 'C'): 2.699330310025533}\n",
            "{('B', 'D'), ('A', 'C'), ('A', 'B')}\n",
            "Target node: A\n",
            "1-hop neighbors of A: {'B', 'C'}\n",
            "2-hop neighbors of A: {'D'}\n",
            "Out of neighborhood of A: set()\n",
            "Is there a path from A to D? False\n",
            "Can there be an unobserved path from A to D? True\n",
            "All nodes reachable from A via paths: {'D', 'B', 'A', 'C'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm 1"
      ],
      "metadata": {
        "id": "DJd_Oa2FnEuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Uniform, Gumbel, Bernoulli\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import product\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "class NNModel(nn.Module):\n",
        "    def __init__(self, u, input_size, output_size, h_size, h_layers):\n",
        "        super(NNModel, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.h_size = h_size\n",
        "        self.h_layers = h_layers\n",
        "        self.u = u\n",
        "        layers = [nn.Linear(self.input_size, self.h_size)]\n",
        "        for l in range(h_layers - 1):\n",
        "            layers.append(nn.Linear(self.h_size, self.h_size))\n",
        "            layers.append(nn.ReLU())\n",
        "        layers.append(nn.Linear(self.h_size, self.output_size))\n",
        "        self.nn = nn.Sequential(*layers)\n",
        "        self.nn.apply(self.init_weights)\n",
        "\n",
        "    def init_weights(self, m):\n",
        "        if type(m) == nn.Linear:\n",
        "            torch.nn.init.xavier_normal_(m.weight, gain=torch.nn.init.calculate_gain('relu'))\n",
        "\n",
        "    def nn_forward(self):\n",
        "        out = self.nn(self.u)\n",
        "        return torch.sigmoid(out)\n",
        "\n",
        "class NCM:\n",
        "    def __init__(self, graph, target_node, lambda_reg, learning_rate,h_size, h_layers):\n",
        "        self.graph = graph\n",
        "        self.h_size = h_size\n",
        "        self.h_layers = h_layers\n",
        "        self.lambda_reg = lambda_reg\n",
        "        self.learning_rate = learning_rate\n",
        "        self.target_node = target_node\n",
        "        self.states = {graph.target_node: Bernoulli(0.5).sample((1,))}\n",
        "        self.u_i = {v: Uniform(0, 1).sample((1,)) for v in graph.one_hop_neighbors | graph.two_hop_neighbors}\n",
        "        self.u_ij = {v: Uniform(0, 1).sample((1,)) for v in graph.one_hop_neighbors}\n",
        "        self.u_do = {v: Uniform(0, 1).sample((1,)) for v in graph.one_hop_neighbors}\n",
        "        self.u = torch.cat(list(self.states.values()) + list(self.u_i.values()) + list(self.u_ij.values()), dim=0)\n",
        "        self.model = NNModel(u = self.u, input_size=len(self.u), output_size=1,h_size = self.h_size, h_layers = self.h_layers)\n",
        "\n",
        "    def ncm_forward(self):\n",
        "        G_i = Gumbel(loc=torch.tensor([0.0]), scale=torch.tensor([1.0])).sample((1,))\n",
        "        f = self.model.nn_forward()\n",
        "        # print(self.states)\n",
        "        if len(self.u_ij) > 0:\n",
        "            return G_i + torch.log(f)\n",
        "        else:\n",
        "            return G_i + torch.log(1 - f)\n",
        "\n",
        "def train(cg,lambdas,learning_rate,h_size,h_layers,num_epochs):\n",
        "    dir_path = \"./model\"\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "    for i in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        ncm_models = {}  # This will store each NCM model\n",
        "        # print('epoch : ', i)\n",
        "        sum_f = 0\n",
        "        for node in cg.set_v:\n",
        "            # Instantiate NCM for each node inside the loop\n",
        "            cg.target_node, cg.one_hop_neighbors, cg.two_hop_neighbors, cg.out_of_neighborhood = cg.categorize_neighbors(target_node=node)\n",
        "            ncm = NCM(cg, cg.target_node,lambda_reg=lambdas, learning_rate=learning_rate, h_size=h_size, h_layers=h_layers)\n",
        "            ncm_models[node] = ncm  # Save the NCM model with the node as the key\n",
        "            f = ncm.ncm_forward()\n",
        "            torch.save(ncm.model.state_dict(), os.path.join(dir_path, f'model_node{node}_epoch{i}.pth'))\n",
        "            sum_f += torch.sum(torch.abs(f))\n",
        "        # print('The sum_f is : ',sum_f)\n",
        "        p_L1 = sum_f / len(cg.set_v)\n",
        "        p_L2 = torch.abs(torch.prod(p_L1) / len(cg.set_v))\n",
        "        optimizer = optim.Adam(ncm.model.parameters(), lr=ncm.learning_rate)\n",
        "        optimizer.zero_grad()\n",
        "        loss = ((1 / len(cg.set_v)) * -torch.log(torch.sum(p_L1))) - (lambdas * torch.log(torch.sum(p_L2)))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        # total_loss = [x for x in total_loss if not math.isnan(x[0])]\n",
        "    # print(\"The loss value is : \", total_loss, '\\n')\n",
        "    best_ncm_model = ncm_models[node]\n",
        "    return total_loss, best_ncm_model, p_L2\n",
        "\n",
        "def kl_divergence(p, q):\n",
        "    return np.sum(np.where(p != 0, p * np.log(p / q), 0))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    cg = CausalGraph(['A', 'B', 'C', 'D'], [('A', 'B'), ('A', 'C'), ('B', 'D')])\n",
        "    cg.target_node, cg.one_hop_neighbors, cg.two_hop_neighbors, cg.out_of_neighborhood = cg.categorize_neighbors(target_node=cg.sort()[0])\n",
        "    datasets = cg.generate_binary_values(cg, num_samples=10)\n",
        "    p_v = cg.calculate_probabilities(datasets)\n",
        "    p_v_joint = cg.calculate_joint_probabilities(datasets)\n",
        "    print(datasets,'\\n',p_v,'\\n',p_v_joint)\n",
        "\n",
        "    # hyperparameters\n",
        "    num_epochs = 3\n",
        "    learning_rates = [0.001, 0.002, 0.005,0.01]\n",
        "    hidden_sizes = [32, 64, 128, 256]\n",
        "    num_layers = [1, 2, 3, 4]\n",
        "    lambdas = [0.01, 0.05, .1,.2,.3]\n",
        "    hyperparameters = product(learning_rates, hidden_sizes, num_layers, lambdas)\n",
        "    total_loss = []\n",
        "    for i, hyperparams in enumerate(hyperparameters):\n",
        "        learning_rate, h_size, h_layers, lambdas = hyperparams\n",
        "        print(f'Training with learning rate: {learning_rate}, h_size: {h_size}, h_layers: {h_layers}, lambdas: {lambdas}')\n",
        "        causal_loss,best_ncm_model,p_do = train(cg, lambdas, learning_rate, h_size, h_layers, num_epochs)\n",
        "        total_loss.append(causal_loss)\n",
        "    # total_loss = [x for x in total_loss if not math.isnan(x[0])]\n",
        "    print(total_loss, best_ncm_model,p_do)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(total_loss)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss over epochs')\n",
        "    plt.savefig('syn Loss over epochs.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BoIHm_K_nEJC",
        "outputId": "b2bfad77-13fb-4147-a104-e4f12f33d303"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'A': 1, 'B': 1, 'C': 1, 'D': 1}, {'A': 1, 'B': 0, 'C': 0, 'D': 1}, {'A': 1, 'B': 0, 'C': 1, 'D': 1}, {'A': 1, 'B': 0, 'C': 0, 'D': 1}, {'A': 0, 'B': 0, 'C': 0, 'D': 0}, {'A': 1, 'B': 0, 'C': 1, 'D': 1}, {'A': 0, 'B': 1, 'C': 1, 'D': 0}, {'A': 1, 'B': 0, 'C': 1, 'D': 1}, {'A': 0, 'B': 1, 'C': 1, 'D': 0}, {'A': 1, 'B': 0, 'C': 0, 'D': 0}] \n",
            " {'A': 0.7, 'B': 0.3, 'C': 0.6, 'D': 0.6} \n",
            " {('A', 'B'): 0.1, ('B', 'A'): 0.1, ('A', 'C'): 0.4, ('C', 'A'): 0.4, ('B', 'D'): 0.1, ('D', 'B'): 0.1, ('A', 'D'): -0.9447468287563885, ('D', 'A'): -0.9447468287563885, ('B', 'C'): -1.0669051119047823, ('C', 'B'): -1.0669051119047823, ('C', 'D'): 1.0050893507067864, ('D', 'C'): 1.0050893507067864}\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  -0.0442470982670784 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  0.09572955965995789 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  0.12420724332332611 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  0.3402271568775177 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  0.6608474254608154 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  0.015782710164785385 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  0.2207680493593216 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  0.018177412450313568 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  0.2517596483230591 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  0.4899959862232208 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  0.15085574984550476 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  0.15708041191101074 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  0.13609667122364044 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  0.27449101209640503 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  0.10683426260948181 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  -0.04721047356724739 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  -0.2868037521839142 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  -0.014734365046024323 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  0.3564201593399048 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  0.6292061805725098 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  0.05084466561675072 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  0.16979047656059265 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  -0.001733526587486267 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  0.040168583393096924 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  0.2631765604019165 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  -0.0926295816898346 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  -0.07808884978294373 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  0.25781065225601196 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  0.2472625970840454 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  0.8254290819168091 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  0.0222295131534338 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  0.014941681176424026 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  0.4911176562309265 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  0.3493485450744629 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  0.19917485117912292 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  0.16243530809879303 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  0.0004427880048751831 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  0.125565305352211 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  0.5850650072097778 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  0.6578317284584045 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  -0.09546168148517609 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  -0.03621315583586693 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  0.2949250340461731 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  0.23720824718475342 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  0.29659509658813477 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  -0.11677693575620651 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  0.15552343428134918 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  0.22192806005477905 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  0.10387594252824783 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  0.36802324652671814 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  -0.0030826590955257416 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  0.19858418405056 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  0.2267552614212036 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  0.20743319392204285 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  0.6823264360427856 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  -0.10745342820882797 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  0.02577793225646019 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  -0.09668070822954178 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  0.34636813402175903 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  0.21960237622261047 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  -0.003280656412243843 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  0.046538449823856354 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  0.11672812700271606 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  0.20912444591522217 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  0.19171157479286194 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  -0.045320820063352585 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  0.012070797383785248 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  0.23469071090221405 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  0.08408229053020477 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  0.22507628798484802 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  0.02230558916926384 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  0.2817128300666809 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  0.062361329793930054 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  0.36646217107772827 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  0.3051677942276001 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  -0.0008629262447357178 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  0.11346007883548737 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  0.14294759929180145 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  0.1849212497472763 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  0.06470651924610138 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  -0.013368682004511356 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  0.10590977966785431 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  0.3038102388381958 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  0.0602344274520874 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  0.2220844030380249 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  0.03317919373512268 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  0.06734306365251541 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  0.0923440158367157 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  0.22699284553527832 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  0.4524296522140503 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  -0.07220229506492615 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  -0.049880173057317734 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  0.15028123557567596 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  0.4978766441345215 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  0.37206560373306274 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  0.159795880317688 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  0.24739253520965576 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  0.1790056824684143 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  0.11883334815502167 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  0.5909320712089539 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  -0.20758649706840515 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  -0.16744138300418854 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  -0.04170231521129608 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  -0.014761671423912048 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  -0.02869148552417755 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  0.13605837523937225 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  0.14546237885951996 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  0.37436479330062866 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  0.35630184412002563 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  0.5033289790153503 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  0.16434328258037567 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  -0.15812444686889648 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  0.5079986453056335 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  0.1669972538948059 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  0.23082858324050903 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  -0.006879553198814392 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  0.20019608736038208 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  -0.018267646431922913 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  0.43164682388305664 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  0.19219844043254852 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  -0.09512525796890259 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  -0.05021131783723831 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  -0.05022881180047989 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  0.25298064947128296 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  0.37909403443336487 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  -0.12430430203676224 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  0.18621063232421875 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  0.1448446810245514 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  0.29150575399398804 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  0.12976664304733276 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  -0.07474642246961594 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  0.03449985384941101 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  0.2710380256175995 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  0.6878761053085327 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  0.425955206155777 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  0.28823965787887573 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  0.17809709906578064 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  0.24237242341041565 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  0.31878983974456787 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  0.7881430387496948 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  0.06178014352917671 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  0.09487885236740112 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  0.26061272621154785 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  0.25123924016952515 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  0.49381789565086365 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  -0.05003665015101433 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  -0.07641839981079102 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  0.1633301079273224 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  0.22640639543533325 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  0.4892388880252838 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  0.06775374710559845 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  -0.00725511834025383 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  0.10721758753061295 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  0.3574376106262207 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  0.6735315322875977 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  0.0638321042060852 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  0.20870664715766907 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  0.20257270336151123 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  0.5505064129829407 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  0.25781431794166565 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  -0.07479842752218246 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  0.12720689177513123 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  -0.111696258187294 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  0.311190664768219 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  0.6691038012504578 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  -0.06607045233249664 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  0.06046958640217781 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  -0.1074640229344368 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  0.3197588622570038 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  0.7656866908073425 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  0.17584800720214844 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  0.16527840495109558 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  0.023272983729839325 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  0.38781625032424927 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  0.6458812952041626 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  -0.08459305018186569 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  0.05717794597148895 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  0.17999786138534546 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  1.0566561222076416 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  0.32559776306152344 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  0.12960539758205414 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  0.0005464814603328705 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  -0.08810757845640182 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  0.13972249627113342 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  0.9853598475456238 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  0.17595770955085754 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  0.18446680903434753 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  0.26432526111602783 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  0.5615324974060059 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  0.586392343044281 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  0.10605458170175552 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  -0.061598967760801315 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  0.19050763547420502 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  0.18088725209236145 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  0.16213363409042358 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  0.047369904816150665 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  0.05945947393774986 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  0.3990982174873352 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  0.05981219559907913 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  0.32367414236068726 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  0.07337139546871185 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  -0.019399110227823257 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  -6.373971700668335e-05 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  0.5703707933425903 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  0.007506430149078369 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  0.10888602584600449 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  -0.027304165065288544 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  0.0294601172208786 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  0.08348676562309265 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  0.6170557141304016 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  -0.017242692410945892 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  0.09028009325265884 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  0.4817272424697876 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  0.3427835702896118 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  0.3773961663246155 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  0.13106971979141235 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  0.05436796694993973 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  0.21472470462322235 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  0.001940503716468811 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  0.8219658136367798 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  0.009177131578326225 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  0.08905106037855148 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  0.2818436026573181 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  0.16913630068302155 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  0.5126549005508423 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  -0.053693220019340515 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  0.10383839905261993 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  0.22298699617385864 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  0.5779402852058411 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  0.476951539516449 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  -0.021546117961406708 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  0.3124236464500427 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  0.05061086267232895 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  0.3086808919906616 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  0.46284565329551697 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  0.010568277910351753 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  0.02710539475083351 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  0.16489911079406738 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  -0.05011332035064697 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  0.2460690438747406 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  0.1819436252117157 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  0.11975529789924622 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  0.292761892080307 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  0.3258831799030304 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  0.3860554099082947 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  0.08864503353834152 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  -0.04536457732319832 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  0.23454588651657104 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  0.11583815515041351 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  0.483202189207077 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  0.06013260781764984 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  -0.04878973215818405 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  0.09068488329648972 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  0.191178098320961 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  0.4214717745780945 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  0.0559268556535244 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  0.25201910734176636 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  0.11753800511360168 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  -0.013253331184387207 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  0.333459734916687 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  0.01850106567144394 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  0.09418631345033646 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  0.31910330057144165 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  0.1748916208744049 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  0.23470643162727356 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  -0.01950208470225334 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  0.1191479042172432 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  0.12532807886600494 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  0.5502685308456421 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  0.5924577713012695 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  0.1288779377937317 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  0.19601017236709595 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  0.41612279415130615 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  0.10008323937654495 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  0.34333088994026184 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  0.0870695412158966 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  0.1435769498348236 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  0.23301982879638672 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  0.09482961148023605 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  0.4698714315891266 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  0.00812949426472187 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  0.21722087264060974 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  0.2786293625831604 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  0.5847906470298767 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  0.39938047528266907 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  0.1564699411392212 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  0.06716952472925186 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  0.22596663236618042 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  0.41000932455062866 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  0.16253423690795898 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  0.2797112762928009 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  0.1006869375705719 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  0.0765710324048996 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  0.15571844577789307 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  0.3817319869995117 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  0.0879376232624054 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  0.13772478699684143 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  -0.135337695479393 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  0.20845073461532593 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  0.3410578966140747 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  -0.005200720392167568 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  0.0559045672416687 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  0.05148688703775406 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  0.34534919261932373 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  0.2801336646080017 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  -0.017017539590597153 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  0.12061307579278946 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  0.13937941193580627 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  0.2473362684249878 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  0.11590033769607544 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  -0.06291600316762924 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  0.2234671413898468 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  0.08000455796718597 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  0.13001643121242523 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  0.5369312763214111 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  0.006072602700442076 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  0.05667377635836601 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  0.04014592617750168 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  0.16513080894947052 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  0.3454534411430359 \n",
            "\n",
            "[-0.0442470982670784, 0.09572955965995789, 0.12420724332332611, 0.3402271568775177, 0.6608474254608154, 0.015782710164785385, 0.2207680493593216, 0.018177412450313568, 0.2517596483230591, 0.4899959862232208, 0.15085574984550476, 0.15708041191101074, 0.13609667122364044, 0.27449101209640503, 0.10683426260948181, -0.04721047356724739, -0.2868037521839142, -0.014734365046024323, 0.3564201593399048, 0.6292061805725098, 0.05084466561675072, 0.16979047656059265, -0.001733526587486267, 0.040168583393096924, 0.2631765604019165, -0.0926295816898346, -0.07808884978294373, 0.25781065225601196, 0.2472625970840454, 0.8254290819168091, 0.0222295131534338, 0.014941681176424026, 0.4911176562309265, 0.3493485450744629, 0.19917485117912292, 0.16243530809879303, 0.0004427880048751831, 0.125565305352211, 0.5850650072097778, 0.6578317284584045, -0.09546168148517609, -0.03621315583586693, 0.2949250340461731, 0.23720824718475342, 0.29659509658813477, -0.11677693575620651, 0.15552343428134918, 0.22192806005477905, 0.10387594252824783, 0.36802324652671814, -0.0030826590955257416, 0.19858418405056, 0.2267552614212036, 0.20743319392204285, 0.6823264360427856, -0.10745342820882797, 0.02577793225646019, -0.09668070822954178, 0.34636813402175903, 0.21960237622261047, -0.003280656412243843, 0.046538449823856354, 0.11672812700271606, 0.20912444591522217, 0.19171157479286194, -0.045320820063352585, 0.012070797383785248, 0.23469071090221405, 0.08408229053020477, 0.22507628798484802, 0.02230558916926384, 0.2817128300666809, 0.062361329793930054, 0.36646217107772827, 0.3051677942276001, -0.0008629262447357178, 0.11346007883548737, 0.14294759929180145, 0.1849212497472763, 0.06470651924610138, -0.013368682004511356, 0.10590977966785431, 0.3038102388381958, 0.0602344274520874, 0.2220844030380249, 0.03317919373512268, 0.06734306365251541, 0.0923440158367157, 0.22699284553527832, 0.4524296522140503, -0.07220229506492615, -0.049880173057317734, 0.15028123557567596, 0.4978766441345215, 0.37206560373306274, 0.159795880317688, 0.24739253520965576, 0.1790056824684143, 0.11883334815502167, 0.5909320712089539, -0.20758649706840515, -0.16744138300418854, -0.04170231521129608, -0.014761671423912048, -0.02869148552417755, 0.13605837523937225, 0.14546237885951996, 0.37436479330062866, 0.35630184412002563, 0.5033289790153503, 0.16434328258037567, -0.15812444686889648, 0.5079986453056335, 0.1669972538948059, 0.23082858324050903, -0.006879553198814392, 0.20019608736038208, -0.018267646431922913, 0.43164682388305664, 0.19219844043254852, -0.09512525796890259, -0.05021131783723831, -0.05022881180047989, 0.25298064947128296, 0.37909403443336487, -0.12430430203676224, 0.18621063232421875, 0.1448446810245514, 0.29150575399398804, 0.12976664304733276, -0.07474642246961594, 0.03449985384941101, 0.2710380256175995, 0.6878761053085327, 0.425955206155777, 0.28823965787887573, 0.17809709906578064, 0.24237242341041565, 0.31878983974456787, 0.7881430387496948, 0.06178014352917671, 0.09487885236740112, 0.26061272621154785, 0.25123924016952515, 0.49381789565086365, -0.05003665015101433, -0.07641839981079102, 0.1633301079273224, 0.22640639543533325, 0.4892388880252838, 0.06775374710559845, -0.00725511834025383, 0.10721758753061295, 0.3574376106262207, 0.6735315322875977, 0.0638321042060852, 0.20870664715766907, 0.20257270336151123, 0.5505064129829407, 0.25781431794166565, -0.07479842752218246, 0.12720689177513123, -0.111696258187294, 0.311190664768219, 0.6691038012504578, -0.06607045233249664, 0.06046958640217781, -0.1074640229344368, 0.3197588622570038, 0.7656866908073425, 0.17584800720214844, 0.16527840495109558, 0.023272983729839325, 0.38781625032424927, 0.6458812952041626, -0.08459305018186569, 0.05717794597148895, 0.17999786138534546, 1.0566561222076416, 0.32559776306152344, 0.12960539758205414, 0.0005464814603328705, -0.08810757845640182, 0.13972249627113342, 0.9853598475456238, 0.17595770955085754, 0.18446680903434753, 0.26432526111602783, 0.5615324974060059, 0.586392343044281, 0.10605458170175552, -0.061598967760801315, 0.19050763547420502, 0.18088725209236145, 0.16213363409042358, 0.047369904816150665, 0.05945947393774986, 0.3990982174873352, 0.05981219559907913, 0.32367414236068726, 0.07337139546871185, -0.019399110227823257, -6.373971700668335e-05, 0.5703707933425903, 0.007506430149078369, 0.10888602584600449, -0.027304165065288544, 0.0294601172208786, 0.08348676562309265, 0.6170557141304016, -0.017242692410945892, 0.09028009325265884, 0.4817272424697876, 0.3427835702896118, 0.3773961663246155, 0.13106971979141235, 0.05436796694993973, 0.21472470462322235, 0.001940503716468811, 0.8219658136367798, 0.009177131578326225, 0.08905106037855148, 0.2818436026573181, 0.16913630068302155, 0.5126549005508423, -0.053693220019340515, 0.10383839905261993, 0.22298699617385864, 0.5779402852058411, 0.476951539516449, -0.021546117961406708, 0.3124236464500427, 0.05061086267232895, 0.3086808919906616, 0.46284565329551697, 0.010568277910351753, 0.02710539475083351, 0.16489911079406738, -0.05011332035064697, 0.2460690438747406, 0.1819436252117157, 0.11975529789924622, 0.292761892080307, 0.3258831799030304, 0.3860554099082947, 0.08864503353834152, -0.04536457732319832, 0.23454588651657104, 0.11583815515041351, 0.483202189207077, 0.06013260781764984, -0.04878973215818405, 0.09068488329648972, 0.191178098320961, 0.4214717745780945, 0.0559268556535244, 0.25201910734176636, 0.11753800511360168, -0.013253331184387207, 0.333459734916687, 0.01850106567144394, 0.09418631345033646, 0.31910330057144165, 0.1748916208744049, 0.23470643162727356, -0.01950208470225334, 0.1191479042172432, 0.12532807886600494, 0.5502685308456421, 0.5924577713012695, 0.1288779377937317, 0.19601017236709595, 0.41612279415130615, 0.10008323937654495, 0.34333088994026184, 0.0870695412158966, 0.1435769498348236, 0.23301982879638672, 0.09482961148023605, 0.4698714315891266, 0.00812949426472187, 0.21722087264060974, 0.2786293625831604, 0.5847906470298767, 0.39938047528266907, 0.1564699411392212, 0.06716952472925186, 0.22596663236618042, 0.41000932455062866, 0.16253423690795898, 0.2797112762928009, 0.1006869375705719, 0.0765710324048996, 0.15571844577789307, 0.3817319869995117, 0.0879376232624054, 0.13772478699684143, -0.135337695479393, 0.20845073461532593, 0.3410578966140747, -0.005200720392167568, 0.0559045672416687, 0.05148688703775406, 0.34534919261932373, 0.2801336646080017, -0.017017539590597153, 0.12061307579278946, 0.13937941193580627, 0.2473362684249878, 0.11590033769607544, -0.06291600316762924, 0.2234671413898468, 0.08000455796718597, 0.13001643121242523, 0.5369312763214111, 0.006072602700442076, 0.05667377635836601, 0.04014592617750168, 0.16513080894947052, 0.3454534411430359] <__main__.NCM object at 0x7f97758f5780> tensor(0.2842, grad_fn=<AbsBackward0>)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm 2 & 3"
      ],
      "metadata": {
        "id": "gGJNIY9Xnd-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "from itertools import product\n",
        "\n",
        "def GNNCausalExplanation(dataset, lambdas, learning_rate, h_size, h_layers, num_epochs, delta):\n",
        "    total_loss = []\n",
        "    for graph_id in dataset:\n",
        "        cg = dataset[graph_id]\n",
        "        print(f\"The graph number {graph_id} contains: {cg.set_v}\")\n",
        "        num_epochs = 2\n",
        "        learning_rates = [0.001, 0.002]\n",
        "        hidden_sizes = [64, 128]\n",
        "        num_layers = [2, 3]\n",
        "        lambdas = [0.01, 0.05]\n",
        "        hyperparameters = product(learning_rates, hidden_sizes, num_layers, lambdas)\n",
        "        for i, hyperparams in enumerate(hyperparameters):\n",
        "            learning_rate, h_size, h_layers, lambdas1 = hyperparams\n",
        "            total_loss = []\n",
        "            min_loss = float('inf')\n",
        "            best_hyperparams = None\n",
        "            causal_loss, best_ncm_model, p_do = train(cg, lambdas1, learning_rate, h_size, h_layers, num_epochs)\n",
        "            total_loss.append(causal_loss)\n",
        "            if causal_loss < min_loss:\n",
        "                best_model = best_ncm_model\n",
        "                best_intervention = p_do\n",
        "                min_loss = causal_loss\n",
        "                best_hyperparams = hyperparams\n",
        "        print('The minimum loss obtained is:', min_loss)\n",
        "        print('The best hyperparameters are:',f'Training with learning rate: {best_hyperparams[0]}, h_size: {best_hyperparams[1]}, h_layers: {best_hyperparams[2]}, lambdas: {best_hyperparams[3]}')\n",
        "        print('The best model is: ', best_model, 'and its information value is: ', best_intervention.data)\n",
        "        print(f\"Target node: {best_model.graph.target_node}\")\n",
        "        print(f\"1-hop neighbors of A: {best_model.graph.one_hop_neighbors}\")\n",
        "        print(f\"2-hop neighbors of A: {best_model.graph.two_hop_neighbors}\")\n",
        "        print(f\"Out of neighborhood of A: {best_model.graph.out_of_neighborhood}\",'\\n')\n",
        "        total_loss.append(causal_loss)\n",
        "    print(total_loss)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start_time1 = time.time()\n",
        "    Mutagenicity = \"/content/drive/MyDrive/data/Mutagenicity/\"\n",
        "    Mutagenicity_df = pd.read_csv(Mutagenicity + 'Mutagenicity_A.txt', sep=',', header=None, names=['from', 'to'])\n",
        "    Mutagenicity_graph_indicator = pd.read_csv(Mutagenicity + 'Mutagenicity_graph_indicator.txt', header=None,names=['graph_id'])\n",
        "    Mutagenicity_node_labels = pd.read_csv(Mutagenicity + 'Mutagenicity_node_labels.txt', header=None,names=['node_label'])\n",
        "    Mutagenicity_df['graph_id'] = Mutagenicity_graph_indicator\n",
        "    Mutagenicity_df['node_label'] = Mutagenicity_node_labels\n",
        "    grouped = Mutagenicity_df.groupby('graph_id')\n",
        "    Mutagenicity_causal_graphs = {}\n",
        "    for graph_id, group in grouped:\n",
        "        V = set(group['from']).union(set(group['to']))\n",
        "        edges = list(zip(group['from'], group['to'])) + list(zip(group['to'], group['from']))\n",
        "        Mutagenicity_causal_graphs[graph_id] = CausalGraph(V=V, path=edges)\n",
        "    end_time1 = time.time()\n",
        "    t1 = end_time1 - start_time1\n",
        "    print('The time of the data preprocessing was : ', t1)\n",
        "\n",
        "    num_subgraph_limit = 5\n",
        "    delta = 0.01\n",
        "    num_nodes_density = 5\n",
        "    start_time2 = time.time()\n",
        "    GNNCausalExplanation(dataset= Mutagenicity_causal_graphs, lambdas = 0.1, learning_rate = 0.001, h_size = 128, h_layers = 2, num_epochs = 2, delta=0.01)\n",
        "    end_time2 = time.time()\n",
        "    t2 = end_time2 - start_time2\n",
        "    print('The time of the calculation was : ',t2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxqOWfkQnNY0",
        "outputId": "f94c5447-27e7-4f17-8a19-24b359822cd4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\n",
            "The loss value is :  0.026450693607330322 \n",
            "\n",
            "The loss value is :  0.12842172384262085 \n",
            "\n",
            "The loss value is :  0.02876713126897812 \n",
            "\n",
            "The loss value is :  0.14828886091709137 \n",
            "\n",
            "The loss value is :  0.03294714540243149 \n",
            "\n",
            "The loss value is :  0.1370050609111786 \n",
            "\n",
            "The loss value is :  0.03437519446015358 \n",
            "\n",
            "The loss value is :  0.15022368729114532 \n",
            "\n",
            "The loss value is :  0.03129970654845238 \n",
            "\n",
            "The loss value is :  0.15444229543209076 \n",
            "\n",
            "The loss value is :  0.02898886427283287 \n",
            "\n",
            "The loss value is :  0.18934127688407898 \n",
            "\n",
            "The loss value is :  0.006326260045170784 \n",
            "\n",
            "The loss value is :  0.17015640437602997 \n",
            "\n",
            "The minimum loss obtained is: 0.17015640437602997\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b542e00> and its information value is:  tensor(0.0386)\n",
            "Target node: 63167\n",
            "1-hop neighbors of A: {63149}\n",
            "2-hop neighbors of A: {63166}\n",
            "Out of neighborhood of A: {63168, 63169, 63170, 63171, 63172, 63173, 63174, 63175, 63176, 63177, 63178, 63179, 63180, 63144, 63145, 63150, 63154, 63164, 63165} \n",
            "\n",
            "The graph number 4217.0 contains: {63175, 63176, 63178, 63179, 63180, 63181, 63182, 63183, 63184, 63185}\n",
            "The loss value is :  0.041460633277893066 \n",
            "\n",
            "The loss value is :  0.07927549630403519 \n",
            "\n",
            "The loss value is :  0.026088103652000427 \n",
            "\n",
            "The loss value is :  0.21528780460357666 \n",
            "\n",
            "The loss value is :  0.0017734970897436142 \n",
            "\n",
            "The loss value is :  0.10247880220413208 \n",
            "\n",
            "The loss value is :  0.0020469874143600464 \n",
            "\n",
            "The loss value is :  0.12321525067090988 \n",
            "\n",
            "The loss value is :  0.03848738595843315 \n",
            "\n",
            "The loss value is :  0.10846251994371414 \n",
            "\n",
            "The loss value is :  0.07109728455543518 \n",
            "\n",
            "The loss value is :  0.18271860480308533 \n",
            "\n",
            "The loss value is :  0.06628923118114471 \n",
            "\n",
            "The loss value is :  0.09322050958871841 \n",
            "\n",
            "The loss value is :  0.06398901343345642 \n",
            "\n",
            "The loss value is :  0.16621771454811096 \n",
            "\n",
            "The minimum loss obtained is: 0.16621771454811096\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b543010> and its information value is:  tensor(0.0711)\n",
            "Target node: 63185\n",
            "1-hop neighbors of A: {63176}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {63175, 63178, 63179, 63180, 63181, 63182, 63183, 63184} \n",
            "\n",
            "The graph number 4218.0 contains: {63178, 63179, 63180, 63182, 63186, 63187, 63188, 63189, 63190, 63191, 63192, 63193, 63194, 63195, 63196, 63197, 63198}\n",
            "The loss value is :  0.004168888553977013 \n",
            "\n",
            "The loss value is :  0.1414172202348709 \n",
            "\n",
            "The loss value is :  0.02254808507859707 \n",
            "\n",
            "The loss value is :  0.12576059997081757 \n",
            "\n",
            "The loss value is :  0.030129211023449898 \n",
            "\n",
            "The loss value is :  0.12593036890029907 \n",
            "\n",
            "The loss value is :  0.014059649780392647 \n",
            "\n",
            "The loss value is :  0.13981246948242188 \n",
            "\n",
            "The loss value is :  0.01820741593837738 \n",
            "\n",
            "The loss value is :  0.16361893713474274 \n",
            "\n",
            "The loss value is :  0.034587401896715164 \n",
            "\n",
            "The loss value is :  0.13285061717033386 \n",
            "\n",
            "The loss value is :  0.01655113324522972 \n",
            "\n",
            "The loss value is :  0.13221628963947296 \n",
            "\n",
            "The loss value is :  0.027286328375339508 \n",
            "\n",
            "The loss value is :  0.13478559255599976 \n",
            "\n",
            "The minimum loss obtained is: 0.13478559255599976\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b542620> and its information value is:  tensor(0.0627)\n",
            "Target node: 63198\n",
            "1-hop neighbors of A: {63193}\n",
            "2-hop neighbors of A: {63190}\n",
            "Out of neighborhood of A: {63178, 63179, 63180, 63182, 63186, 63187, 63188, 63189, 63191, 63192, 63194, 63195, 63196, 63197} \n",
            "\n",
            "The graph number 4219.0 contains: {63200, 63201, 63202, 63203, 63204, 63205, 63193, 63194, 63195, 63196, 63199}\n",
            "The loss value is :  -0.015444761142134666 \n",
            "\n",
            "The loss value is :  0.07396402955055237 \n",
            "\n",
            "The loss value is :  0.016961926594376564 \n",
            "\n",
            "The loss value is :  0.10783189535140991 \n",
            "\n",
            "The loss value is :  -0.06075045466423035 \n",
            "\n",
            "The loss value is :  0.08771920949220657 \n",
            "\n",
            "The loss value is :  0.009227622300386429 \n",
            "\n",
            "The loss value is :  0.10460284352302551 \n",
            "\n",
            "The loss value is :  0.0358906164765358 \n",
            "\n",
            "The loss value is :  0.101080983877182 \n",
            "\n",
            "The loss value is :  -0.011470865458250046 \n",
            "\n",
            "The loss value is :  0.1932375729084015 \n",
            "\n",
            "The loss value is :  0.028722336515784264 \n",
            "\n",
            "The loss value is :  0.0988859161734581 \n",
            "\n",
            "The loss value is :  0.016534417867660522 \n",
            "\n",
            "The loss value is :  0.12762188911437988 \n",
            "\n",
            "The minimum loss obtained is: 0.12762188911437988\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b542530> and its information value is:  tensor(0.0861)\n",
            "Target node: 63199\n",
            "1-hop neighbors of A: {63193}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {63200, 63201, 63202, 63203, 63204, 63205, 63194, 63195, 63196} \n",
            "\n",
            "The graph number 4220.0 contains: {63200, 63202, 63203, 63206, 63207, 63208, 63209, 63210, 63211, 63212, 63213, 63196, 63198, 63199}\n",
            "The loss value is :  -0.011464972048997879 \n",
            "\n",
            "The loss value is :  0.10733682662248611 \n",
            "\n",
            "The loss value is :  0.030393242835998535 \n",
            "\n",
            "The loss value is :  0.10821224004030228 \n",
            "\n",
            "The loss value is :  0.0364588238298893 \n",
            "\n",
            "The loss value is :  0.1620332896709442 \n",
            "\n",
            "The loss value is :  0.045963313430547714 \n",
            "\n",
            "The loss value is :  0.10153047740459442 \n",
            "\n",
            "The loss value is :  0.020006991922855377 \n",
            "\n",
            "The loss value is :  0.14771611988544464 \n",
            "\n",
            "The loss value is :  0.03072025068104267 \n",
            "\n",
            "The loss value is :  0.13270168006420135 \n",
            "\n",
            "The loss value is :  0.0505751296877861 \n",
            "\n",
            "The loss value is :  0.1594327688217163 \n",
            "\n",
            "The loss value is :  -0.009916841983795166 \n",
            "\n",
            "The loss value is :  0.08573903143405914 \n",
            "\n",
            "The minimum loss obtained is: 0.08573903143405914\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b543cd0> and its information value is:  tensor(0.1045)\n",
            "Target node: 63199\n",
            "1-hop neighbors of A: {63207}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {63200, 63202, 63203, 63206, 63208, 63209, 63210, 63211, 63212, 63213, 63196, 63198} \n",
            "\n",
            "The graph number 4221.0 contains: {63211, 63212, 63213, 63214, 63215, 63216, 63217, 63218, 63219, 63220, 63221, 63222}\n",
            "The loss value is :  0.008352449163794518 \n",
            "\n",
            "The loss value is :  0.10069844126701355 \n",
            "\n",
            "The loss value is :  0.02751747891306877 \n",
            "\n",
            "The loss value is :  0.0691881775856018 \n",
            "\n",
            "The loss value is :  0.028186863288283348 \n",
            "\n",
            "The loss value is :  0.1334259957075119 \n",
            "\n",
            "The loss value is :  0.02733772248029709 \n",
            "\n",
            "The loss value is :  0.13314120471477509 \n",
            "\n",
            "The loss value is :  -0.030427874997258186 \n",
            "\n",
            "The loss value is :  0.10594974458217621 \n",
            "\n",
            "The loss value is :  0.06960952281951904 \n",
            "\n",
            "The loss value is :  0.11818058043718338 \n",
            "\n",
            "The loss value is :  0.056198686361312866 \n",
            "\n",
            "The loss value is :  0.06095636636018753 \n",
            "\n",
            "The loss value is :  0.023441608995199203 \n",
            "\n",
            "The loss value is :  0.11999627947807312 \n",
            "\n",
            "The minimum loss obtained is: 0.11999627947807312\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b542620> and its information value is:  tensor(0.0860)\n",
            "Target node: 63222\n",
            "1-hop neighbors of A: {63215}\n",
            "2-hop neighbors of A: {63211}\n",
            "Out of neighborhood of A: {63212, 63213, 63214, 63216, 63217, 63218, 63219, 63220, 63221} \n",
            "\n",
            "The graph number 4222.0 contains: {63216, 63217, 63218, 63220, 63222, 63223, 63224, 63225, 63226, 63227, 63228, 63229}\n",
            "The loss value is :  0.01745610684156418 \n",
            "\n",
            "The loss value is :  0.1638486385345459 \n",
            "\n",
            "The loss value is :  0.022382058203220367 \n",
            "\n",
            "The loss value is :  0.09058146923780441 \n",
            "\n",
            "The loss value is :  0.014783257618546486 \n",
            "\n",
            "The loss value is :  0.08370769023895264 \n",
            "\n",
            "The loss value is :  0.01026662066578865 \n",
            "\n",
            "The loss value is :  0.16066551208496094 \n",
            "\n",
            "The loss value is :  0.03460565209388733 \n",
            "\n",
            "The loss value is :  0.07982788980007172 \n",
            "\n",
            "The loss value is :  0.004121892154216766 \n",
            "\n",
            "The loss value is :  0.14876030385494232 \n",
            "\n",
            "The loss value is :  0.0017955750226974487 \n",
            "\n",
            "The loss value is :  0.09239707887172699 \n",
            "\n",
            "The loss value is :  0.011083552613854408 \n",
            "\n",
            "The loss value is :  0.12956014275550842 \n",
            "\n",
            "The minimum loss obtained is: 0.12956014275550842\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b5437c0> and its information value is:  tensor(0.0801)\n",
            "Target node: 63229\n",
            "1-hop neighbors of A: {63226}\n",
            "2-hop neighbors of A: {63220}\n",
            "Out of neighborhood of A: {63216, 63217, 63218, 63222, 63223, 63224, 63225, 63227, 63228} \n",
            "\n",
            "The graph number 4223.0 contains: {63232, 63215, 63217, 63219, 63224, 63227, 63228, 63229, 63230, 63231}\n",
            "The loss value is :  0.030736366286873817 \n",
            "\n",
            "The loss value is :  0.014545895159244537 \n",
            "\n",
            "The loss value is :  -0.007796593010425568 \n",
            "\n",
            "The loss value is :  0.06950654834508896 \n",
            "\n",
            "The loss value is :  0.02465248294174671 \n",
            "\n",
            "The loss value is :  0.08848223090171814 \n",
            "\n",
            "The loss value is :  0.052707307040691376 \n",
            "\n",
            "The loss value is :  0.22447717189788818 \n",
            "\n",
            "The loss value is :  0.007469577714800835 \n",
            "\n",
            "The loss value is :  0.05597294121980667 \n",
            "\n",
            "The loss value is :  0.027867741882801056 \n",
            "\n",
            "The loss value is :  0.11147551983594894 \n",
            "\n",
            "The loss value is :  0.03868487477302551 \n",
            "\n",
            "The loss value is :  0.10828730463981628 \n",
            "\n",
            "The loss value is :  -0.00034597888588905334 \n",
            "\n",
            "The loss value is :  0.036931660026311874 \n",
            "\n",
            "The minimum loss obtained is: 0.036931660026311874\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b541150> and its information value is:  tensor(0.1684)\n",
            "Target node: 63231\n",
            "1-hop neighbors of A: {63217}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {63232, 63215, 63219, 63224, 63227, 63228, 63229, 63230} \n",
            "\n",
            "The graph number 4224.0 contains: {63232, 63233, 63234, 63235, 63219, 63221}\n",
            "The loss value is :  0.07417165488004684 \n",
            "\n",
            "The loss value is :  0.061959173530340195 \n",
            "\n",
            "The loss value is :  0.06357745826244354 \n",
            "\n",
            "The loss value is :  0.1010754182934761 \n",
            "\n",
            "The loss value is :  -0.004731800407171249 \n",
            "\n",
            "The loss value is :  0.1803920865058899 \n",
            "\n",
            "The loss value is :  0.03992188721895218 \n",
            "\n",
            "The loss value is :  0.05982821434736252 \n",
            "\n",
            "The loss value is :  0.023432031273841858 \n",
            "\n",
            "The loss value is :  0.056008659303188324 \n",
            "\n",
            "The loss value is :  -0.02936599776148796 \n",
            "\n",
            "The loss value is :  0.06960107386112213 \n",
            "\n",
            "The loss value is :  -0.024512112140655518 \n",
            "\n",
            "The loss value is :  0.19749689102172852 \n",
            "\n",
            "The loss value is :  0.03065006248652935 \n",
            "\n",
            "The loss value is :  0.0024536848068237305 \n",
            "\n",
            "The minimum loss obtained is: 0.0024536848068237305\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b541cf0> and its information value is:  tensor(0.2492)\n",
            "Target node: 63221\n",
            "1-hop neighbors of A: {63235}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {63232, 63233, 63234, 63219} \n",
            "\n",
            "The graph number 4225.0 contains: {63236, 63237, 63238, 63239, 63240, 63241, 63242, 63243, 63244, 63245, 63246, 63247, 63248, 63249, 63250, 63251, 63252, 63253, 63254, 63255, 63256, 63257, 63258, 63259, 63260, 63261, 63262, 63263, 63264, 63221, 63222, 63223, 63224, 63225, 63226, 63227, 63228, 63229}\n",
            "The loss value is :  0.029497826471924782 \n",
            "\n",
            "The loss value is :  0.18019303679466248 \n",
            "\n",
            "The loss value is :  0.04414154961705208 \n",
            "\n",
            "The loss value is :  0.1751360148191452 \n",
            "\n",
            "The loss value is :  0.030330782756209373 \n",
            "\n",
            "The loss value is :  0.183050274848938 \n",
            "\n",
            "The loss value is :  0.026740409433841705 \n",
            "\n",
            "The loss value is :  0.1800905466079712 \n",
            "\n",
            "The loss value is :  0.03410414233803749 \n",
            "\n",
            "The loss value is :  0.18050962686538696 \n",
            "\n",
            "The loss value is :  0.03810261934995651 \n",
            "\n",
            "The loss value is :  0.17891673743724823 \n",
            "\n",
            "The loss value is :  0.033794037997722626 \n",
            "\n",
            "The loss value is :  0.1817772090435028 \n",
            "\n",
            "The loss value is :  0.032258350402116776 \n",
            "\n",
            "The loss value is :  0.1803436279296875 \n",
            "\n",
            "The minimum loss obtained is: 0.1803436279296875\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974a0fc280> and its information value is:  tensor(0.0269)\n",
            "Target node: 63229\n",
            "1-hop neighbors of A: {63245}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {63236, 63237, 63238, 63239, 63240, 63241, 63242, 63243, 63244, 63246, 63247, 63248, 63249, 63250, 63251, 63252, 63253, 63254, 63255, 63256, 63257, 63258, 63259, 63260, 63261, 63262, 63263, 63264, 63221, 63222, 63223, 63224, 63225, 63226, 63227, 63228} \n",
            "\n",
            "The graph number 4226.0 contains: {63265, 63266, 63267, 63268, 63269, 63270, 63271, 63272, 63273, 63274, 63275, 63276, 63253, 63254, 63255}\n",
            "The loss value is :  0.04293046146631241 \n",
            "\n",
            "The loss value is :  0.13112694025039673 \n",
            "\n",
            "The loss value is :  0.013168297708034515 \n",
            "\n",
            "The loss value is :  0.1380946934223175 \n",
            "\n",
            "The loss value is :  0.03865335136651993 \n",
            "\n",
            "The loss value is :  0.14015036821365356 \n",
            "\n",
            "The loss value is :  0.05038844048976898 \n",
            "\n",
            "The loss value is :  0.1161823496222496 \n",
            "\n",
            "The loss value is :  0.018972191959619522 \n",
            "\n",
            "The loss value is :  0.14316175878047943 \n",
            "\n",
            "The loss value is :  0.01965593546628952 \n",
            "\n",
            "The loss value is :  0.10422728210687637 \n",
            "\n",
            "The loss value is :  0.04818638786673546 \n",
            "\n",
            "The loss value is :  0.1307714879512787 \n",
            "\n",
            "The loss value is :  0.029210343956947327 \n",
            "\n",
            "The loss value is :  0.14366360008716583 \n",
            "\n",
            "The minimum loss obtained is: 0.14366360008716583\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b543220> and its information value is:  tensor(0.0621)\n",
            "Target node: 63255\n",
            "1-hop neighbors of A: {63267, 63268}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {63265, 63266, 63269, 63270, 63271, 63272, 63273, 63274, 63275, 63276, 63253, 63254} \n",
            "\n",
            "The graph number 4227.0 contains: {63269, 63270, 63271, 63272, 63273, 63274, 63275, 63276, 63277, 63278, 63279, 63280, 63281, 63282, 63283, 63284, 63285, 63286, 63287, 63288, 63289}\n",
            "The loss value is :  0.020345374941825867 \n",
            "\n",
            "The loss value is :  0.13844937086105347 \n",
            "\n",
            "The loss value is :  0.01305354479700327 \n",
            "\n",
            "The loss value is :  0.16017115116119385 \n",
            "\n",
            "The loss value is :  0.03201161324977875 \n",
            "\n",
            "The loss value is :  0.1519869714975357 \n",
            "\n",
            "The loss value is :  0.03291545435786247 \n",
            "\n",
            "The loss value is :  0.1563919484615326 \n",
            "\n",
            "The loss value is :  0.02049493044614792 \n",
            "\n",
            "The loss value is :  0.1385476142168045 \n",
            "\n",
            "The loss value is :  0.009028127416968346 \n",
            "\n",
            "The loss value is :  0.15482203662395477 \n",
            "\n",
            "The loss value is :  0.021683422848582268 \n",
            "\n",
            "The loss value is :  0.15043365955352783 \n",
            "\n",
            "The loss value is :  0.01749226823449135 \n",
            "\n",
            "The loss value is :  0.1488887369632721 \n",
            "\n",
            "The minimum loss obtained is: 0.1488887369632721\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b5421a0> and its information value is:  tensor(0.0493)\n",
            "Target node: 63289\n",
            "1-hop neighbors of A: {63273}\n",
            "2-hop neighbors of A: {63288}\n",
            "Out of neighborhood of A: {63269, 63270, 63271, 63272, 63274, 63275, 63276, 63277, 63278, 63279, 63280, 63281, 63282, 63283, 63284, 63285, 63286, 63287} \n",
            "\n",
            "The graph number 4228.0 contains: {63296, 63297, 63298, 63299, 63300, 63301, 63302, 63303, 63273, 63274, 63275, 63278, 63279, 63289, 63290, 63291, 63292, 63293, 63294, 63295}\n",
            "The loss value is :  0.023310918360948563 \n",
            "\n",
            "The loss value is :  0.14982002973556519 \n",
            "\n",
            "The loss value is :  0.02923305332660675 \n",
            "\n",
            "The loss value is :  0.15091443061828613 \n",
            "\n",
            "The loss value is :  0.02148726023733616 \n",
            "\n",
            "The loss value is :  0.11079121381044388 \n",
            "\n",
            "The loss value is :  0.04521312564611435 \n",
            "\n",
            "The loss value is :  0.1340704709291458 \n",
            "\n",
            "The loss value is :  0.045378170907497406 \n",
            "\n",
            "The loss value is :  0.12081289291381836 \n",
            "\n",
            "The loss value is :  0.01669660024344921 \n",
            "\n",
            "The loss value is :  0.10409874469041824 \n",
            "\n",
            "The loss value is :  0.04038292169570923 \n",
            "\n",
            "The loss value is :  0.1458367109298706 \n",
            "\n",
            "The loss value is :  0.032085783779621124 \n",
            "\n",
            "The loss value is :  0.11515629291534424 \n",
            "\n",
            "The minimum loss obtained is: 0.11515629291534424\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b5435b0> and its information value is:  tensor(0.0707)\n",
            "Target node: 63295\n",
            "1-hop neighbors of A: {63279}\n",
            "2-hop neighbors of A: {63296, 63297}\n",
            "Out of neighborhood of A: {63298, 63299, 63300, 63301, 63302, 63303, 63273, 63274, 63275, 63278, 63289, 63290, 63291, 63292, 63293, 63294} \n",
            "\n",
            "The graph number 4229.0 contains: {63300, 63301, 63302, 63303, 63304, 63305, 63306, 63307, 63308}\n",
            "The loss value is :  0.05264420062303543 \n",
            "\n",
            "The loss value is :  0.039508670568466187 \n",
            "\n",
            "The loss value is :  0.017268670722842216 \n",
            "\n",
            "The loss value is :  0.13533823192119598 \n",
            "\n",
            "The loss value is :  0.0020124968141317368 \n",
            "\n",
            "The loss value is :  0.17438367009162903 \n",
            "\n",
            "The loss value is :  -0.009982680901885033 \n",
            "\n",
            "The loss value is :  0.09825260192155838 \n",
            "\n",
            "The loss value is :  0.016597431153059006 \n",
            "\n",
            "The loss value is :  0.08405396342277527 \n",
            "\n",
            "The loss value is :  0.0234963558614254 \n",
            "\n",
            "The loss value is :  0.12106499075889587 \n",
            "\n",
            "The loss value is :  0.004889309406280518 \n",
            "\n",
            "The loss value is :  0.14077338576316833 \n",
            "\n",
            "The loss value is :  0.006021963432431221 \n",
            "\n",
            "The loss value is :  0.10220801830291748 \n",
            "\n",
            "The minimum loss obtained is: 0.10220801830291748\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b5420e0> and its information value is:  tensor(0.1165)\n",
            "Target node: 63308\n",
            "1-hop neighbors of A: {63305}\n",
            "2-hop neighbors of A: {63307, 63302}\n",
            "Out of neighborhood of A: {63300, 63301, 63303, 63304, 63306} \n",
            "\n",
            "The graph number 4230.0 contains: {63300, 63301, 63302, 63304, 63306, 63309, 63310, 63311, 63312, 63313, 63314, 63315, 63316, 63317, 63318}\n",
            "The loss value is :  0.03800046443939209 \n",
            "\n",
            "The loss value is :  0.15806254744529724 \n",
            "\n",
            "The loss value is :  0.013017616234719753 \n",
            "\n",
            "The loss value is :  0.14686399698257446 \n",
            "\n",
            "The loss value is :  0.02365359477698803 \n",
            "\n",
            "The loss value is :  0.12625010311603546 \n",
            "\n",
            "The loss value is :  0.04990358650684357 \n",
            "\n",
            "The loss value is :  0.1231701672077179 \n",
            "\n",
            "The loss value is :  0.0284615196287632 \n",
            "\n",
            "The loss value is :  0.11700306087732315 \n",
            "\n",
            "The loss value is :  0.046561092138290405 \n",
            "\n",
            "The loss value is :  0.15046052634716034 \n",
            "\n",
            "The loss value is :  0.026249228045344353 \n",
            "\n",
            "The loss value is :  0.12933017313480377 \n",
            "\n",
            "The loss value is :  0.019473083317279816 \n",
            "\n",
            "The loss value is :  0.12143976986408234 \n",
            "\n",
            "The minimum loss obtained is: 0.12143976986408234\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b542830> and its information value is:  tensor(0.0751)\n",
            "Target node: 63318\n",
            "1-hop neighbors of A: {63310}\n",
            "2-hop neighbors of A: {63313, 63306, 63317}\n",
            "Out of neighborhood of A: {63300, 63301, 63302, 63304, 63309, 63311, 63312, 63314, 63315, 63316} \n",
            "\n",
            "The graph number 4231.0 contains: {63310, 63311, 63313, 63318, 63319, 63320, 63321, 63322, 63323, 63324, 63325, 63326, 63327, 63328, 63329, 63330, 63331, 63332, 63333}\n",
            "The loss value is :  0.03346162661910057 \n",
            "\n",
            "The loss value is :  0.11599918454885483 \n",
            "\n",
            "The loss value is :  0.035096243023872375 \n",
            "\n",
            "The loss value is :  0.11242178082466125 \n",
            "\n",
            "The loss value is :  0.011682229116559029 \n",
            "\n",
            "The loss value is :  0.12159055471420288 \n",
            "\n",
            "The loss value is :  0.017927374690771103 \n",
            "\n",
            "The loss value is :  0.1403152495622635 \n",
            "\n",
            "The loss value is :  0.008366892114281654 \n",
            "\n",
            "The loss value is :  0.13257507979869843 \n",
            "\n",
            "The loss value is :  0.022860947996377945 \n",
            "\n",
            "The loss value is :  0.1368834674358368 \n",
            "\n",
            "The loss value is :  0.029679669067263603 \n",
            "\n",
            "The loss value is :  0.14108096063137054 \n",
            "\n",
            "The loss value is :  0.023090383037924767 \n",
            "\n",
            "The loss value is :  0.13364654779434204 \n",
            "\n",
            "The minimum loss obtained is: 0.13364654779434204\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b541660> and its information value is:  tensor(0.0601)\n",
            "Target node: 63333\n",
            "1-hop neighbors of A: {63328}\n",
            "2-hop neighbors of A: {63324}\n",
            "Out of neighborhood of A: {63310, 63311, 63313, 63318, 63319, 63320, 63321, 63322, 63323, 63325, 63326, 63327, 63329, 63330, 63331, 63332} \n",
            "\n",
            "The graph number 4232.0 contains: {63360, 63361, 63362, 63363, 63364, 63365, 63366, 63367, 63368, 63369, 63370, 63371, 63372, 63373, 63374, 63375, 63376, 63377, 63378, 63325, 63327, 63328, 63329, 63330, 63331, 63332, 63334, 63335, 63336, 63337, 63338, 63339, 63340, 63341, 63342, 63343, 63344, 63345, 63346, 63347, 63348, 63349, 63350, 63351, 63352, 63353, 63354, 63355, 63356, 63357, 63358, 63359}\n",
            "The loss value is :  0.037239160388708115 \n",
            "\n",
            "The loss value is :  0.18084338307380676 \n",
            "\n",
            "The loss value is :  0.03509921580553055 \n",
            "\n",
            "The loss value is :  0.18096424639225006 \n",
            "\n",
            "The loss value is :  0.035774435847997665 \n",
            "\n",
            "The loss value is :  0.20286594331264496 \n",
            "\n",
            "The loss value is :  0.03842057287693024 \n",
            "\n",
            "The loss value is :  0.19801175594329834 \n",
            "\n",
            "The loss value is :  0.03754198178648949 \n",
            "\n",
            "The loss value is :  0.1864844411611557 \n",
            "\n",
            "The loss value is :  0.041326556354761124 \n",
            "\n",
            "The loss value is :  0.19541791081428528 \n",
            "\n",
            "The loss value is :  0.03334002569317818 \n",
            "\n",
            "The loss value is :  0.18045131862163544 \n",
            "\n",
            "The loss value is :  0.0342334508895874 \n",
            "\n",
            "The loss value is :  0.19030199944972992 \n",
            "\n",
            "The minimum loss obtained is: 0.19030199944972992\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974a08cca0> and its information value is:  tensor(0.0214)\n",
            "Target node: 63359\n",
            "1-hop neighbors of A: {63362, 63363, 63358}\n",
            "2-hop neighbors of A: {63360, 63361, 63367, 63368}\n",
            "Out of neighborhood of A: {63364, 63365, 63366, 63369, 63370, 63371, 63372, 63373, 63374, 63375, 63376, 63377, 63378, 63325, 63327, 63328, 63329, 63330, 63331, 63332, 63334, 63335, 63336, 63337, 63338, 63339, 63340, 63341, 63342, 63343, 63344, 63345, 63346, 63347, 63348, 63349, 63350, 63351, 63352, 63353, 63354, 63355, 63356, 63357} \n",
            "\n",
            "The graph number 4233.0 contains: {63362, 63363, 63365, 63366, 63368, 63369, 63370, 63371, 63375, 63377, 63379, 63381, 63382, 63383, 63384, 63385}\n",
            "The loss value is :  0.03336509317159653 \n",
            "\n",
            "The loss value is :  0.12988238036632538 \n",
            "\n",
            "The loss value is :  0.011099981144070625 \n",
            "\n",
            "The loss value is :  0.17292076349258423 \n",
            "\n",
            "The loss value is :  0.01298775989562273 \n",
            "\n",
            "The loss value is :  0.1330234855413437 \n",
            "\n",
            "The loss value is :  0.018081067129969597 \n",
            "\n",
            "The loss value is :  0.1100548505783081 \n",
            "\n",
            "The loss value is :  0.027929192408919334 \n",
            "\n",
            "The loss value is :  0.14005620777606964 \n",
            "\n",
            "The loss value is :  0.03259919211268425 \n",
            "\n",
            "The loss value is :  0.1272166669368744 \n",
            "\n",
            "The loss value is :  0.015607462264597416 \n",
            "\n",
            "The loss value is :  0.14921802282333374 \n",
            "\n",
            "The loss value is :  0.011543698608875275 \n",
            "\n",
            "The loss value is :  0.12292774021625519 \n",
            "\n",
            "The minimum loss obtained is: 0.12292774021625519\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b543fa0> and its information value is:  tensor(0.0719)\n",
            "Target node: 63385\n",
            "1-hop neighbors of A: {63370}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {63362, 63363, 63365, 63366, 63368, 63369, 63371, 63375, 63377, 63379, 63381, 63382, 63383, 63384} \n",
            "\n",
            "The graph number 4234.0 contains: {63370, 63372, 63373, 63376, 63377, 63378, 63379, 63380, 63385, 63386, 63387, 63388, 63389, 63390, 63391, 63392, 63393, 63394, 63395, 63396, 63397}\n",
            "The loss value is :  0.028503092005848885 \n",
            "\n",
            "The loss value is :  0.16057412326335907 \n",
            "\n",
            "The loss value is :  0.021769901737570763 \n",
            "\n",
            "The loss value is :  0.14087970554828644 \n",
            "\n",
            "The loss value is :  0.0243416428565979 \n",
            "\n",
            "The loss value is :  0.17020322382450104 \n",
            "\n",
            "The loss value is :  0.023201214149594307 \n",
            "\n",
            "The loss value is :  0.13547363877296448 \n",
            "\n",
            "The loss value is :  0.026258012279868126 \n",
            "\n",
            "The loss value is :  0.1584499329328537 \n",
            "\n",
            "The loss value is :  0.023120637983083725 \n",
            "\n",
            "The loss value is :  0.13029563426971436 \n",
            "\n",
            "The loss value is :  0.02378227561712265 \n",
            "\n",
            "The loss value is :  0.1218981221318245 \n",
            "\n",
            "The loss value is :  0.025429466739296913 \n",
            "\n",
            "The loss value is :  0.13282252848148346 \n",
            "\n",
            "The minimum loss obtained is: 0.13282252848148346\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b542d10> and its information value is:  tensor(0.0581)\n",
            "Target node: 63397\n",
            "1-hop neighbors of A: {63394}\n",
            "2-hop neighbors of A: {63393}\n",
            "Out of neighborhood of A: {63370, 63372, 63373, 63376, 63377, 63378, 63379, 63380, 63385, 63386, 63387, 63388, 63389, 63390, 63391, 63392, 63395, 63396} \n",
            "\n",
            "The graph number 4235.0 contains: {63394, 63395, 63396, 63397, 63398, 63399, 63400, 63401, 63402, 63403, 63404, 63405}\n",
            "The loss value is :  -0.017373237758874893 \n",
            "\n",
            "The loss value is :  0.10112547874450684 \n",
            "\n",
            "The loss value is :  0.019972404465079308 \n",
            "\n",
            "The loss value is :  0.14056631922721863 \n",
            "\n",
            "The loss value is :  0.014909118413925171 \n",
            "\n",
            "The loss value is :  0.14738468825817108 \n",
            "\n",
            "The loss value is :  0.015330863185226917 \n",
            "\n",
            "The loss value is :  0.10995741188526154 \n",
            "\n",
            "The loss value is :  0.03064611554145813 \n",
            "\n",
            "The loss value is :  0.10952728241682053 \n",
            "\n",
            "The loss value is :  0.04165014252066612 \n",
            "\n",
            "The loss value is :  0.15120355784893036 \n",
            "\n",
            "The loss value is :  0.02484005317091942 \n",
            "\n",
            "The loss value is :  0.2163623571395874 \n",
            "\n",
            "The loss value is :  0.05028794705867767 \n",
            "\n",
            "The loss value is :  0.11966204643249512 \n",
            "\n",
            "The minimum loss obtained is: 0.11966204643249512\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b540af0> and its information value is:  tensor(0.0862)\n",
            "Target node: 63405\n",
            "1-hop neighbors of A: {63404}\n",
            "2-hop neighbors of A: {63403}\n",
            "Out of neighborhood of A: {63394, 63395, 63396, 63397, 63398, 63399, 63400, 63401, 63402} \n",
            "\n",
            "The graph number 4236.0 contains: {63395, 63397, 63398, 63399, 63400, 63401, 63404, 63406, 63407, 63408, 63409, 63410}\n",
            "The loss value is :  0.03284766152501106 \n",
            "\n",
            "The loss value is :  0.1375965178012848 \n",
            "\n",
            "The loss value is :  0.029911499470472336 \n",
            "\n",
            "The loss value is :  0.10736250132322311 \n",
            "\n",
            "The loss value is :  0.05619025230407715 \n",
            "\n",
            "The loss value is :  0.1139160692691803 \n",
            "\n",
            "The loss value is :  -0.0023283734917640686 \n",
            "\n",
            "The loss value is :  0.09701943397521973 \n",
            "\n",
            "The loss value is :  -0.006473703309893608 \n",
            "\n",
            "The loss value is :  0.12393469363451004 \n",
            "\n",
            "The loss value is :  0.015540366992354393 \n",
            "\n",
            "The loss value is :  0.14797472953796387 \n",
            "\n",
            "The loss value is :  -0.010070029646158218 \n",
            "\n",
            "The loss value is :  0.11631585657596588 \n",
            "\n",
            "The loss value is :  -0.00806724838912487 \n",
            "\n",
            "The loss value is :  0.12435711175203323 \n",
            "\n",
            "The minimum loss obtained is: 0.12435711175203323\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b543b20> and its information value is:  tensor(0.0833)\n",
            "Target node: 63410\n",
            "1-hop neighbors of A: {63400}\n",
            "2-hop neighbors of A: {63409, 63398}\n",
            "Out of neighborhood of A: {63395, 63397, 63399, 63401, 63404, 63406, 63407, 63408} \n",
            "\n",
            "The graph number 4237.0 contains: {63424, 63401, 63402, 63405, 63411, 63412, 63413, 63414, 63415, 63416, 63417, 63418, 63419, 63420, 63421, 63422, 63423}\n",
            "The loss value is :  0.017719952389597893 \n",
            "\n",
            "The loss value is :  0.1900102198123932 \n",
            "\n",
            "The loss value is :  0.026566721498966217 \n",
            "\n",
            "The loss value is :  0.13152927160263062 \n",
            "\n",
            "The loss value is :  0.0049325283616781235 \n",
            "\n",
            "The loss value is :  0.13535656034946442 \n",
            "\n",
            "The loss value is :  0.028893014416098595 \n",
            "\n",
            "The loss value is :  0.10594885051250458 \n",
            "\n",
            "The loss value is :  0.02577897347509861 \n",
            "\n",
            "The loss value is :  0.12432761490345001 \n",
            "\n",
            "The loss value is :  0.038949914276599884 \n",
            "\n",
            "The loss value is :  0.11300447583198547 \n",
            "\n",
            "The loss value is :  0.04072800278663635 \n",
            "\n",
            "The loss value is :  0.11891051381826401 \n",
            "\n",
            "The loss value is :  0.030958151444792747 \n",
            "\n",
            "The loss value is :  0.16236594319343567 \n",
            "\n",
            "The minimum loss obtained is: 0.16236594319343567\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b541f90> and its information value is:  tensor(0.0486)\n",
            "Target node: 63423\n",
            "1-hop neighbors of A: {63424, 63422}\n",
            "2-hop neighbors of A: {63421}\n",
            "Out of neighborhood of A: {63401, 63402, 63405, 63411, 63412, 63413, 63414, 63415, 63416, 63417, 63418, 63419, 63420} \n",
            "\n",
            "The graph number 4238.0 contains: {63424, 63425, 63426, 63427, 63428, 63429, 63430, 63431, 63432, 63433, 63434, 63435, 63436, 63437, 63417, 63418, 63419, 63420, 63422, 63423}\n",
            "The loss value is :  0.022210905328392982 \n",
            "\n",
            "The loss value is :  0.1403253823518753 \n",
            "\n",
            "The loss value is :  0.026527462527155876 \n",
            "\n",
            "The loss value is :  0.15621061623096466 \n",
            "\n",
            "The loss value is :  0.026510003954172134 \n",
            "\n",
            "The loss value is :  0.1250428557395935 \n",
            "\n",
            "The loss value is :  0.018209043890237808 \n",
            "\n",
            "The loss value is :  0.14562153816223145 \n",
            "\n",
            "The loss value is :  0.00798114575445652 \n",
            "\n",
            "The loss value is :  0.14817658066749573 \n",
            "\n",
            "The loss value is :  0.045000527054071426 \n",
            "\n",
            "The loss value is :  0.14873965084552765 \n",
            "\n",
            "The loss value is :  0.06249848008155823 \n",
            "\n",
            "The loss value is :  0.1608026921749115 \n",
            "\n",
            "The loss value is :  0.023535985499620438 \n",
            "\n",
            "The loss value is :  0.14040370285511017 \n",
            "\n",
            "The minimum loss obtained is: 0.14040370285511017\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b540a90> and its information value is:  tensor(0.0549)\n",
            "Target node: 63423\n",
            "1-hop neighbors of A: {63432}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {63424, 63425, 63426, 63427, 63428, 63429, 63430, 63431, 63433, 63434, 63435, 63436, 63437, 63417, 63418, 63419, 63420, 63422} \n",
            "\n",
            "The graph number 4239.0 contains: {63435, 63436, 63437, 63438, 63439, 63440, 63441, 63442, 63443, 63444, 63445, 63446, 63447, 63448, 63449, 63450}\n",
            "The loss value is :  0.031919464468955994 \n",
            "\n",
            "The loss value is :  0.13773098587989807 \n",
            "\n",
            "The loss value is :  0.04377227649092674 \n",
            "\n",
            "The loss value is :  0.10005465149879456 \n",
            "\n",
            "The loss value is :  0.024840183556079865 \n",
            "\n",
            "The loss value is :  0.17494580149650574 \n",
            "\n",
            "The loss value is :  0.057386524975299835 \n",
            "\n",
            "The loss value is :  0.1263231635093689 \n",
            "\n",
            "The loss value is :  0.0424615815281868 \n",
            "\n",
            "The loss value is :  0.10472961515188217 \n",
            "\n",
            "The loss value is :  0.038438498973846436 \n",
            "\n",
            "The loss value is :  0.14564786851406097 \n",
            "\n",
            "The loss value is :  0.011015146970748901 \n",
            "\n",
            "The loss value is :  0.1225467398762703 \n",
            "\n",
            "The loss value is :  0.024406734853982925 \n",
            "\n",
            "The loss value is :  0.11627322435379028 \n",
            "\n",
            "The minimum loss obtained is: 0.11627322435379028\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b542e90> and its information value is:  tensor(0.0762)\n",
            "Target node: 63450\n",
            "1-hop neighbors of A: {63441}\n",
            "2-hop neighbors of A: {63449, 63442, 63437}\n",
            "Out of neighborhood of A: {63435, 63436, 63438, 63439, 63440, 63443, 63444, 63445, 63446, 63447, 63448} \n",
            "\n",
            "The graph number 4240.0 contains: {63456, 63457, 63458, 63459, 63460, 63461, 63462, 63463, 63442, 63443, 63451, 63452, 63453, 63454, 63455}\n",
            "The loss value is :  0.017759639769792557 \n",
            "\n",
            "The loss value is :  0.09881013631820679 \n",
            "\n",
            "The loss value is :  0.045487381517887115 \n",
            "\n",
            "The loss value is :  0.10947271436452866 \n",
            "\n",
            "The loss value is :  0.044253066182136536 \n",
            "\n",
            "The loss value is :  0.12420132756233215 \n",
            "\n",
            "The loss value is :  0.042978301644325256 \n",
            "\n",
            "The loss value is :  0.1366809606552124 \n",
            "\n",
            "The loss value is :  0.027613066136837006 \n",
            "\n",
            "The loss value is :  0.14063990116119385 \n",
            "\n",
            "The loss value is :  0.0022734813392162323 \n",
            "\n",
            "The loss value is :  0.11975090950727463 \n",
            "\n",
            "The loss value is :  0.006958175450563431 \n",
            "\n",
            "The loss value is :  0.13906729221343994 \n",
            "\n",
            "The loss value is :  0.038197942078113556 \n",
            "\n",
            "The loss value is :  0.16221509873867035 \n",
            "\n",
            "The minimum loss obtained is: 0.16221509873867035\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b542350> and its information value is:  tensor(0.0530)\n",
            "Target node: 63455\n",
            "1-hop neighbors of A: {63458, 63454}\n",
            "2-hop neighbors of A: {63456, 63457, 63462, 63463}\n",
            "Out of neighborhood of A: {63459, 63460, 63461, 63442, 63443, 63451, 63452, 63453} \n",
            "\n",
            "The graph number 4241.0 contains: {63455, 63457, 63459, 63460, 63461, 63462, 63463, 63464, 63465, 63466, 63467, 63468, 63469, 63470, 63471, 63472, 63473, 63474, 63475, 63476, 63477}\n",
            "The loss value is :  0.0175057090818882 \n",
            "\n",
            "The loss value is :  0.11327983438968658 \n",
            "\n",
            "The loss value is :  0.03800593689084053 \n",
            "\n",
            "The loss value is :  0.14676083624362946 \n",
            "\n",
            "The loss value is :  0.04177066311240196 \n",
            "\n",
            "The loss value is :  0.15555192530155182 \n",
            "\n",
            "The loss value is :  0.042696934193372726 \n",
            "\n",
            "The loss value is :  0.13574843108654022 \n",
            "\n",
            "The loss value is :  0.03400568664073944 \n",
            "\n",
            "The loss value is :  0.126634880900383 \n",
            "\n",
            "The loss value is :  0.024773573502898216 \n",
            "\n",
            "The loss value is :  0.11845865845680237 \n",
            "\n",
            "The loss value is :  0.02909701131284237 \n",
            "\n",
            "The loss value is :  0.14128276705741882 \n",
            "\n",
            "The loss value is :  0.02356996387243271 \n",
            "\n",
            "The loss value is :  0.14624106884002686 \n",
            "\n",
            "The minimum loss obtained is: 0.14624106884002686\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b543700> and its information value is:  tensor(0.0506)\n",
            "Target node: 63477\n",
            "1-hop neighbors of A: {63464}\n",
            "2-hop neighbors of A: {63459, 63461}\n",
            "Out of neighborhood of A: {63455, 63457, 63460, 63462, 63463, 63465, 63466, 63467, 63468, 63469, 63470, 63471, 63472, 63473, 63474, 63475, 63476} \n",
            "\n",
            "The graph number 4242.0 contains: {63488, 63489, 63490, 63491, 63492, 63493, 63466, 63467, 63469, 63478, 63479, 63480, 63481, 63482, 63483, 63484, 63485, 63486, 63487}\n",
            "The loss value is :  0.04235224425792694 \n",
            "\n",
            "The loss value is :  0.1555146872997284 \n",
            "\n",
            "The loss value is :  0.036650191992521286 \n",
            "\n",
            "The loss value is :  0.11954858154058456 \n",
            "\n",
            "The loss value is :  0.04653068259358406 \n",
            "\n",
            "The loss value is :  0.14125484228134155 \n",
            "\n",
            "The loss value is :  0.04414799064397812 \n",
            "\n",
            "The loss value is :  0.15943670272827148 \n",
            "\n",
            "The loss value is :  0.025240732356905937 \n",
            "\n",
            "The loss value is :  0.13465909659862518 \n",
            "\n",
            "The loss value is :  0.029200410470366478 \n",
            "\n",
            "The loss value is :  0.14678539335727692 \n",
            "\n",
            "The loss value is :  0.02898191474378109 \n",
            "\n",
            "The loss value is :  0.13025842607021332 \n",
            "\n",
            "The loss value is :  0.019918907433748245 \n",
            "\n",
            "The loss value is :  0.13461175560951233 \n",
            "\n",
            "The minimum loss obtained is: 0.13461175560951233\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b542620> and its information value is:  tensor(0.0595)\n",
            "Target node: 63487\n",
            "1-hop neighbors of A: {63490, 63484}\n",
            "2-hop neighbors of A: {63481, 63486}\n",
            "Out of neighborhood of A: {63488, 63489, 63491, 63492, 63493, 63466, 63467, 63469, 63478, 63479, 63480, 63482, 63483, 63485} \n",
            "\n",
            "The graph number 4243.0 contains: {63489, 63494, 63495, 63496, 63497, 63498, 63483, 63484, 63486, 63487}\n",
            "The loss value is :  0.0268842875957489 \n",
            "\n",
            "The loss value is :  0.08979937434196472 \n",
            "\n",
            "The loss value is :  0.041410572826862335 \n",
            "\n",
            "The loss value is :  0.1347343921661377 \n",
            "\n",
            "The loss value is :  0.00739673525094986 \n",
            "\n",
            "The loss value is :  0.10722775012254715 \n",
            "\n",
            "The loss value is :  0.02339816652238369 \n",
            "\n",
            "The loss value is :  0.14324653148651123 \n",
            "\n",
            "The loss value is :  0.0061590103432536125 \n",
            "\n",
            "The loss value is :  0.14490541815757751 \n",
            "\n",
            "The loss value is :  -0.001042976975440979 \n",
            "\n",
            "The loss value is :  0.15014687180519104 \n",
            "\n",
            "The loss value is :  0.004931975156068802 \n",
            "\n",
            "The loss value is :  0.1519124060869217 \n",
            "\n",
            "The loss value is :  0.017414219677448273 \n",
            "\n",
            "The loss value is :  0.07297348976135254 \n",
            "\n",
            "The minimum loss obtained is: 0.07297348976135254\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b5422f0> and its information value is:  tensor(0.1325)\n",
            "Target node: 63487\n",
            "1-hop neighbors of A: {63497}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {63489, 63494, 63495, 63496, 63498, 63483, 63484, 63486} \n",
            "\n",
            "The graph number 4244.0 contains: {63489, 63490, 63499, 63500, 63501, 63502, 63503, 63504, 63505, 63506, 63507}\n",
            "The loss value is :  0.04903828352689743 \n",
            "\n",
            "The loss value is :  0.13642571866512299 \n",
            "\n",
            "The loss value is :  0.014857950620353222 \n",
            "\n",
            "The loss value is :  0.15201012790203094 \n",
            "\n",
            "The loss value is :  0.034627579152584076 \n",
            "\n",
            "The loss value is :  0.11020578444004059 \n",
            "\n",
            "The loss value is :  0.007283985614776611 \n",
            "\n",
            "The loss value is :  0.13389456272125244 \n",
            "\n",
            "The loss value is :  0.003757370635867119 \n",
            "\n",
            "The loss value is :  0.05508648604154587 \n",
            "\n",
            "The loss value is :  -0.0013814829289913177 \n",
            "\n",
            "The loss value is :  0.11022472381591797 \n",
            "\n",
            "The loss value is :  0.017007865011692047 \n",
            "\n",
            "The loss value is :  0.09513872861862183 \n",
            "\n",
            "The loss value is :  0.009633359499275684 \n",
            "\n",
            "The loss value is :  0.12700596451759338 \n",
            "\n",
            "The minimum loss obtained is: 0.12700596451759338\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b5408e0> and its information value is:  tensor(0.0864)\n",
            "Target node: 63507\n",
            "1-hop neighbors of A: {63503}\n",
            "2-hop neighbors of A: {63506, 63502}\n",
            "Out of neighborhood of A: {63489, 63490, 63499, 63500, 63501, 63504, 63505} \n",
            "\n",
            "The graph number 4245.0 contains: {63504, 63505, 63506, 63507, 63508, 63509, 63510, 63511, 63512, 63513, 63514, 63515, 63516, 63517, 63518, 63519}\n",
            "The loss value is :  0.0019447635859251022 \n",
            "\n",
            "The loss value is :  0.0969729870557785 \n",
            "\n",
            "The loss value is :  -0.00019701384007930756 \n",
            "\n",
            "The loss value is :  0.16387739777565002 \n",
            "\n",
            "The loss value is :  0.043724771589040756 \n",
            "\n",
            "The loss value is :  0.10994908213615417 \n",
            "\n",
            "The loss value is :  0.007184548303484917 \n",
            "\n",
            "The loss value is :  0.12708349525928497 \n",
            "\n",
            "The loss value is :  0.03161187097430229 \n",
            "\n",
            "The loss value is :  0.16399629414081573 \n",
            "\n",
            "The loss value is :  0.019001899287104607 \n",
            "\n",
            "The loss value is :  0.111285500228405 \n",
            "\n",
            "The loss value is :  0.028791362419724464 \n",
            "\n",
            "The loss value is :  0.13657405972480774 \n",
            "\n",
            "The loss value is :  0.013828386552631855 \n",
            "\n",
            "The loss value is :  0.07899877429008484 \n",
            "\n",
            "The minimum loss obtained is: 0.07899877429008484\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b541990> and its information value is:  tensor(0.1062)\n",
            "Target node: 63519\n",
            "1-hop neighbors of A: {63517}\n",
            "2-hop neighbors of A: {63514}\n",
            "Out of neighborhood of A: {63504, 63505, 63506, 63507, 63508, 63509, 63510, 63511, 63512, 63513, 63515, 63516, 63518} \n",
            "\n",
            "The graph number 4246.0 contains: {63505, 63507, 63508, 63509, 63511, 63512, 63513, 63515, 63516, 63517, 63518, 63519, 63520, 63521, 63522, 63523, 63524, 63525, 63526, 63527, 63528, 63529, 63530, 63531, 63532, 63533, 63534, 63535, 63536, 63537, 63538, 63539, 63540, 63541, 63542, 63543}\n",
            "The loss value is :  0.035011615604162216 \n",
            "\n",
            "The loss value is :  0.17039015889167786 \n",
            "\n",
            "The loss value is :  0.03587799519300461 \n",
            "\n",
            "The loss value is :  0.17970289289951324 \n",
            "\n",
            "The loss value is :  0.045104287564754486 \n",
            "\n",
            "The loss value is :  0.17336535453796387 \n",
            "\n",
            "The loss value is :  0.03311268985271454 \n",
            "\n",
            "The loss value is :  0.16581402719020844 \n",
            "\n",
            "The loss value is :  0.030648529529571533 \n",
            "\n",
            "The loss value is :  0.16766589879989624 \n",
            "\n",
            "The loss value is :  0.0312657505273819 \n",
            "\n",
            "The loss value is :  0.17719805240631104 \n",
            "\n",
            "The loss value is :  0.02899596467614174 \n",
            "\n",
            "The loss value is :  0.17733918130397797 \n",
            "\n",
            "The loss value is :  0.03524399176239967 \n",
            "\n",
            "The loss value is :  0.15917497873306274 \n",
            "\n",
            "The minimum loss obtained is: 0.15917497873306274\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974e6c40d0> and its information value is:  tensor(0.0359)\n",
            "Target node: 63543\n",
            "1-hop neighbors of A: {63541}\n",
            "2-hop neighbors of A: {63540, 63542}\n",
            "Out of neighborhood of A: {63505, 63507, 63508, 63509, 63511, 63512, 63513, 63515, 63516, 63517, 63518, 63519, 63520, 63521, 63522, 63523, 63524, 63525, 63526, 63527, 63528, 63529, 63530, 63531, 63532, 63533, 63534, 63535, 63536, 63537, 63538, 63539} \n",
            "\n",
            "The graph number 4247.0 contains: {63552, 63553, 63554, 63555, 63556, 63557, 63558, 63559, 63534, 63535, 63538, 63539, 63542, 63544, 63545, 63546, 63547, 63548, 63549, 63550, 63551}\n",
            "The loss value is :  0.03723757714033127 \n",
            "\n",
            "The loss value is :  0.15392573177814484 \n",
            "\n",
            "The loss value is :  0.03437569737434387 \n",
            "\n",
            "The loss value is :  0.1587257981300354 \n",
            "\n",
            "The loss value is :  0.022316601127386093 \n",
            "\n",
            "The loss value is :  0.15570342540740967 \n",
            "\n",
            "The loss value is :  0.02378137782216072 \n",
            "\n",
            "The loss value is :  0.15396615862846375 \n",
            "\n",
            "The loss value is :  0.02035430446267128 \n",
            "\n",
            "The loss value is :  0.1497822403907776 \n",
            "\n",
            "The loss value is :  0.025578733533620834 \n",
            "\n",
            "The loss value is :  0.13462796807289124 \n",
            "\n",
            "The loss value is :  0.026246432214975357 \n",
            "\n",
            "The loss value is :  0.17521116137504578 \n",
            "\n",
            "The loss value is :  0.040259718894958496 \n",
            "\n",
            "The loss value is :  0.16754232347011566 \n",
            "\n",
            "The minimum loss obtained is: 0.16754232347011566\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b542b60> and its information value is:  tensor(0.0407)\n",
            "Target node: 63551\n",
            "1-hop neighbors of A: {63535}\n",
            "2-hop neighbors of A: {63550}\n",
            "Out of neighborhood of A: {63552, 63553, 63554, 63555, 63556, 63557, 63558, 63559, 63534, 63538, 63539, 63542, 63544, 63545, 63546, 63547, 63548, 63549} \n",
            "\n",
            "The graph number 4248.0 contains: {63559, 63560, 63561, 63562, 63563, 63564, 63565, 63566, 63567, 63568, 63569, 63570, 63571, 63572, 63573, 63574, 63575, 63576, 63577, 63578, 63579, 63580, 63544, 63545, 63546, 63547}\n",
            "The loss value is :  0.032173752784729004 \n",
            "\n",
            "The loss value is :  0.16496500372886658 \n",
            "\n",
            "The loss value is :  0.023292001336812973 \n",
            "\n",
            "The loss value is :  0.1745491772890091 \n",
            "\n",
            "The loss value is :  0.02780950255692005 \n",
            "\n",
            "The loss value is :  0.16664539277553558 \n",
            "\n",
            "The loss value is :  0.026769911870360374 \n",
            "\n",
            "The loss value is :  0.17227092385292053 \n",
            "\n",
            "The loss value is :  0.028407255187630653 \n",
            "\n",
            "The loss value is :  0.1724221408367157 \n",
            "\n",
            "The loss value is :  0.04781033843755722 \n",
            "\n",
            "The loss value is :  0.16690067946910858 \n",
            "\n",
            "The loss value is :  0.03552532568573952 \n",
            "\n",
            "The loss value is :  0.17755161225795746 \n",
            "\n",
            "The loss value is :  0.03842664510011673 \n",
            "\n",
            "The loss value is :  0.17264968156814575 \n",
            "\n",
            "The minimum loss obtained is: 0.17264968156814575\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b5427a0> and its information value is:  tensor(0.0344)\n",
            "Target node: 63547\n",
            "1-hop neighbors of A: {63565, 63566, 63567}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {63559, 63560, 63561, 63562, 63563, 63564, 63568, 63569, 63570, 63571, 63572, 63573, 63574, 63575, 63576, 63577, 63578, 63579, 63580, 63544, 63545, 63546} \n",
            "\n",
            "The graph number 4249.0 contains: {63584, 63585, 63586, 63587, 63588, 63573, 63574, 63576, 63577, 63578, 63580, 63581, 63582, 63583}\n",
            "The loss value is :  0.021349046379327774 \n",
            "\n",
            "The loss value is :  0.14868848025798798 \n",
            "\n",
            "The loss value is :  0.04315076768398285 \n",
            "\n",
            "The loss value is :  0.13656802475452423 \n",
            "\n",
            "The loss value is :  0.012605788186192513 \n",
            "\n",
            "The loss value is :  0.1454448103904724 \n",
            "\n",
            "The loss value is :  0.04626547545194626 \n",
            "\n",
            "The loss value is :  0.10673363506793976 \n",
            "\n",
            "The loss value is :  0.00018470920622348785 \n",
            "\n",
            "The loss value is :  0.15444116294384003 \n",
            "\n",
            "The loss value is :  0.03970782831311226 \n",
            "\n",
            "The loss value is :  0.14013166725635529 \n",
            "\n",
            "The loss value is :  0.009511128067970276 \n",
            "\n",
            "The loss value is :  0.0983724594116211 \n",
            "\n",
            "The loss value is :  0.000338919460773468 \n",
            "\n",
            "The loss value is :  0.13402198255062103 \n",
            "\n",
            "The minimum loss obtained is: 0.13402198255062103\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b543130> and its information value is:  tensor(0.0702)\n",
            "Target node: 63583\n",
            "1-hop neighbors of A: {63585, 63578}\n",
            "2-hop neighbors of A: {63584, 63574}\n",
            "Out of neighborhood of A: {63586, 63587, 63588, 63573, 63576, 63577, 63580, 63581, 63582} \n",
            "\n",
            "The graph number 4250.0 contains: {63580, 63581, 63582, 63583, 63584, 63585, 63589, 63590, 63591, 63592, 63593, 63594, 63595, 63596, 63597, 63598, 63599, 63600, 63601, 63602, 63603, 63604, 63605}\n",
            "The loss value is :  0.014080042950809002 \n",
            "\n",
            "The loss value is :  0.1518431007862091 \n",
            "\n",
            "The loss value is :  0.03761150687932968 \n",
            "\n",
            "The loss value is :  0.1438516527414322 \n",
            "\n",
            "The loss value is :  0.024064596742391586 \n",
            "\n",
            "The loss value is :  0.14368297159671783 \n",
            "\n",
            "The loss value is :  0.04910828173160553 \n",
            "\n",
            "The loss value is :  0.14125190675258636 \n",
            "\n",
            "The loss value is :  0.025886043906211853 \n",
            "\n",
            "The loss value is :  0.15507195889949799 \n",
            "\n",
            "The loss value is :  0.04705933481454849 \n",
            "\n",
            "The loss value is :  0.15372639894485474 \n",
            "\n",
            "The loss value is :  0.02119193598628044 \n",
            "\n",
            "The loss value is :  0.17392697930335999 \n",
            "\n",
            "The loss value is :  0.0355268269777298 \n",
            "\n",
            "The loss value is :  0.12187857925891876 \n",
            "\n",
            "The minimum loss obtained is: 0.12187857925891876\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b5418a0> and its information value is:  tensor(0.0632)\n",
            "Target node: 63605\n",
            "1-hop neighbors of A: {63602}\n",
            "2-hop neighbors of A: {63604, 63599}\n",
            "Out of neighborhood of A: {63580, 63581, 63582, 63583, 63584, 63585, 63589, 63590, 63591, 63592, 63593, 63594, 63595, 63596, 63597, 63598, 63600, 63601, 63603} \n",
            "\n",
            "The graph number 4251.0 contains: {63616, 63617, 63618, 63619, 63620, 63621, 63597, 63598, 63599, 63600, 63601, 63602, 63603, 63605, 63606, 63607, 63608, 63609, 63610, 63611, 63612, 63613, 63614, 63615}\n",
            "The loss value is :  0.034067552536726 \n",
            "\n",
            "The loss value is :  0.13688825070858002 \n",
            "\n",
            "The loss value is :  0.007802166044712067 \n",
            "\n",
            "The loss value is :  0.15926982462406158 \n",
            "\n",
            "The loss value is :  0.032219626009464264 \n",
            "\n",
            "The loss value is :  0.15936912596225739 \n",
            "\n",
            "The loss value is :  0.03339191526174545 \n",
            "\n",
            "The loss value is :  0.16157656908035278 \n",
            "\n",
            "The loss value is :  0.03259975090622902 \n",
            "\n",
            "The loss value is :  0.12953883409500122 \n",
            "\n",
            "The loss value is :  0.03764703869819641 \n",
            "\n",
            "The loss value is :  0.15629199147224426 \n",
            "\n",
            "The loss value is :  0.029411746188998222 \n",
            "\n",
            "The loss value is :  0.14863383769989014 \n",
            "\n",
            "The loss value is :  0.02962631918489933 \n",
            "\n",
            "The loss value is :  0.15398240089416504 \n",
            "\n",
            "The minimum loss obtained is: 0.15398240089416504\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b542980> and its information value is:  tensor(0.0440)\n",
            "Target node: 63615\n",
            "1-hop neighbors of A: {63600}\n",
            "2-hop neighbors of A: {63599}\n",
            "Out of neighborhood of A: {63616, 63617, 63618, 63619, 63620, 63621, 63597, 63598, 63601, 63602, 63603, 63605, 63606, 63607, 63608, 63609, 63610, 63611, 63612, 63613, 63614} \n",
            "\n",
            "The graph number 4252.0 contains: {63621, 63622, 63623, 63624, 63625, 63626, 63627, 63628, 63629, 63630, 63609, 63610, 63611, 63612}\n",
            "The loss value is :  0.03106127306818962 \n",
            "\n",
            "The loss value is :  0.10928750783205032 \n",
            "\n",
            "The loss value is :  0.019651606678962708 \n",
            "\n",
            "The loss value is :  0.11513146013021469 \n",
            "\n",
            "The loss value is :  0.040755219757556915 \n",
            "\n",
            "The loss value is :  0.1303611844778061 \n",
            "\n",
            "The loss value is :  0.0183686763048172 \n",
            "\n",
            "The loss value is :  0.11521837115287781 \n",
            "\n",
            "The loss value is :  0.0221274234354496 \n",
            "\n",
            "The loss value is :  0.14003539085388184 \n",
            "\n",
            "The loss value is :  0.005111146718263626 \n",
            "\n",
            "The loss value is :  0.09582162648439407 \n",
            "\n",
            "The loss value is :  0.032167889177799225 \n",
            "\n",
            "The loss value is :  0.15405507385730743 \n",
            "\n",
            "The loss value is :  0.035966020077466965 \n",
            "\n",
            "The loss value is :  0.10603664070367813 \n",
            "\n",
            "The minimum loss obtained is: 0.10603664070367813\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b543d90> and its information value is:  tensor(0.0884)\n",
            "Target node: 63612\n",
            "1-hop neighbors of A: {63626, 63627, 63628}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {63621, 63622, 63623, 63624, 63625, 63629, 63630, 63609, 63610, 63611} \n",
            "\n",
            "The graph number 4253.0 contains: {63629, 63630, 63631, 63632, 63633, 63634, 63635, 63636, 63637, 63638, 63639, 63640, 63641, 63642}\n",
            "The loss value is :  0.027431383728981018 \n",
            "\n",
            "The loss value is :  0.11472068727016449 \n",
            "\n",
            "The loss value is :  0.017675597220659256 \n",
            "\n",
            "The loss value is :  0.1401737481355667 \n",
            "\n",
            "The loss value is :  0.022507432848215103 \n",
            "\n",
            "The loss value is :  0.08281760662794113 \n",
            "\n",
            "The loss value is :  0.010390768758952618 \n",
            "\n",
            "The loss value is :  0.10824012756347656 \n",
            "\n",
            "The loss value is :  0.01207041461020708 \n",
            "\n",
            "The loss value is :  0.11462394148111343 \n",
            "\n",
            "The loss value is :  0.019480634480714798 \n",
            "\n",
            "The loss value is :  0.09623780846595764 \n",
            "\n",
            "The loss value is :  0.06895680725574493 \n",
            "\n",
            "The loss value is :  0.11086594313383102 \n",
            "\n",
            "The loss value is :  0.03419902175664902 \n",
            "\n",
            "The loss value is :  0.13779550790786743 \n",
            "\n",
            "The minimum loss obtained is: 0.13779550790786743\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b543ee0> and its information value is:  tensor(0.0681)\n",
            "Target node: 63642\n",
            "1-hop neighbors of A: {63638}\n",
            "2-hop neighbors of A: {63641, 63634}\n",
            "Out of neighborhood of A: {63629, 63630, 63631, 63632, 63633, 63635, 63636, 63637, 63639, 63640} \n",
            "\n",
            "The graph number 4254.0 contains: {63648, 63633, 63636, 63638, 63641, 63642, 63643, 63644, 63645, 63646, 63647}\n",
            "The loss value is :  0.04423607140779495 \n",
            "\n",
            "The loss value is :  0.09788381308317184 \n",
            "\n",
            "The loss value is :  0.01122452411800623 \n",
            "\n",
            "The loss value is :  0.0652182474732399 \n",
            "\n",
            "The loss value is :  0.02500801347196102 \n",
            "\n",
            "The loss value is :  0.13055159151554108 \n",
            "\n",
            "The loss value is :  0.03137318417429924 \n",
            "\n",
            "The loss value is :  0.09819741547107697 \n",
            "\n",
            "The loss value is :  0.05924315005540848 \n",
            "\n",
            "The loss value is :  0.09374871850013733 \n",
            "\n",
            "The loss value is :  0.04127836227416992 \n",
            "\n",
            "The loss value is :  0.088651143014431 \n",
            "\n",
            "The loss value is :  0.006589164957404137 \n",
            "\n",
            "The loss value is :  0.12175241857767105 \n",
            "\n",
            "The loss value is :  0.026065293699502945 \n",
            "\n",
            "The loss value is :  0.104885034263134 \n",
            "\n",
            "The minimum loss obtained is: 0.104885034263134\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b540df0> and its information value is:  tensor(0.1011)\n",
            "Target node: 63647\n",
            "1-hop neighbors of A: {63646}\n",
            "2-hop neighbors of A: {63648, 63643}\n",
            "Out of neighborhood of A: {63633, 63636, 63638, 63641, 63642, 63644, 63645} \n",
            "\n",
            "The graph number 4255.0 contains: {63630, 63632, 63634, 63641, 63642, 63644, 63645, 63649, 63650, 63651, 63652, 63653, 63654, 63655, 63656, 63657, 63658, 63659, 63660, 63661, 63662, 63663, 63664}\n",
            "The loss value is :  0.017301566898822784 \n",
            "\n",
            "The loss value is :  0.16731376945972443 \n",
            "\n",
            "The loss value is :  0.0174984410405159 \n",
            "\n",
            "The loss value is :  0.14968471229076385 \n",
            "\n",
            "The loss value is :  0.037226755172014236 \n",
            "\n",
            "The loss value is :  0.14249055087566376 \n",
            "\n",
            "The loss value is :  0.03421188145875931 \n",
            "\n",
            "The loss value is :  0.14725260436534882 \n",
            "\n",
            "The loss value is :  0.02231384627521038 \n",
            "\n",
            "The loss value is :  0.14458692073822021 \n",
            "\n",
            "The loss value is :  0.0195614006370306 \n",
            "\n",
            "The loss value is :  0.15733979642391205 \n",
            "\n",
            "The loss value is :  0.026076719164848328 \n",
            "\n",
            "The loss value is :  0.16874341666698456 \n",
            "\n",
            "The loss value is :  0.02952684462070465 \n",
            "\n",
            "The loss value is :  0.16648592054843903 \n",
            "\n",
            "The minimum loss obtained is: 0.16648592054843903\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b543a00> and its information value is:  tensor(0.0392)\n",
            "Target node: 63664\n",
            "1-hop neighbors of A: {63659}\n",
            "2-hop neighbors of A: {63656}\n",
            "Out of neighborhood of A: {63630, 63632, 63634, 63641, 63642, 63644, 63645, 63649, 63650, 63651, 63652, 63653, 63654, 63655, 63657, 63658, 63660, 63661, 63662, 63663} \n",
            "\n",
            "The graph number 4256.0 contains: {63659, 63661, 63662, 63663, 63664, 63665, 63666, 63667, 63668, 63669, 63670, 63671, 63672, 63673, 63674, 63675, 63676, 63677}\n",
            "The loss value is :  0.030906397849321365 \n",
            "\n",
            "The loss value is :  0.16121315956115723 \n",
            "\n",
            "The loss value is :  0.012382173910737038 \n",
            "\n",
            "The loss value is :  0.15205512940883636 \n",
            "\n",
            "The loss value is :  0.017359573394060135 \n",
            "\n",
            "The loss value is :  0.11713390052318573 \n",
            "\n",
            "The loss value is :  0.03012416511774063 \n",
            "\n",
            "The loss value is :  0.15347574651241302 \n",
            "\n",
            "The loss value is :  0.006114358082413673 \n",
            "\n",
            "The loss value is :  0.12929129600524902 \n",
            "\n",
            "The loss value is :  0.030156098306179047 \n",
            "\n",
            "The loss value is :  0.10796862095594406 \n",
            "\n",
            "The loss value is :  0.02760881744325161 \n",
            "\n",
            "The loss value is :  0.13131405413150787 \n",
            "\n",
            "The loss value is :  0.033623628318309784 \n",
            "\n",
            "The loss value is :  0.14624591171741486 \n",
            "\n",
            "The minimum loss obtained is: 0.14624591171741486\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b541390> and its information value is:  tensor(0.0547)\n",
            "Target node: 63677\n",
            "1-hop neighbors of A: {63675}\n",
            "2-hop neighbors of A: {63673}\n",
            "Out of neighborhood of A: {63659, 63661, 63662, 63663, 63664, 63665, 63666, 63667, 63668, 63669, 63670, 63671, 63672, 63674, 63676} \n",
            "\n",
            "The graph number 4257.0 contains: {63680, 63681, 63660, 63662, 63664, 63665, 63668, 63670, 63676, 63677, 63678, 63679}\n",
            "The loss value is :  -0.0019566789269447327 \n",
            "\n",
            "The loss value is :  0.15919889509677887 \n",
            "\n",
            "The loss value is :  0.0598292201757431 \n",
            "\n",
            "The loss value is :  0.09673906117677689 \n",
            "\n",
            "The loss value is :  -0.0014280658215284348 \n",
            "\n",
            "The loss value is :  0.08562199771404266 \n",
            "\n",
            "The loss value is :  0.008259343914687634 \n",
            "\n",
            "The loss value is :  0.16601239144802094 \n",
            "\n",
            "The loss value is :  0.05383475869894028 \n",
            "\n",
            "The loss value is :  0.15298154950141907 \n",
            "\n",
            "The loss value is :  0.023860011249780655 \n",
            "\n",
            "The loss value is :  0.12726756930351257 \n",
            "\n",
            "The loss value is :  0.023050347343087196 \n",
            "\n",
            "The loss value is :  0.08337845653295517 \n",
            "\n",
            "The loss value is :  0.021218184381723404 \n",
            "\n",
            "The loss value is :  0.12820842862129211 \n",
            "\n",
            "The minimum loss obtained is: 0.12820842862129211\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b541a80> and its information value is:  tensor(0.0809)\n",
            "Target node: 63679\n",
            "1-hop neighbors of A: {63681, 63678}\n",
            "2-hop neighbors of A: {63680, 63677}\n",
            "Out of neighborhood of A: {63660, 63662, 63664, 63665, 63668, 63670, 63676} \n",
            "\n",
            "The graph number 4258.0 contains: {63682, 63683, 63684, 63685, 63686, 63687, 63688, 63689, 63690, 63691, 63660, 63663, 63667, 63668, 63670, 63671, 63673, 63674, 63675, 63676, 63677}\n",
            "The loss value is :  0.023958196863532066 \n",
            "\n",
            "The loss value is :  0.18097367882728577 \n",
            "\n",
            "The loss value is :  0.02442975342273712 \n",
            "\n",
            "The loss value is :  0.1528754085302353 \n",
            "\n",
            "The loss value is :  0.030975455418229103 \n",
            "\n",
            "The loss value is :  0.1428156942129135 \n",
            "\n",
            "The loss value is :  0.032954517751932144 \n",
            "\n",
            "The loss value is :  0.13402362167835236 \n",
            "\n",
            "The loss value is :  0.024481382220983505 \n",
            "\n",
            "The loss value is :  0.13719798624515533 \n",
            "\n",
            "The loss value is :  0.030186504125595093 \n",
            "\n",
            "The loss value is :  0.18468421697616577 \n",
            "\n",
            "The loss value is :  0.035955965518951416 \n",
            "\n",
            "The loss value is :  0.12854087352752686 \n",
            "\n",
            "The loss value is :  0.025400735437870026 \n",
            "\n",
            "The loss value is :  0.1354389637708664 \n",
            "\n",
            "The minimum loss obtained is: 0.1354389637708664\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b543df0> and its information value is:  tensor(0.0566)\n",
            "Target node: 63677\n",
            "1-hop neighbors of A: {63676}\n",
            "2-hop neighbors of A: {63691}\n",
            "Out of neighborhood of A: {63682, 63683, 63684, 63685, 63686, 63687, 63688, 63689, 63690, 63660, 63663, 63667, 63668, 63670, 63671, 63673, 63674, 63675} \n",
            "\n",
            "The graph number 4259.0 contains: {63681, 63692, 63693, 63694, 63695, 63696, 63697, 63698, 63699, 63700}\n",
            "The loss value is :  -0.007741602137684822 \n",
            "\n",
            "The loss value is :  0.056399159133434296 \n",
            "\n",
            "The loss value is :  0.03497796505689621 \n",
            "\n",
            "The loss value is :  0.13377858698368073 \n",
            "\n",
            "The loss value is :  0.020858081057667732 \n",
            "\n",
            "The loss value is :  0.10957962274551392 \n",
            "\n",
            "The loss value is :  0.028526978567242622 \n",
            "\n",
            "The loss value is :  0.07889837771654129 \n",
            "\n",
            "The loss value is :  0.01134333573281765 \n",
            "\n",
            "The loss value is :  0.122652068734169 \n",
            "\n",
            "The loss value is :  0.0008022859692573547 \n",
            "\n",
            "The loss value is :  0.13930873572826385 \n",
            "\n",
            "The loss value is :  0.041804127395153046 \n",
            "\n",
            "The loss value is :  0.06162310764193535 \n",
            "\n",
            "The loss value is :  0.029720094054937363 \n",
            "\n",
            "The loss value is :  0.08043263852596283 \n",
            "\n",
            "The minimum loss obtained is: 0.08043263852596283\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b543580> and its information value is:  tensor(0.1260)\n",
            "Target node: 63700\n",
            "1-hop neighbors of A: {63696}\n",
            "2-hop neighbors of A: {63695}\n",
            "Out of neighborhood of A: {63681, 63692, 63693, 63694, 63697, 63698, 63699} \n",
            "\n",
            "The graph number 4260.0 contains: {63697, 63698, 63699, 63700, 63701, 63702, 63703, 63704, 63705, 63706, 63707, 63708, 63709}\n",
            "The loss value is :  0.032659418880939484 \n",
            "\n",
            "The loss value is :  0.0984032079577446 \n",
            "\n",
            "The loss value is :  -0.007327865809202194 \n",
            "\n",
            "The loss value is :  0.13994313776493073 \n",
            "\n",
            "The loss value is :  -0.006865197792649269 \n",
            "\n",
            "The loss value is :  0.1125354915857315 \n",
            "\n",
            "The loss value is :  0.010063511319458485 \n",
            "\n",
            "The loss value is :  0.1240575909614563 \n",
            "\n",
            "The loss value is :  0.01329848449677229 \n",
            "\n",
            "The loss value is :  0.09352727234363556 \n",
            "\n",
            "The loss value is :  0.01622181199491024 \n",
            "\n",
            "The loss value is :  0.1437263786792755 \n",
            "\n",
            "The loss value is :  0.07301218807697296 \n",
            "\n",
            "The loss value is :  0.1234765499830246 \n",
            "\n",
            "The loss value is :  0.02915853261947632 \n",
            "\n",
            "The loss value is :  0.12363682687282562 \n",
            "\n",
            "The minimum loss obtained is: 0.12363682687282562\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b540d60> and its information value is:  tensor(0.0798)\n",
            "Target node: 63709\n",
            "1-hop neighbors of A: {63705}\n",
            "2-hop neighbors of A: {63701}\n",
            "Out of neighborhood of A: {63697, 63698, 63699, 63700, 63702, 63703, 63704, 63706, 63707, 63708} \n",
            "\n",
            "The graph number 4261.0 contains: {63712, 63713, 63714, 63715, 63716, 63696, 63705, 63706, 63707, 63709, 63710, 63711}\n",
            "The loss value is :  0.031923457980155945 \n",
            "\n",
            "The loss value is :  0.13389749825000763 \n",
            "\n",
            "The loss value is :  0.008396057412028313 \n",
            "\n",
            "The loss value is :  0.16623036563396454 \n",
            "\n",
            "The loss value is :  0.020388316363096237 \n",
            "\n",
            "The loss value is :  0.1156642735004425 \n",
            "\n",
            "The loss value is :  0.023721598088741302 \n",
            "\n",
            "The loss value is :  0.12922880053520203 \n",
            "\n",
            "The loss value is :  0.03850499540567398 \n",
            "\n",
            "The loss value is :  0.08497166633605957 \n",
            "\n",
            "The loss value is :  0.04962284862995148 \n",
            "\n",
            "The loss value is :  0.07236753404140472 \n",
            "\n",
            "The loss value is :  0.007417742162942886 \n",
            "\n",
            "The loss value is :  0.1571161448955536 \n",
            "\n",
            "The loss value is :  0.03392605856060982 \n",
            "\n",
            "The loss value is :  0.15210595726966858 \n",
            "\n",
            "The minimum loss obtained is: 0.15210595726966858\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b540bb0> and its information value is:  tensor(0.0676)\n",
            "Target node: 63711\n",
            "1-hop neighbors of A: {63706, 63715}\n",
            "2-hop neighbors of A: {63712}\n",
            "Out of neighborhood of A: {63713, 63714, 63716, 63696, 63705, 63707, 63709, 63710} \n",
            "\n",
            "The graph number 4262.0 contains: {63696, 63697, 63698, 63699, 63703, 63707, 63709, 63711, 63713, 63714, 63715, 63716, 63717, 63718, 63719, 63720, 63721, 63722, 63723, 63724, 63725, 63726, 63727, 63728, 63729, 63730, 63731, 63732, 63733, 63734, 63735, 63736, 63737, 63738, 63739}\n",
            "The loss value is :  0.03321443870663643 \n",
            "\n",
            "The loss value is :  0.15749353170394897 \n",
            "\n",
            "The loss value is :  0.02582186833024025 \n",
            "\n",
            "The loss value is :  0.1943863183259964 \n",
            "\n",
            "The loss value is :  0.036251991987228394 \n",
            "\n",
            "The loss value is :  0.16658008098602295 \n",
            "\n",
            "The loss value is :  0.04128916934132576 \n",
            "\n",
            "The loss value is :  0.16370323300361633 \n",
            "\n",
            "The loss value is :  0.027172815054655075 \n",
            "\n",
            "The loss value is :  0.16567204892635345 \n",
            "\n",
            "The loss value is :  0.02436390519142151 \n",
            "\n",
            "The loss value is :  0.17718039453029633 \n",
            "\n",
            "The loss value is :  0.030636543408036232 \n",
            "\n",
            "The loss value is :  0.17601577937602997 \n",
            "\n",
            "The loss value is :  0.028865871950984 \n",
            "\n",
            "The loss value is :  0.16956987977027893 \n",
            "\n",
            "The minimum loss obtained is: 0.16956987977027893\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b542b30> and its information value is:  tensor(0.0317)\n",
            "Target node: 63739\n",
            "1-hop neighbors of A: {63736}\n",
            "2-hop neighbors of A: {63737, 63738}\n",
            "Out of neighborhood of A: {63696, 63697, 63698, 63699, 63703, 63707, 63709, 63711, 63713, 63714, 63715, 63716, 63717, 63718, 63719, 63720, 63721, 63722, 63723, 63724, 63725, 63726, 63727, 63728, 63729, 63730, 63731, 63732, 63733, 63734, 63735} \n",
            "\n",
            "The graph number 4263.0 contains: {63744, 63745, 63746, 63747, 63736, 63737, 63738, 63739, 63740, 63741, 63742, 63743}\n",
            "The loss value is :  0.023926306515932083 \n",
            "\n",
            "The loss value is :  0.04717278480529785 \n",
            "\n",
            "The loss value is :  0.016034966334700584 \n",
            "\n",
            "The loss value is :  0.10067079961299896 \n",
            "\n",
            "The loss value is :  0.05315959453582764 \n",
            "\n",
            "The loss value is :  0.07229183614253998 \n",
            "\n",
            "The loss value is :  0.022039392963051796 \n",
            "\n",
            "The loss value is :  0.10473329573869705 \n",
            "\n",
            "The loss value is :  0.08721283078193665 \n",
            "\n",
            "The loss value is :  0.09259561449289322 \n",
            "\n",
            "The loss value is :  0.01724608801305294 \n",
            "\n",
            "The loss value is :  0.11278092861175537 \n",
            "\n",
            "The loss value is :  0.02126566879451275 \n",
            "\n",
            "The loss value is :  0.1334650218486786 \n",
            "\n",
            "The loss value is :  0.0034457966685295105 \n",
            "\n",
            "The loss value is :  0.10944968461990356 \n",
            "\n",
            "The minimum loss obtained is: 0.10944968461990356\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b542b30> and its information value is:  tensor(0.0931)\n",
            "Target node: 63743\n",
            "1-hop neighbors of A: {63737}\n",
            "2-hop neighbors of A: {63744, 63740}\n",
            "Out of neighborhood of A: {63745, 63746, 63747, 63736, 63738, 63739, 63741, 63742} \n",
            "\n",
            "The graph number 4264.0 contains: {63748, 63749, 63750, 63751, 63752, 63753, 63754, 63755, 63740, 63742}\n",
            "The loss value is :  -0.0015196558088064194 \n",
            "\n",
            "The loss value is :  0.08426542580127716 \n",
            "\n",
            "The loss value is :  0.017639918252825737 \n",
            "\n",
            "The loss value is :  0.15731798112392426 \n",
            "\n",
            "The loss value is :  0.023071108385920525 \n",
            "\n",
            "The loss value is :  0.164980947971344 \n",
            "\n",
            "The loss value is :  0.02233179844915867 \n",
            "\n",
            "The loss value is :  0.07568730413913727 \n",
            "\n",
            "The loss value is :  0.025081535801291466 \n",
            "\n",
            "The loss value is :  0.09883709996938705 \n",
            "\n",
            "The loss value is :  0.024919936433434486 \n",
            "\n",
            "The loss value is :  0.11680746078491211 \n",
            "\n",
            "The loss value is :  0.0023668911308050156 \n",
            "\n",
            "The loss value is :  0.09022702276706696 \n",
            "\n",
            "The loss value is :  -0.0049627311527729034 \n",
            "\n",
            "The loss value is :  0.17976780235767365 \n",
            "\n",
            "The minimum loss obtained is: 0.17976780235767365\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b541870> and its information value is:  tensor(0.0650)\n",
            "Target node: 63742\n",
            "1-hop neighbors of A: {63752, 63750, 63751}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {63748, 63749, 63753, 63754, 63755, 63740} \n",
            "\n",
            "The graph number 4265.0 contains: {63753, 63754, 63755, 63756, 63757, 63758, 63759, 63760, 63761, 63762, 63763, 63764, 63765, 63766, 63767, 63768, 63769, 63770, 63771, 63772, 63773, 63774}\n",
            "The loss value is :  0.036979131400585175 \n",
            "\n",
            "The loss value is :  0.15325598418712616 \n",
            "\n",
            "The loss value is :  0.012094350531697273 \n",
            "\n",
            "The loss value is :  0.15441854298114777 \n",
            "\n",
            "The loss value is :  0.02814321219921112 \n",
            "\n",
            "The loss value is :  0.1579018235206604 \n",
            "\n",
            "The loss value is :  0.016426578164100647 \n",
            "\n",
            "The loss value is :  0.15163962543010712 \n",
            "\n",
            "The loss value is :  0.03315137326717377 \n",
            "\n",
            "The loss value is :  0.1447591632604599 \n",
            "\n",
            "The loss value is :  0.014571400359272957 \n",
            "\n",
            "The loss value is :  0.13156679272651672 \n",
            "\n",
            "The loss value is :  0.0396142452955246 \n",
            "\n",
            "The loss value is :  0.15890417993068695 \n",
            "\n",
            "The loss value is :  0.027691664174199104 \n",
            "\n",
            "The loss value is :  0.15606313943862915 \n",
            "\n",
            "The minimum loss obtained is: 0.15606313943862915\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b540e80> and its information value is:  tensor(0.0447)\n",
            "Target node: 63774\n",
            "1-hop neighbors of A: {63767}\n",
            "2-hop neighbors of A: {63763, 63772, 63773}\n",
            "Out of neighborhood of A: {63753, 63754, 63755, 63756, 63757, 63758, 63759, 63760, 63761, 63762, 63764, 63765, 63766, 63768, 63769, 63770, 63771} \n",
            "\n",
            "The graph number 4266.0 contains: {63768, 63775, 63776, 63777, 63778, 63779, 63780, 63781, 63782, 63783, 63784, 63785, 63786, 63787, 63788, 63789, 63790, 63791, 63792}\n",
            "The loss value is :  0.03252353146672249 \n",
            "\n",
            "The loss value is :  0.10524451732635498 \n",
            "\n",
            "The loss value is :  0.02784379944205284 \n",
            "\n",
            "The loss value is :  0.17560388147830963 \n",
            "\n",
            "The loss value is :  0.039734676480293274 \n",
            "\n",
            "The loss value is :  0.1798836886882782 \n",
            "\n",
            "The loss value is :  0.012780693359673023 \n",
            "\n",
            "The loss value is :  0.10574809461832047 \n",
            "\n",
            "The loss value is :  0.016778208315372467 \n",
            "\n",
            "The loss value is :  0.13643911480903625 \n",
            "\n",
            "The loss value is :  0.03129824995994568 \n",
            "\n",
            "The loss value is :  0.1365155726671219 \n",
            "\n",
            "The loss value is :  0.011287675239145756 \n",
            "\n",
            "The loss value is :  0.13044553995132446 \n",
            "\n",
            "The loss value is :  0.02900063246488571 \n",
            "\n",
            "The loss value is :  0.15434826910495758 \n",
            "\n",
            "The minimum loss obtained is: 0.15434826910495758\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b5438e0> and its information value is:  tensor(0.0491)\n",
            "Target node: 63792\n",
            "1-hop neighbors of A: {63791}\n",
            "2-hop neighbors of A: {63787}\n",
            "Out of neighborhood of A: {63768, 63775, 63776, 63777, 63778, 63779, 63780, 63781, 63782, 63783, 63784, 63785, 63786, 63788, 63789, 63790} \n",
            "\n",
            "The graph number 4267.0 contains: {63784, 63788, 63791, 63792, 63793, 63794, 63795, 63796, 63797}\n",
            "The loss value is :  0.03469156473875046 \n",
            "\n",
            "The loss value is :  0.11727170646190643 \n",
            "\n",
            "The loss value is :  0.009809369221329689 \n",
            "\n",
            "The loss value is :  0.050383590161800385 \n",
            "\n",
            "The loss value is :  0.03640928864479065 \n",
            "\n",
            "The loss value is :  0.13150882720947266 \n",
            "\n",
            "The loss value is :  0.016763772815465927 \n",
            "\n",
            "The loss value is :  0.1585049033164978 \n",
            "\n",
            "The loss value is :  0.03871433809399605 \n",
            "\n",
            "The loss value is :  0.10223467648029327 \n",
            "\n",
            "The loss value is :  0.03164326772093773 \n",
            "\n",
            "The loss value is :  0.11174686253070831 \n",
            "\n",
            "The loss value is :  0.02090884931385517 \n",
            "\n",
            "The loss value is :  0.1676170378923416 \n",
            "\n",
            "The loss value is :  0.08821866661310196 \n",
            "\n",
            "The loss value is :  0.19645686447620392 \n",
            "\n",
            "The minimum loss obtained is: 0.19645686447620392\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b540e80> and its information value is:  tensor(0.0649)\n",
            "Target node: 63797\n",
            "1-hop neighbors of A: {63794}\n",
            "2-hop neighbors of A: {63792}\n",
            "Out of neighborhood of A: {63784, 63788, 63791, 63793, 63795, 63796} \n",
            "\n",
            "The graph number 4268.0 contains: {63808, 63809, 63810, 63811, 63812, 63813, 63781, 63784, 63785, 63786, 63788, 63789, 63790, 63793, 63794, 63795, 63796, 63797, 63798, 63799, 63800, 63801, 63802, 63803, 63804, 63805, 63806, 63807}\n",
            "The loss value is :  0.037158094346523285 \n",
            "\n",
            "The loss value is :  0.1632693111896515 \n",
            "\n",
            "The loss value is :  0.027042193338274956 \n",
            "\n",
            "The loss value is :  0.18384431302547455 \n",
            "\n",
            "The loss value is :  0.03686309978365898 \n",
            "\n",
            "The loss value is :  0.1858965903520584 \n",
            "\n",
            "The loss value is :  0.04406296834349632 \n",
            "\n",
            "The loss value is :  0.16096839308738708 \n",
            "\n",
            "The loss value is :  0.024282336235046387 \n",
            "\n",
            "The loss value is :  0.14759786427021027 \n",
            "\n",
            "The loss value is :  0.031091080978512764 \n",
            "\n",
            "The loss value is :  0.1424526572227478 \n",
            "\n",
            "The loss value is :  0.022082621231675148 \n",
            "\n",
            "The loss value is :  0.12971754372119904 \n",
            "\n",
            "The loss value is :  0.03579678386449814 \n",
            "\n",
            "The loss value is :  0.15151986479759216 \n",
            "\n",
            "The minimum loss obtained is: 0.15151986479759216\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b542200> and its information value is:  tensor(0.0426)\n",
            "Target node: 63807\n",
            "1-hop neighbors of A: {63795}\n",
            "2-hop neighbors of A: {63808, 63809}\n",
            "Out of neighborhood of A: {63810, 63811, 63812, 63813, 63781, 63784, 63785, 63786, 63788, 63789, 63790, 63793, 63794, 63796, 63797, 63798, 63799, 63800, 63801, 63802, 63803, 63804, 63805, 63806} \n",
            "\n",
            "The graph number 4269.0 contains: {63812, 63813, 63814, 63815, 63816, 63817, 63818, 63819, 63820, 63821, 63822, 63823, 63824, 63825, 63826, 63827}\n",
            "The loss value is :  0.016524869948625565 \n",
            "\n",
            "The loss value is :  0.13233716785907745 \n",
            "\n",
            "The loss value is :  0.02301594242453575 \n",
            "\n",
            "The loss value is :  0.12902776896953583 \n",
            "\n",
            "The loss value is :  0.02273767814040184 \n",
            "\n",
            "The loss value is :  0.08952812105417252 \n",
            "\n",
            "The loss value is :  0.03225580230355263 \n",
            "\n",
            "The loss value is :  0.10406911373138428 \n",
            "\n",
            "The loss value is :  0.024603499099612236 \n",
            "\n",
            "The loss value is :  0.17347514629364014 \n",
            "\n",
            "The loss value is :  0.04438576102256775 \n",
            "\n",
            "The loss value is :  0.10371769219636917 \n",
            "\n",
            "The loss value is :  -0.003578588366508484 \n",
            "\n",
            "The loss value is :  0.10220068693161011 \n",
            "\n",
            "The loss value is :  0.024171747267246246 \n",
            "\n",
            "The loss value is :  0.1557544320821762 \n",
            "\n",
            "The minimum loss obtained is: 0.1557544320821762\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b543e50> and its information value is:  tensor(0.0537)\n",
            "Target node: 63827\n",
            "1-hop neighbors of A: {63826}\n",
            "2-hop neighbors of A: {63825}\n",
            "Out of neighborhood of A: {63812, 63813, 63814, 63815, 63816, 63817, 63818, 63819, 63820, 63821, 63822, 63823, 63824} \n",
            "\n",
            "The graph number 4270.0 contains: {63815, 63817, 63818, 63820, 63821, 63822, 63823, 63824, 63825, 63827, 63831, 63832, 63833, 63834, 63835, 63836, 63837, 63838, 63839, 63840, 63841, 63842, 63843}\n",
            "The loss value is :  0.0065666548907756805 \n",
            "\n",
            "The loss value is :  0.16520771384239197 \n",
            "\n",
            "The loss value is :  0.025910060852766037 \n",
            "\n",
            "The loss value is :  0.15178509056568146 \n",
            "\n",
            "The loss value is :  0.03885258734226227 \n",
            "\n",
            "The loss value is :  0.15949590504169464 \n",
            "\n",
            "The loss value is :  0.0378580279648304 \n",
            "\n",
            "The loss value is :  0.15060153603553772 \n",
            "\n",
            "The loss value is :  0.031049935147166252 \n",
            "\n",
            "The loss value is :  0.13760438561439514 \n",
            "\n",
            "The loss value is :  0.018917415291070938 \n",
            "\n",
            "The loss value is :  0.15720686316490173 \n",
            "\n",
            "The loss value is :  0.032980777323246 \n",
            "\n",
            "The loss value is :  0.19035370647907257 \n",
            "\n",
            "The loss value is :  0.02998287044465542 \n",
            "\n",
            "The loss value is :  0.11851408332586288 \n",
            "\n",
            "The minimum loss obtained is: 0.11851408332586288\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b542aa0> and its information value is:  tensor(0.0655)\n",
            "Target node: 63843\n",
            "1-hop neighbors of A: {63827}\n",
            "2-hop neighbors of A: {63842}\n",
            "Out of neighborhood of A: {63815, 63817, 63818, 63820, 63821, 63822, 63823, 63824, 63825, 63831, 63832, 63833, 63834, 63835, 63836, 63837, 63838, 63839, 63840, 63841} \n",
            "\n",
            "The graph number 4271.0 contains: {63843, 63844, 63845, 63846, 63847, 63848, 63849, 63850, 63851, 63852, 63853, 63827, 63828, 63829, 63830}\n",
            "The loss value is :  -0.0005627907812595367 \n",
            "\n",
            "The loss value is :  0.16723664104938507 \n",
            "\n",
            "The loss value is :  0.026847289875149727 \n",
            "\n",
            "The loss value is :  0.14788617193698883 \n",
            "\n",
            "The loss value is :  0.020405028015375137 \n",
            "\n",
            "The loss value is :  0.1657479703426361 \n",
            "\n",
            "The loss value is :  0.01578032225370407 \n",
            "\n",
            "The loss value is :  0.15544505417346954 \n",
            "\n",
            "The loss value is :  0.04446876794099808 \n",
            "\n",
            "The loss value is :  0.12768179178237915 \n",
            "\n",
            "The loss value is :  0.028750792145729065 \n",
            "\n",
            "The loss value is :  0.12163145840167999 \n",
            "\n",
            "The loss value is :  0.022330252453684807 \n",
            "\n",
            "The loss value is :  0.08715224266052246 \n",
            "\n",
            "The loss value is :  0.04175759479403496 \n",
            "\n",
            "The loss value is :  0.12091978639364243 \n",
            "\n",
            "The minimum loss obtained is: 0.12091978639364243\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b541930> and its information value is:  tensor(0.0755)\n",
            "Target node: 63830\n",
            "1-hop neighbors of A: {63849}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {63843, 63844, 63845, 63846, 63847, 63848, 63850, 63851, 63852, 63853, 63827, 63828, 63829} \n",
            "\n",
            "The graph number 4272.0 contains: {63851, 63852, 63853, 63854, 63855, 63856, 63857, 63858, 63859, 63860, 63861, 63862, 63863, 63864, 63865, 63866, 63867, 63868}\n",
            "The loss value is :  0.03318444639444351 \n",
            "\n",
            "The loss value is :  0.15930277109146118 \n",
            "\n",
            "The loss value is :  0.005908414721488953 \n",
            "\n",
            "The loss value is :  0.1610165685415268 \n",
            "\n",
            "The loss value is :  0.024472417309880257 \n",
            "\n",
            "The loss value is :  0.12735570967197418 \n",
            "\n",
            "The loss value is :  0.020571695640683174 \n",
            "\n",
            "The loss value is :  0.14658905565738678 \n",
            "\n",
            "The loss value is :  0.0030900202691555023 \n",
            "\n",
            "The loss value is :  0.13982951641082764 \n",
            "\n",
            "The loss value is :  0.025438454002141953 \n",
            "\n",
            "The loss value is :  0.10919696092605591 \n",
            "\n",
            "The loss value is :  0.03297755494713783 \n",
            "\n",
            "The loss value is :  0.12580186128616333 \n",
            "\n",
            "The loss value is :  0.018528036773204803 \n",
            "\n",
            "The loss value is :  0.18165147304534912 \n",
            "\n",
            "The minimum loss obtained is: 0.18165147304534912\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b543fa0> and its information value is:  tensor(0.0391)\n",
            "Target node: 63868\n",
            "1-hop neighbors of A: {63866}\n",
            "2-hop neighbors of A: {63864}\n",
            "Out of neighborhood of A: {63851, 63852, 63853, 63854, 63855, 63856, 63857, 63858, 63859, 63860, 63861, 63862, 63863, 63865, 63867} \n",
            "\n",
            "The graph number 4273.0 contains: {63872, 63873, 63856, 63857, 63858, 63859, 63861, 63862, 63863, 63864, 63865, 63866, 63867, 63869, 63870, 63871}\n",
            "The loss value is :  0.01749613881111145 \n",
            "\n",
            "The loss value is :  0.13940301537513733 \n",
            "\n",
            "The loss value is :  0.003893895074725151 \n",
            "\n",
            "The loss value is :  0.12860411405563354 \n",
            "\n",
            "The loss value is :  0.0076626744121313095 \n",
            "\n",
            "The loss value is :  0.11424953490495682 \n",
            "\n",
            "The loss value is :  0.026170367375016212 \n",
            "\n",
            "The loss value is :  0.11512792110443115 \n",
            "\n",
            "The loss value is :  0.02826038748025894 \n",
            "\n",
            "The loss value is :  0.12005611509084702 \n",
            "\n",
            "The loss value is :  0.007464444264769554 \n",
            "\n",
            "The loss value is :  0.11886794865131378 \n",
            "\n",
            "The loss value is :  0.017223821952939034 \n",
            "\n",
            "The loss value is :  0.17580772936344147 \n",
            "\n",
            "The loss value is :  0.028902675956487656 \n",
            "\n",
            "The loss value is :  0.1492806226015091 \n",
            "\n",
            "The minimum loss obtained is: 0.1492806226015091\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b5437c0> and its information value is:  tensor(0.0569)\n",
            "Target node: 63871\n",
            "1-hop neighbors of A: {63867}\n",
            "2-hop neighbors of A: {63870}\n",
            "Out of neighborhood of A: {63872, 63873, 63856, 63857, 63858, 63859, 63861, 63862, 63863, 63864, 63865, 63866, 63869} \n",
            "\n",
            "The graph number 4274.0 contains: {63873, 63874, 63875, 63876, 63877, 63878, 63879, 63880, 63881, 63882, 63883, 63884, 63885, 63886, 63887, 63888, 63889, 63890, 63891, 63892, 63893, 63894, 63895, 63896, 63897, 63898, 63899, 63900, 63901, 63902, 63903, 63904, 63905, 63857, 63858, 63859, 63860, 63861, 63862, 63863}\n",
            "The loss value is :  0.03752781078219414 \n",
            "\n",
            "The loss value is :  0.1940053552389145 \n",
            "\n",
            "The loss value is :  0.029599744826555252 \n",
            "\n",
            "The loss value is :  0.16919207572937012 \n",
            "\n",
            "The loss value is :  0.04347642511129379 \n",
            "\n",
            "The loss value is :  0.1846398562192917 \n",
            "\n",
            "The loss value is :  0.033865734934806824 \n",
            "\n",
            "The loss value is :  0.1668524295091629 \n",
            "\n",
            "The loss value is :  0.03982941433787346 \n",
            "\n",
            "The loss value is :  0.19594654440879822 \n",
            "\n",
            "The loss value is :  0.02944149076938629 \n",
            "\n",
            "The loss value is :  0.1623275727033615 \n",
            "\n",
            "The loss value is :  0.03228317201137543 \n",
            "\n",
            "The loss value is :  0.18049658834934235 \n",
            "\n",
            "The loss value is :  0.032890383154153824 \n",
            "\n",
            "The loss value is :  0.18219390511512756 \n",
            "\n",
            "The minimum loss obtained is: 0.18219390511512756\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f9749f18460> and its information value is:  tensor(0.0258)\n",
            "Target node: 63863\n",
            "1-hop neighbors of A: {63880, 63881}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {63873, 63874, 63875, 63876, 63877, 63878, 63879, 63882, 63883, 63884, 63885, 63886, 63887, 63888, 63889, 63890, 63891, 63892, 63893, 63894, 63895, 63896, 63897, 63898, 63899, 63900, 63901, 63902, 63903, 63904, 63905, 63857, 63858, 63859, 63860, 63861, 63862} \n",
            "\n",
            "The graph number 4275.0 contains: {63893, 63894, 63895, 63896, 63897, 63899, 63900, 63905, 63906, 63907, 63908, 63909, 63910, 63911, 63912, 63913, 63914, 63915}\n",
            "The loss value is :  0.011915045790374279 \n",
            "\n",
            "The loss value is :  0.10559980571269989 \n",
            "\n",
            "The loss value is :  0.03629770129919052 \n",
            "\n",
            "The loss value is :  0.1553546041250229 \n",
            "\n",
            "The loss value is :  0.032591354101896286 \n",
            "\n",
            "The loss value is :  0.14089025557041168 \n",
            "\n",
            "The loss value is :  0.03508489206433296 \n",
            "\n",
            "The loss value is :  0.12757423520088196 \n",
            "\n",
            "The loss value is :  -0.003264784812927246 \n",
            "\n",
            "The loss value is :  0.14042364060878754 \n",
            "\n",
            "The loss value is :  0.021584905683994293 \n",
            "\n",
            "The loss value is :  0.1514204889535904 \n",
            "\n",
            "The loss value is :  0.016929006204009056 \n",
            "\n",
            "The loss value is :  0.14567165076732635 \n",
            "\n",
            "The loss value is :  0.012041778303682804 \n",
            "\n",
            "The loss value is :  0.1381937861442566 \n",
            "\n",
            "The minimum loss obtained is: 0.1381937861442566\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b543c10> and its information value is:  tensor(0.0590)\n",
            "Target node: 63915\n",
            "1-hop neighbors of A: {63900}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {63893, 63894, 63895, 63896, 63897, 63899, 63905, 63906, 63907, 63908, 63909, 63910, 63911, 63912, 63913, 63914} \n",
            "\n",
            "The graph number 4276.0 contains: {63916, 63917, 63918, 63919, 63920, 63921, 63922, 63923, 63924, 63925, 63926, 63927, 63928, 63929, 63930, 63931, 63932, 63933, 63934}\n",
            "The loss value is :  0.036574386060237885 \n",
            "\n",
            "The loss value is :  0.17062324285507202 \n",
            "\n",
            "The loss value is :  0.020007947459816933 \n",
            "\n",
            "The loss value is :  0.12688349187374115 \n",
            "\n",
            "The loss value is :  0.044461410492658615 \n",
            "\n",
            "The loss value is :  0.1265368014574051 \n",
            "\n",
            "The loss value is :  0.03188077732920647 \n",
            "\n",
            "The loss value is :  0.14547394216060638 \n",
            "\n",
            "The loss value is :  0.03221196308732033 \n",
            "\n",
            "The loss value is :  0.1402948498725891 \n",
            "\n",
            "The loss value is :  0.01364089921116829 \n",
            "\n",
            "The loss value is :  0.18426749110221863 \n",
            "\n",
            "The loss value is :  0.015731483697891235 \n",
            "\n",
            "The loss value is :  0.14351753890514374 \n",
            "\n",
            "The loss value is :  0.03082471713423729 \n",
            "\n",
            "The loss value is :  0.15297754108905792 \n",
            "\n",
            "The minimum loss obtained is: 0.15297754108905792\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b540c70> and its information value is:  tensor(0.0498)\n",
            "Target node: 63934\n",
            "1-hop neighbors of A: {63925}\n",
            "2-hop neighbors of A: {63922}\n",
            "Out of neighborhood of A: {63916, 63917, 63918, 63919, 63920, 63921, 63923, 63924, 63926, 63927, 63928, 63929, 63930, 63931, 63932, 63933} \n",
            "\n",
            "The graph number 4277.0 contains: {63936, 63937, 63938, 63939, 63940, 63941, 63942, 63943, 63944, 63945, 63946, 63947, 63948, 63949, 63950, 63951, 63952, 63935}\n",
            "The loss value is :  0.024132654070854187 \n",
            "\n",
            "The loss value is :  0.11412189900875092 \n",
            "\n",
            "The loss value is :  0.013305285014212132 \n",
            "\n",
            "The loss value is :  0.13150209188461304 \n",
            "\n",
            "The loss value is :  0.03287370130419731 \n",
            "\n",
            "The loss value is :  0.1264793872833252 \n",
            "\n",
            "The loss value is :  0.02586289308965206 \n",
            "\n",
            "The loss value is :  0.13731694221496582 \n",
            "\n",
            "The loss value is :  -0.007492745295166969 \n",
            "\n",
            "The loss value is :  0.14531254768371582 \n",
            "\n",
            "The loss value is :  0.013172492384910583 \n",
            "\n",
            "The loss value is :  0.17168256640434265 \n",
            "\n",
            "The loss value is :  0.011989301070570946 \n",
            "\n",
            "The loss value is :  0.13214612007141113 \n",
            "\n",
            "The loss value is :  0.027958745136857033 \n",
            "\n",
            "The loss value is :  0.16196420788764954 \n",
            "\n",
            "The minimum loss obtained is: 0.16196420788764954\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b542830> and its information value is:  tensor(0.0471)\n",
            "Target node: 63935\n",
            "1-hop neighbors of A: {63936, 63937, 63938}\n",
            "2-hop neighbors of A: {63939, 63940, 63941, 63942, 63943}\n",
            "Out of neighborhood of A: {63944, 63945, 63946, 63947, 63948, 63949, 63950, 63951, 63952} \n",
            "\n",
            "The graph number 4278.0 contains: {63938, 63941, 63943, 63944, 63946, 63949, 63951, 63952, 63953, 63954, 63955, 63956, 63957, 63958}\n",
            "The loss value is :  -0.004013927653431892 \n",
            "\n",
            "The loss value is :  0.12513625621795654 \n",
            "\n",
            "The loss value is :  0.029621407389640808 \n",
            "\n",
            "The loss value is :  0.12603552639484406 \n",
            "\n",
            "The loss value is :  0.03019093908369541 \n",
            "\n",
            "The loss value is :  0.12785379588603973 \n",
            "\n",
            "The loss value is :  -0.006771884858608246 \n",
            "\n",
            "The loss value is :  0.15180468559265137 \n",
            "\n",
            "The loss value is :  -0.007505564019083977 \n",
            "\n",
            "The loss value is :  0.1314891129732132 \n",
            "\n",
            "The loss value is :  0.000753842294216156 \n",
            "\n",
            "The loss value is :  0.10747960209846497 \n",
            "\n",
            "The loss value is :  0.022216983139514923 \n",
            "\n",
            "The loss value is :  0.12334675341844559 \n",
            "\n",
            "The loss value is :  0.0074203405529260635 \n",
            "\n",
            "The loss value is :  0.10361725091934204 \n",
            "\n",
            "The minimum loss obtained is: 0.10361725091934204\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b541b10> and its information value is:  tensor(0.0902)\n",
            "Target node: 63958\n",
            "1-hop neighbors of A: {63938}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {63941, 63943, 63944, 63946, 63949, 63951, 63952, 63953, 63954, 63955, 63956, 63957} \n",
            "\n",
            "The graph number 4279.0 contains: {63940, 63941, 63942, 63947, 63951, 63959, 63960, 63961, 63962, 63963, 63964, 63965, 63966, 63967}\n",
            "The loss value is :  0.004720456898212433 \n",
            "\n",
            "The loss value is :  0.12237490713596344 \n",
            "\n",
            "The loss value is :  0.02415456250309944 \n",
            "\n",
            "The loss value is :  0.13733384013175964 \n",
            "\n",
            "The loss value is :  0.023735323920845985 \n",
            "\n",
            "The loss value is :  0.1643718183040619 \n",
            "\n",
            "The loss value is :  0.02395106852054596 \n",
            "\n",
            "The loss value is :  0.1677526831626892 \n",
            "\n",
            "The loss value is :  0.010784772224724293 \n",
            "\n",
            "The loss value is :  0.11968620866537094 \n",
            "\n",
            "The loss value is :  0.03101573884487152 \n",
            "\n",
            "The loss value is :  0.2032683789730072 \n",
            "\n",
            "The loss value is :  0.023792792111635208 \n",
            "\n",
            "The loss value is :  0.12517383694648743 \n",
            "\n",
            "The loss value is :  0.012345184572041035 \n",
            "\n",
            "The loss value is :  0.11785104870796204 \n",
            "\n",
            "The minimum loss obtained is: 0.11785104870796204\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b540670> and its information value is:  tensor(0.0802)\n",
            "Target node: 63967\n",
            "1-hop neighbors of A: {63951}\n",
            "2-hop neighbors of A: {63966}\n",
            "Out of neighborhood of A: {63940, 63941, 63942, 63947, 63959, 63960, 63961, 63962, 63963, 63964, 63965} \n",
            "\n",
            "The graph number 4280.0 contains: {63968, 63969, 63970, 63971, 63972, 63973, 63951, 63952, 63953, 63955, 63956, 63967}\n",
            "The loss value is :  0.021161727607250214 \n",
            "\n",
            "The loss value is :  0.1191122755408287 \n",
            "\n",
            "The loss value is :  0.008407345041632652 \n",
            "\n",
            "The loss value is :  0.1363012045621872 \n",
            "\n",
            "The loss value is :  0.007848248817026615 \n",
            "\n",
            "The loss value is :  0.10614714026451111 \n",
            "\n",
            "The loss value is :  0.02980033867061138 \n",
            "\n",
            "The loss value is :  0.12628550827503204 \n",
            "\n",
            "The loss value is :  0.016183100640773773 \n",
            "\n",
            "The loss value is :  0.08065416663885117 \n",
            "\n",
            "The loss value is :  0.03594487905502319 \n",
            "\n",
            "The loss value is :  0.10958437621593475 \n",
            "\n",
            "The loss value is :  0.032965872436761856 \n",
            "\n",
            "The loss value is :  0.14012545347213745 \n",
            "\n",
            "The loss value is :  0.01705782860517502 \n",
            "\n",
            "The loss value is :  0.13861727714538574 \n",
            "\n",
            "The minimum loss obtained is: 0.13861727714538574\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b5423e0> and its information value is:  tensor(0.0748)\n",
            "Target node: 63967\n",
            "1-hop neighbors of A: {63951}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {63968, 63969, 63970, 63971, 63972, 63973, 63952, 63953, 63955, 63956} \n",
            "\n",
            "The graph number 4281.0 contains: {63956, 63957, 63973, 63974, 63975, 63976, 63977, 63978, 63979, 63980, 63981, 63982, 63983, 63984, 63985, 63986, 63987, 63988, 63989, 63990, 63991, 63992, 63993, 63994, 63995, 63996, 63997}\n",
            "The loss value is :  0.02607090398669243 \n",
            "\n",
            "The loss value is :  0.1623436063528061 \n",
            "\n",
            "The loss value is :  0.03404403477907181 \n",
            "\n",
            "The loss value is :  0.18178671598434448 \n",
            "\n",
            "The loss value is :  0.03531496226787567 \n",
            "\n",
            "The loss value is :  0.16144025325775146 \n",
            "\n",
            "The loss value is :  0.026652295142412186 \n",
            "\n",
            "The loss value is :  0.1742285192012787 \n",
            "\n",
            "The loss value is :  0.023263029754161835 \n",
            "\n",
            "The loss value is :  0.15328513085842133 \n",
            "\n",
            "The loss value is :  0.031885940581560135 \n",
            "\n",
            "The loss value is :  0.15282267332077026 \n",
            "\n",
            "The loss value is :  0.025877414271235466 \n",
            "\n",
            "The loss value is :  0.16084811091423035 \n",
            "\n",
            "The loss value is :  0.03768329694867134 \n",
            "\n",
            "The loss value is :  0.16015519201755524 \n",
            "\n",
            "The minimum loss obtained is: 0.16015519201755524\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b541d50> and its information value is:  tensor(0.0391)\n",
            "Target node: 63997\n",
            "1-hop neighbors of A: {63981}\n",
            "2-hop neighbors of A: {63977, 63987}\n",
            "Out of neighborhood of A: {63956, 63957, 63973, 63974, 63975, 63976, 63978, 63979, 63980, 63982, 63983, 63984, 63985, 63986, 63988, 63989, 63990, 63991, 63992, 63993, 63994, 63995, 63996} \n",
            "\n",
            "The graph number 4282.0 contains: {64000, 64001, 64002, 64003, 64004, 64005, 64006, 64007, 64008, 64009, 64010, 64011, 64012, 64013, 64014, 64015, 64016, 64017, 64018, 63983, 63986, 63987, 63988, 63989, 63990, 63991, 63992, 63993, 63994, 63995, 63998, 63999}\n",
            "The loss value is :  0.031722310930490494 \n",
            "\n",
            "The loss value is :  0.17481453716754913 \n",
            "\n",
            "The loss value is :  0.023875128477811813 \n",
            "\n",
            "The loss value is :  0.16977635025978088 \n",
            "\n",
            "The loss value is :  0.031671375036239624 \n",
            "\n",
            "The loss value is :  0.19724494218826294 \n",
            "\n",
            "The loss value is :  0.03673873469233513 \n",
            "\n",
            "The loss value is :  0.17173230648040771 \n",
            "\n",
            "The loss value is :  0.032934755086898804 \n",
            "\n",
            "The loss value is :  0.16402815282344818 \n",
            "\n",
            "The loss value is :  0.02094591036438942 \n",
            "\n",
            "The loss value is :  0.18370147049427032 \n",
            "\n",
            "The loss value is :  0.030954863876104355 \n",
            "\n",
            "The loss value is :  0.16453517973423004 \n",
            "\n",
            "The loss value is :  0.036093443632125854 \n",
            "\n",
            "The loss value is :  0.1706133335828781 \n",
            "\n",
            "The minimum loss obtained is: 0.1706133335828781\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b542e60> and its information value is:  tensor(0.0323)\n",
            "Target node: 63999\n",
            "1-hop neighbors of A: {63986}\n",
            "2-hop neighbors of A: {64000}\n",
            "Out of neighborhood of A: {64001, 64002, 64003, 64004, 64005, 64006, 64007, 64008, 64009, 64010, 64011, 64012, 64013, 64014, 64015, 64016, 64017, 64018, 63983, 63987, 63988, 63989, 63990, 63991, 63992, 63993, 63994, 63995, 63998} \n",
            "\n",
            "The graph number 4283.0 contains: {64015, 64017, 64018, 64019, 64020, 64021, 64022, 64023, 64024, 64025}\n",
            "The loss value is :  0.02565426006913185 \n",
            "\n",
            "The loss value is :  0.07749475538730621 \n",
            "\n",
            "The loss value is :  -0.002491820603609085 \n",
            "\n",
            "The loss value is :  0.07780555635690689 \n",
            "\n",
            "The loss value is :  0.012645692564547062 \n",
            "\n",
            "The loss value is :  0.07174480706453323 \n",
            "\n",
            "The loss value is :  -0.0005320105701684952 \n",
            "\n",
            "The loss value is :  0.12331754714250565 \n",
            "\n",
            "The loss value is :  0.05370257794857025 \n",
            "\n",
            "The loss value is :  0.13057450950145721 \n",
            "\n",
            "The loss value is :  0.04936712980270386 \n",
            "\n",
            "The loss value is :  0.10879473388195038 \n",
            "\n",
            "The loss value is :  0.04208512604236603 \n",
            "\n",
            "The loss value is :  0.056612540036439896 \n",
            "\n",
            "The loss value is :  0.023963989689946175 \n",
            "\n",
            "The loss value is :  0.09828129410743713 \n",
            "\n",
            "The minimum loss obtained is: 0.09828129410743713\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b540d30> and its information value is:  tensor(0.1119)\n",
            "Target node: 64025\n",
            "1-hop neighbors of A: {64019}\n",
            "2-hop neighbors of A: {64024, 64017}\n",
            "Out of neighborhood of A: {64015, 64018, 64020, 64021, 64022, 64023} \n",
            "\n",
            "The graph number 4284.0 contains: {64020, 64021, 64024, 64025, 64026, 64027, 64028, 64029, 64030, 64031, 64032, 64033, 64034, 64035, 64036, 64037, 64038}\n",
            "The loss value is :  0.019234932959079742 \n",
            "\n",
            "The loss value is :  0.12479465454816818 \n",
            "\n",
            "The loss value is :  0.027283435687422752 \n",
            "\n",
            "The loss value is :  0.1502300500869751 \n",
            "\n",
            "The loss value is :  0.020634857937693596 \n",
            "\n",
            "The loss value is :  0.1412012279033661 \n",
            "\n",
            "The loss value is :  0.030124004930257797 \n",
            "\n",
            "The loss value is :  0.11562288552522659 \n",
            "\n",
            "The loss value is :  0.04162817448377609 \n",
            "\n",
            "The loss value is :  0.16157761216163635 \n",
            "\n",
            "The loss value is :  0.009682361036539078 \n",
            "\n",
            "The loss value is :  0.1115010529756546 \n",
            "\n",
            "The loss value is :  0.02008650451898575 \n",
            "\n",
            "The loss value is :  0.16368521749973297 \n",
            "\n",
            "The loss value is :  0.036766525357961655 \n",
            "\n",
            "The loss value is :  0.15948912501335144 \n",
            "\n",
            "The minimum loss obtained is: 0.15948912501335144\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b542350> and its information value is:  tensor(0.0499)\n",
            "Target node: 64038\n",
            "1-hop neighbors of A: {64036}\n",
            "2-hop neighbors of A: {64032}\n",
            "Out of neighborhood of A: {64020, 64021, 64024, 64025, 64026, 64027, 64028, 64029, 64030, 64031, 64033, 64034, 64035, 64037} \n",
            "\n",
            "The graph number 4285.0 contains: {64015, 64016, 64020, 64021, 64022, 64023, 64025, 64026, 64029, 64034, 64036, 64037, 64038, 64039, 64040, 64041, 64042, 64043, 64044, 64045, 64046, 64047, 64048, 64049}\n",
            "The loss value is :  0.029835190623998642 \n",
            "\n",
            "The loss value is :  0.16198131442070007 \n",
            "\n",
            "The loss value is :  0.02418667823076248 \n",
            "\n",
            "The loss value is :  0.16569343209266663 \n",
            "\n",
            "The loss value is :  0.028843902051448822 \n",
            "\n",
            "The loss value is :  0.14545178413391113 \n",
            "\n",
            "The loss value is :  0.014422891661524773 \n",
            "\n",
            "The loss value is :  0.13260573148727417 \n",
            "\n",
            "The loss value is :  0.03011399880051613 \n",
            "\n",
            "The loss value is :  0.16741353273391724 \n",
            "\n",
            "The loss value is :  0.04186656326055527 \n",
            "\n",
            "The loss value is :  0.16799962520599365 \n",
            "\n",
            "The loss value is :  0.03918292745947838 \n",
            "\n",
            "The loss value is :  0.16571220755577087 \n",
            "\n",
            "The loss value is :  0.03584250807762146 \n",
            "\n",
            "The loss value is :  0.180509552359581 \n",
            "\n",
            "The minimum loss obtained is: 0.180509552359581\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b5412d0> and its information value is:  tensor(0.0329)\n",
            "Target node: 64049\n",
            "1-hop neighbors of A: {64022}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {64015, 64016, 64020, 64021, 64023, 64025, 64026, 64029, 64034, 64036, 64037, 64038, 64039, 64040, 64041, 64042, 64043, 64044, 64045, 64046, 64047, 64048} \n",
            "\n",
            "The graph number 4286.0 contains: {64022, 64023, 64025, 64027, 64028, 64030, 64037, 64039, 64040, 64041, 64049, 64050, 64051, 64052, 64053, 64054, 64055, 64056, 64057, 64058, 64059, 64060, 64061}\n",
            "The loss value is :  0.021187007427215576 \n",
            "\n",
            "The loss value is :  0.15761566162109375 \n",
            "\n",
            "The loss value is :  0.03429771214723587 \n",
            "\n",
            "The loss value is :  0.13773185014724731 \n",
            "\n",
            "The loss value is :  0.02792903408408165 \n",
            "\n",
            "The loss value is :  0.1707945019006729 \n",
            "\n",
            "The loss value is :  0.03357512876391411 \n",
            "\n",
            "The loss value is :  0.1514980047941208 \n",
            "\n",
            "The loss value is :  0.029300421476364136 \n",
            "\n",
            "The loss value is :  0.15995782613754272 \n",
            "\n",
            "The loss value is :  0.039244502782821655 \n",
            "\n",
            "The loss value is :  0.14561744034290314 \n",
            "\n",
            "The loss value is :  0.009568987414240837 \n",
            "\n",
            "The loss value is :  0.16929592192173004 \n",
            "\n",
            "The loss value is :  0.02703319862484932 \n",
            "\n",
            "The loss value is :  0.17389802634716034 \n",
            "\n",
            "The minimum loss obtained is: 0.17389802634716034\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b5418a0> and its information value is:  tensor(0.0362)\n",
            "Target node: 64061\n",
            "1-hop neighbors of A: {64041}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {64022, 64023, 64025, 64027, 64028, 64030, 64037, 64039, 64040, 64049, 64050, 64051, 64052, 64053, 64054, 64055, 64056, 64057, 64058, 64059, 64060} \n",
            "\n",
            "The graph number 4287.0 contains: {64064, 64065, 64066, 64067, 64068, 64069, 64070, 64071, 64072, 64073, 64074, 64075, 64076, 64062, 64063}\n",
            "The loss value is :  0.010382824577391148 \n",
            "\n",
            "The loss value is :  0.1308566927909851 \n",
            "\n",
            "The loss value is :  -4.6644359827041626e-05 \n",
            "\n",
            "The loss value is :  0.1459973305463791 \n",
            "\n",
            "The loss value is :  0.029252881184220314 \n",
            "\n",
            "The loss value is :  0.13840870559215546 \n",
            "\n",
            "The loss value is :  0.0372491180896759 \n",
            "\n",
            "The loss value is :  0.13120785355567932 \n",
            "\n",
            "The loss value is :  0.02330262027680874 \n",
            "\n",
            "The loss value is :  0.11412693560123444 \n",
            "\n",
            "The loss value is :  0.007272811606526375 \n",
            "\n",
            "The loss value is :  0.11261789500713348 \n",
            "\n",
            "The loss value is :  0.026252754032611847 \n",
            "\n",
            "The loss value is :  0.15289530158042908 \n",
            "\n",
            "The loss value is :  -0.027038127183914185 \n",
            "\n",
            "The loss value is :  0.14308884739875793 \n",
            "\n",
            "The minimum loss obtained is: 0.14308884739875793\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b541330> and its information value is:  tensor(0.0624)\n",
            "Target node: 64063\n",
            "1-hop neighbors of A: {64066, 64067, 64062}\n",
            "2-hop neighbors of A: {64064, 64065, 64068, 64072, 64074, 64075, 64076}\n",
            "Out of neighborhood of A: {64073, 64069, 64070, 64071} \n",
            "\n",
            "The graph number 4288.0 contains: {64067, 64068, 64069, 64070, 64071, 64072, 64073, 64076, 64077, 64078, 64079, 64080, 64081, 64082, 64083, 64084, 64085, 64086, 64087}\n",
            "The loss value is :  0.014930490404367447 \n",
            "\n",
            "The loss value is :  0.13614130020141602 \n",
            "\n",
            "The loss value is :  0.031403157860040665 \n",
            "\n",
            "The loss value is :  0.12824499607086182 \n",
            "\n",
            "The loss value is :  0.03830593079328537 \n",
            "\n",
            "The loss value is :  0.13412676751613617 \n",
            "\n",
            "The loss value is :  0.023205090314149857 \n",
            "\n",
            "The loss value is :  0.14028383791446686 \n",
            "\n",
            "The loss value is :  0.02057364396750927 \n",
            "\n",
            "The loss value is :  0.10925512760877609 \n",
            "\n",
            "The loss value is :  0.02713717892765999 \n",
            "\n",
            "The loss value is :  0.133425772190094 \n",
            "\n",
            "The loss value is :  0.024864934384822845 \n",
            "\n",
            "The loss value is :  0.1582314521074295 \n",
            "\n",
            "The loss value is :  0.038284722715616226 \n",
            "\n",
            "The loss value is :  0.1493075042963028 \n",
            "\n",
            "The minimum loss obtained is: 0.1493075042963028\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b5411b0> and its information value is:  tensor(0.0516)\n",
            "Target node: 64087\n",
            "1-hop neighbors of A: {64084}\n",
            "2-hop neighbors of A: {64085, 64086}\n",
            "Out of neighborhood of A: {64067, 64068, 64069, 64070, 64071, 64072, 64073, 64076, 64077, 64078, 64079, 64080, 64081, 64082, 64083} \n",
            "\n",
            "The graph number 4289.0 contains: {64084, 64085, 64086, 64087, 64088, 64089, 64090, 64091, 64092, 64093, 64094, 64095, 64096, 64097, 64098, 64099, 64100}\n",
            "The loss value is :  0.04576655849814415 \n",
            "\n",
            "The loss value is :  0.12127814441919327 \n",
            "\n",
            "The loss value is :  0.020625993609428406 \n",
            "\n",
            "The loss value is :  0.13232551515102386 \n",
            "\n",
            "The loss value is :  -0.008809784427285194 \n",
            "\n",
            "The loss value is :  0.12946124374866486 \n",
            "\n",
            "The loss value is :  0.022588875144720078 \n",
            "\n",
            "The loss value is :  0.12835583090782166 \n",
            "\n",
            "The loss value is :  0.02610982581973076 \n",
            "\n",
            "The loss value is :  0.0989687591791153 \n",
            "\n",
            "The loss value is :  0.03065910004079342 \n",
            "\n",
            "The loss value is :  0.18516060709953308 \n",
            "\n",
            "The loss value is :  0.024214215576648712 \n",
            "\n",
            "The loss value is :  0.13949480652809143 \n",
            "\n",
            "The loss value is :  0.04350237175822258 \n",
            "\n",
            "The loss value is :  0.13182397186756134 \n",
            "\n",
            "The minimum loss obtained is: 0.13182397186756134\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b541a50> and its information value is:  tensor(0.0644)\n",
            "Target node: 64100\n",
            "1-hop neighbors of A: {64098}\n",
            "2-hop neighbors of A: {64099}\n",
            "Out of neighborhood of A: {64084, 64085, 64086, 64087, 64088, 64089, 64090, 64091, 64092, 64093, 64094, 64095, 64096, 64097} \n",
            "\n",
            "The graph number 4290.0 contains: {64098, 64099, 64100, 64101, 64102, 64103, 64104, 64105, 64106, 64107, 64108, 64109, 64110, 64111}\n",
            "The loss value is :  0.036781515926122665 \n",
            "\n",
            "The loss value is :  0.11787949502468109 \n",
            "\n",
            "The loss value is :  0.017972441390156746 \n",
            "\n",
            "The loss value is :  0.10660549253225327 \n",
            "\n",
            "The loss value is :  0.024237241595983505 \n",
            "\n",
            "The loss value is :  0.10407577455043793 \n",
            "\n",
            "The loss value is :  0.0023476071655750275 \n",
            "\n",
            "The loss value is :  0.12845028936862946 \n",
            "\n",
            "The loss value is :  0.014867517165839672 \n",
            "\n",
            "The loss value is :  0.11544033885002136 \n",
            "\n",
            "The loss value is :  0.011002245359122753 \n",
            "\n",
            "The loss value is :  0.12978185713291168 \n",
            "\n",
            "The loss value is :  0.04196253418922424 \n",
            "\n",
            "The loss value is :  0.12699849903583527 \n",
            "\n",
            "The loss value is :  0.016270404681563377 \n",
            "\n",
            "The loss value is :  0.1328359693288803 \n",
            "\n",
            "The minimum loss obtained is: 0.1328359693288803\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b5428c0> and its information value is:  tensor(0.0709)\n",
            "Target node: 64111\n",
            "1-hop neighbors of A: {64104}\n",
            "2-hop neighbors of A: {64106, 64100}\n",
            "Out of neighborhood of A: {64098, 64099, 64101, 64102, 64103, 64105, 64107, 64108, 64109, 64110} \n",
            "\n",
            "The graph number 4291.0 contains: {64104, 64105, 64106, 64107, 64108, 64111, 64112, 64113, 64114, 64115, 64116, 64117, 64118, 64119, 64120, 64121, 64122, 64123, 64124, 64125, 64126}\n",
            "The loss value is :  0.021081920713186264 \n",
            "\n",
            "The loss value is :  0.16162578761577606 \n",
            "\n",
            "The loss value is :  0.013043422251939774 \n",
            "\n",
            "The loss value is :  0.1498344987630844 \n",
            "\n",
            "The loss value is :  0.02286970615386963 \n",
            "\n",
            "The loss value is :  0.136563241481781 \n",
            "\n",
            "The loss value is :  0.013728562742471695 \n",
            "\n",
            "The loss value is :  0.14254030585289001 \n",
            "\n",
            "The loss value is :  0.029545078054070473 \n",
            "\n",
            "The loss value is :  0.1687903106212616 \n",
            "\n",
            "The loss value is :  0.01024622656404972 \n",
            "\n",
            "The loss value is :  0.16734765470027924 \n",
            "\n",
            "The loss value is :  0.024828922003507614 \n",
            "\n",
            "The loss value is :  0.15951907634735107 \n",
            "\n",
            "The loss value is :  0.020834919065237045 \n",
            "\n",
            "The loss value is :  0.13144074380397797 \n",
            "\n",
            "The minimum loss obtained is: 0.13144074380397797\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b541fc0> and its information value is:  tensor(0.0589)\n",
            "Target node: 64126\n",
            "1-hop neighbors of A: {64123}\n",
            "2-hop neighbors of A: {64121}\n",
            "Out of neighborhood of A: {64104, 64105, 64106, 64107, 64108, 64111, 64112, 64113, 64114, 64115, 64116, 64117, 64118, 64119, 64120, 64122, 64124, 64125} \n",
            "\n",
            "The graph number 4292.0 contains: {64128, 64129, 64130, 64131, 64132, 64133, 64134, 64118, 64119, 64121, 64123, 64124, 64126, 64127}\n",
            "The loss value is :  0.0066160354763269424 \n",
            "\n",
            "The loss value is :  0.14607900381088257 \n",
            "\n",
            "The loss value is :  0.0130500802770257 \n",
            "\n",
            "The loss value is :  0.10210492461919785 \n",
            "\n",
            "The loss value is :  0.008840431459248066 \n",
            "\n",
            "The loss value is :  0.15344016253948212 \n",
            "\n",
            "The loss value is :  0.02089284360408783 \n",
            "\n",
            "The loss value is :  0.10086973756551743 \n",
            "\n",
            "The loss value is :  0.03146304190158844 \n",
            "\n",
            "The loss value is :  0.1321900486946106 \n",
            "\n",
            "The loss value is :  0.0024461299180984497 \n",
            "\n",
            "The loss value is :  0.15323922038078308 \n",
            "\n",
            "The loss value is :  0.004427758976817131 \n",
            "\n",
            "The loss value is :  0.12116578966379166 \n",
            "\n",
            "The loss value is :  0.013829702511429787 \n",
            "\n",
            "The loss value is :  0.11207413673400879 \n",
            "\n",
            "The minimum loss obtained is: 0.11207413673400879\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b540f40> and its information value is:  tensor(0.0841)\n",
            "Target node: 64127\n",
            "1-hop neighbors of A: {64123}\n",
            "2-hop neighbors of A: {64126}\n",
            "Out of neighborhood of A: {64128, 64129, 64130, 64131, 64132, 64133, 64134, 64118, 64119, 64121, 64124} \n",
            "\n",
            "The graph number 4293.0 contains: {64135, 64136, 64137, 64138, 64139, 64140, 64141, 64142, 64143, 64144, 64145, 64146, 64147, 64148, 64149, 64150, 64151, 64152, 64153, 64124, 64127}\n",
            "The loss value is :  0.04191408306360245 \n",
            "\n",
            "The loss value is :  0.143925741314888 \n",
            "\n",
            "The loss value is :  0.02375624142587185 \n",
            "\n",
            "The loss value is :  0.15273922681808472 \n",
            "\n",
            "The loss value is :  0.031910259276628494 \n",
            "\n",
            "The loss value is :  0.15978696942329407 \n",
            "\n",
            "The loss value is :  0.017562951892614365 \n",
            "\n",
            "The loss value is :  0.1551211178302765 \n",
            "\n",
            "The loss value is :  0.034147508442401886 \n",
            "\n",
            "The loss value is :  0.11763856559991837 \n",
            "\n",
            "The loss value is :  0.0176188163459301 \n",
            "\n",
            "The loss value is :  0.17381103336811066 \n",
            "\n",
            "The loss value is :  0.037097495049238205 \n",
            "\n",
            "The loss value is :  0.13500137627124786 \n",
            "\n",
            "The loss value is :  0.01715174689888954 \n",
            "\n",
            "The loss value is :  0.1556979864835739 \n",
            "\n",
            "The minimum loss obtained is: 0.1556979864835739\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b543490> and its information value is:  tensor(0.0460)\n",
            "Target node: 64127\n",
            "1-hop neighbors of A: {64137}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {64135, 64136, 64138, 64139, 64140, 64141, 64142, 64143, 64144, 64145, 64146, 64147, 64148, 64149, 64150, 64151, 64152, 64153, 64124} \n",
            "\n",
            "The graph number 4294.0 contains: {64144, 64145, 64146, 64147, 64148, 64149, 64150, 64151, 64152, 64153, 64154, 64155, 64156, 64157, 64158, 64159, 64160, 64161, 64162, 64163, 64164, 64165, 64166, 64167, 64168, 64169}\n",
            "The loss value is :  0.027219215407967567 \n",
            "\n",
            "The loss value is :  0.15466400980949402 \n",
            "\n",
            "The loss value is :  0.01965084858238697 \n",
            "\n",
            "The loss value is :  0.14679363369941711 \n",
            "\n",
            "The loss value is :  0.03227299824357033 \n",
            "\n",
            "The loss value is :  0.17189688980579376 \n",
            "\n",
            "The loss value is :  0.041086066514253616 \n",
            "\n",
            "The loss value is :  0.15446950495243073 \n",
            "\n",
            "The loss value is :  0.021109819412231445 \n",
            "\n",
            "The loss value is :  0.17709477245807648 \n",
            "\n",
            "The loss value is :  0.03758997470140457 \n",
            "\n",
            "The loss value is :  0.19175595045089722 \n",
            "\n",
            "The loss value is :  0.036360468715429306 \n",
            "\n",
            "The loss value is :  0.16945193707942963 \n",
            "\n",
            "The loss value is :  0.029464110732078552 \n",
            "\n",
            "The loss value is :  0.15959666669368744 \n",
            "\n",
            "The minimum loss obtained is: 0.15959666669368744\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b542680> and its information value is:  tensor(0.0399)\n",
            "Target node: 64169\n",
            "1-hop neighbors of A: {64151}\n",
            "2-hop neighbors of A: {64152}\n",
            "Out of neighborhood of A: {64144, 64145, 64146, 64147, 64148, 64149, 64150, 64153, 64154, 64155, 64156, 64157, 64158, 64159, 64160, 64161, 64162, 64163, 64164, 64165, 64166, 64167, 64168} \n",
            "\n",
            "The graph number 4295.0 contains: {64151, 64169, 64170, 64171, 64172, 64173, 64174, 64175, 64176, 64177, 64178, 64179, 64180, 64181, 64182, 64183, 64184, 64185, 64186, 64187, 64188, 64189}\n",
            "The loss value is :  0.033612001687288284 \n",
            "\n",
            "The loss value is :  0.13024021685123444 \n",
            "\n",
            "The loss value is :  0.028513437137007713 \n",
            "\n",
            "The loss value is :  0.16591046750545502 \n",
            "\n",
            "The loss value is :  0.02906743809580803 \n",
            "\n",
            "The loss value is :  0.18305601179599762 \n",
            "\n",
            "The loss value is :  0.0282553993165493 \n",
            "\n",
            "The loss value is :  0.16291698813438416 \n",
            "\n",
            "The loss value is :  0.0358353927731514 \n",
            "\n",
            "The loss value is :  0.16278766095638275 \n",
            "\n",
            "The loss value is :  0.025429964065551758 \n",
            "\n",
            "The loss value is :  0.14285294711589813 \n",
            "\n",
            "The loss value is :  0.025346720591187477 \n",
            "\n",
            "The loss value is :  0.15125618875026703 \n",
            "\n",
            "The loss value is :  0.018414661288261414 \n",
            "\n",
            "The loss value is :  0.14204935729503632 \n",
            "\n",
            "The minimum loss obtained is: 0.14204935729503632\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b5436a0> and its information value is:  tensor(0.0518)\n",
            "Target node: 64189\n",
            "1-hop neighbors of A: {64184}\n",
            "2-hop neighbors of A: {64178, 64188}\n",
            "Out of neighborhood of A: {64151, 64169, 64170, 64171, 64172, 64173, 64174, 64175, 64176, 64177, 64179, 64180, 64181, 64182, 64183, 64185, 64186, 64187} \n",
            "\n",
            "The graph number 4296.0 contains: {64192, 64193, 64194, 64195, 64196, 64197, 64198, 64199, 64200, 64201, 64202, 64203, 64173, 64175, 64176, 64177, 64178, 64179, 64180, 64181, 64182, 64183, 64185, 64186, 64187, 64190, 64191}\n",
            "The loss value is :  0.03541294485330582 \n",
            "\n",
            "The loss value is :  0.1637781858444214 \n",
            "\n",
            "The loss value is :  0.02233007177710533 \n",
            "\n",
            "The loss value is :  0.13325060904026031 \n",
            "\n",
            "The loss value is :  0.03158719465136528 \n",
            "\n",
            "The loss value is :  0.14403848350048065 \n",
            "\n",
            "The loss value is :  0.02665538340806961 \n",
            "\n",
            "The loss value is :  0.1439538300037384 \n",
            "\n",
            "The loss value is :  0.019514288753271103 \n",
            "\n",
            "The loss value is :  0.1531599462032318 \n",
            "\n",
            "The loss value is :  0.03218018263578415 \n",
            "\n",
            "The loss value is :  0.1514849066734314 \n",
            "\n",
            "The loss value is :  0.03535013645887375 \n",
            "\n",
            "The loss value is :  0.18261998891830444 \n",
            "\n",
            "The loss value is :  0.035313524305820465 \n",
            "\n",
            "The loss value is :  0.1386810541152954 \n",
            "\n",
            "The minimum loss obtained is: 0.1386810541152954\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b543190> and its information value is:  tensor(0.0500)\n",
            "Target node: 64191\n",
            "1-hop neighbors of A: {64173}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {64192, 64193, 64194, 64195, 64196, 64197, 64198, 64199, 64200, 64201, 64202, 64203, 64175, 64176, 64177, 64178, 64179, 64180, 64181, 64182, 64183, 64185, 64186, 64187, 64190} \n",
            "\n",
            "The graph number 4297.0 contains: {64202, 64203, 64204, 64205, 64206, 64207, 64208, 64209, 64210, 64211}\n",
            "The loss value is :  0.042061880230903625 \n",
            "\n",
            "The loss value is :  0.09881751984357834 \n",
            "\n",
            "The loss value is :  0.06359042972326279 \n",
            "\n",
            "The loss value is :  0.07506686449050903 \n",
            "\n",
            "The loss value is :  0.03170793503522873 \n",
            "\n",
            "The loss value is :  0.17044486105442047 \n",
            "\n",
            "The loss value is :  0.03578723594546318 \n",
            "\n",
            "The loss value is :  0.1436445713043213 \n",
            "\n",
            "The loss value is :  0.057471565902233124 \n",
            "\n",
            "The loss value is :  0.16564658284187317 \n",
            "\n",
            "The loss value is :  0.06515661627054214 \n",
            "\n",
            "The loss value is :  0.10812489688396454 \n",
            "\n",
            "The loss value is :  0.024889767169952393 \n",
            "\n",
            "The loss value is :  0.0417831689119339 \n",
            "\n",
            "The loss value is :  0.009311962872743607 \n",
            "\n",
            "The loss value is :  0.1740163266658783 \n",
            "\n",
            "The minimum loss obtained is: 0.1740163266658783\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b5400d0> and its information value is:  tensor(0.0675)\n",
            "Target node: 64211\n",
            "1-hop neighbors of A: {64203}\n",
            "2-hop neighbors of A: {64202, 64206}\n",
            "Out of neighborhood of A: {64204, 64205, 64207, 64208, 64209, 64210} \n",
            "\n",
            "The graph number 4298.0 contains: {64203, 64204, 64205, 64206, 64207, 64208, 64210, 64211, 64212, 64213, 64214, 64215, 64216, 64217, 64218, 64219, 64220}\n",
            "The loss value is :  0.03426221385598183 \n",
            "\n",
            "The loss value is :  0.1562911570072174 \n",
            "\n",
            "The loss value is :  0.03176148235797882 \n",
            "\n",
            "The loss value is :  0.10288169234991074 \n",
            "\n",
            "The loss value is :  0.03452207148075104 \n",
            "\n",
            "The loss value is :  0.1640719324350357 \n",
            "\n",
            "The loss value is :  0.0014025606215000153 \n",
            "\n",
            "The loss value is :  0.11012853682041168 \n",
            "\n",
            "The loss value is :  0.0035794414579868317 \n",
            "\n",
            "The loss value is :  0.15694989264011383 \n",
            "\n",
            "The loss value is :  0.022981688380241394 \n",
            "\n",
            "The loss value is :  0.1619499772787094 \n",
            "\n",
            "The loss value is :  0.0357007198035717 \n",
            "\n",
            "The loss value is :  0.13061974942684174 \n",
            "\n",
            "The loss value is :  0.03176632896065712 \n",
            "\n",
            "The loss value is :  0.14448150992393494 \n",
            "\n",
            "The minimum loss obtained is: 0.14448150992393494\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b540cd0> and its information value is:  tensor(0.0573)\n",
            "Target node: 64220\n",
            "1-hop neighbors of A: {64210}\n",
            "2-hop neighbors of A: {64218, 64219}\n",
            "Out of neighborhood of A: {64203, 64204, 64205, 64206, 64207, 64208, 64211, 64212, 64213, 64214, 64215, 64216, 64217} \n",
            "\n",
            "The graph number 4299.0 contains: {64210, 64220, 64221, 64222, 64223, 64224, 64225, 64226, 64227, 64228, 64229, 64230, 64231, 64232, 64233, 64234, 64235, 64236, 64237, 64238, 64239, 64240}\n",
            "The loss value is :  0.029857154935598373 \n",
            "\n",
            "The loss value is :  0.17752622067928314 \n",
            "\n",
            "The loss value is :  0.03733184561133385 \n",
            "\n",
            "The loss value is :  0.15075211226940155 \n",
            "\n",
            "The loss value is :  0.03667394071817398 \n",
            "\n",
            "The loss value is :  0.1684611439704895 \n",
            "\n",
            "The loss value is :  0.007431505247950554 \n",
            "\n",
            "The loss value is :  0.16926182806491852 \n",
            "\n",
            "The loss value is :  0.033390507102012634 \n",
            "\n",
            "The loss value is :  0.12081603705883026 \n",
            "\n",
            "The loss value is :  0.02239309623837471 \n",
            "\n",
            "The loss value is :  0.1431885063648224 \n",
            "\n",
            "The loss value is :  0.03154149651527405 \n",
            "\n",
            "The loss value is :  0.12756268680095673 \n",
            "\n",
            "The loss value is :  0.021224264055490494 \n",
            "\n",
            "The loss value is :  0.1424824446439743 \n",
            "\n",
            "The minimum loss obtained is: 0.1424824446439743\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b542e90> and its information value is:  tensor(0.0516)\n",
            "Target node: 64240\n",
            "1-hop neighbors of A: {64235}\n",
            "2-hop neighbors of A: {64228}\n",
            "Out of neighborhood of A: {64210, 64220, 64221, 64222, 64223, 64224, 64225, 64226, 64227, 64229, 64230, 64231, 64232, 64233, 64234, 64236, 64237, 64238, 64239} \n",
            "\n",
            "The graph number 4300.0 contains: {64226, 64227, 64228, 64229, 64232, 64233, 64234, 64236, 64237, 64238, 64239, 64240, 64241, 64242, 64243, 64244, 64245, 64246, 64247, 64249, 64250, 64251, 64252, 64253, 64254, 64255}\n",
            "The loss value is :  0.03110622800886631 \n",
            "\n",
            "The loss value is :  0.18375736474990845 \n",
            "\n",
            "The loss value is :  0.024269379675388336 \n",
            "\n",
            "The loss value is :  0.16626262664794922 \n",
            "\n",
            "The loss value is :  0.02668909728527069 \n",
            "\n",
            "The loss value is :  0.17272572219371796 \n",
            "\n",
            "The loss value is :  0.02330600470304489 \n",
            "\n",
            "The loss value is :  0.18002094328403473 \n",
            "\n",
            "The loss value is :  0.04571253061294556 \n",
            "\n",
            "The loss value is :  0.16906383633613586 \n",
            "\n",
            "The loss value is :  0.038177717477083206 \n",
            "\n",
            "The loss value is :  0.19277997314929962 \n",
            "\n",
            "The loss value is :  0.038564980030059814 \n",
            "\n",
            "The loss value is :  0.17250148952007294 \n",
            "\n",
            "The loss value is :  0.03566254675388336 \n",
            "\n",
            "The loss value is :  0.15650039911270142 \n",
            "\n",
            "The minimum loss obtained is: 0.15650039911270142\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b5414b0> and its information value is:  tensor(0.0413)\n",
            "Target node: 64255\n",
            "1-hop neighbors of A: {64233}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {64226, 64227, 64228, 64229, 64232, 64234, 64236, 64237, 64238, 64239, 64240, 64241, 64242, 64243, 64244, 64245, 64246, 64247, 64249, 64250, 64251, 64252, 64253, 64254} \n",
            "\n",
            "The graph number 4301.0 contains: {64256, 64257, 64258, 64259, 64260, 64261, 64262, 64263, 64264, 64265, 64266, 64267, 64268, 64269, 64234, 64235, 64236, 64237, 64238, 64239, 64240, 64241, 64242, 64243}\n",
            "The loss value is :  0.02971133030951023 \n",
            "\n",
            "The loss value is :  0.15889498591423035 \n",
            "\n",
            "The loss value is :  0.032490141689777374 \n",
            "\n",
            "The loss value is :  0.13418123126029968 \n",
            "\n",
            "The loss value is :  0.030959894880652428 \n",
            "\n",
            "The loss value is :  0.158028706908226 \n",
            "\n",
            "The loss value is :  0.025959935039281845 \n",
            "\n",
            "The loss value is :  0.1736556887626648 \n",
            "\n",
            "The loss value is :  0.03442363068461418 \n",
            "\n",
            "The loss value is :  0.16786989569664001 \n",
            "\n",
            "The loss value is :  0.017261631786823273 \n",
            "\n",
            "The loss value is :  0.14408358931541443 \n",
            "\n",
            "The loss value is :  0.0233051348477602 \n",
            "\n",
            "The loss value is :  0.17777083814144135 \n",
            "\n",
            "The loss value is :  0.022058973088860512 \n",
            "\n",
            "The loss value is :  0.16152556240558624 \n",
            "\n",
            "The minimum loss obtained is: 0.16152556240558624\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b541a50> and its information value is:  tensor(0.0405)\n",
            "Target node: 64243\n",
            "1-hop neighbors of A: {64268, 64269}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {64256, 64257, 64258, 64259, 64260, 64261, 64262, 64263, 64264, 64265, 64266, 64267, 64234, 64235, 64236, 64237, 64238, 64239, 64240, 64241, 64242} \n",
            "\n",
            "The graph number 4302.0 contains: {64270, 64271, 64272, 64273, 64274, 64275, 64276, 64277, 64278, 64279, 64280, 64281, 64282, 64283, 64284, 64285, 64286, 64287, 64288, 64289, 64290, 64291, 64243, 64244, 64245, 64247, 64248}\n",
            "The loss value is :  0.02979409694671631 \n",
            "\n",
            "The loss value is :  0.15919412672519684 \n",
            "\n",
            "The loss value is :  0.029375795274972916 \n",
            "\n",
            "The loss value is :  0.1566101759672165 \n",
            "\n",
            "The loss value is :  0.0302144642919302 \n",
            "\n",
            "The loss value is :  0.16882094740867615 \n",
            "\n",
            "The loss value is :  0.016745787113904953 \n",
            "\n",
            "The loss value is :  0.15834853053092957 \n",
            "\n",
            "The loss value is :  0.034841082990169525 \n",
            "\n",
            "The loss value is :  0.16692376136779785 \n",
            "\n",
            "The loss value is :  0.03701377287507057 \n",
            "\n",
            "The loss value is :  0.14562423527240753 \n",
            "\n",
            "The loss value is :  0.026062745600938797 \n",
            "\n",
            "The loss value is :  0.16052715480327606 \n",
            "\n",
            "The loss value is :  0.028259340673685074 \n",
            "\n",
            "The loss value is :  0.1547122299671173 \n",
            "\n",
            "The minimum loss obtained is: 0.1547122299671173\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b5438b0> and its information value is:  tensor(0.0416)\n",
            "Target node: 64248\n",
            "1-hop neighbors of A: {64278}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {64270, 64271, 64272, 64273, 64274, 64275, 64276, 64277, 64279, 64280, 64281, 64282, 64283, 64284, 64285, 64286, 64287, 64288, 64289, 64290, 64291, 64243, 64244, 64245, 64247} \n",
            "\n",
            "The graph number 4303.0 contains: {64289, 64290, 64291, 64292, 64293, 64294, 64295, 64296, 64297, 64280, 64281, 64283, 64284, 64287}\n",
            "The loss value is :  0.09532105922698975 \n",
            "\n",
            "The loss value is :  0.11777430027723312 \n",
            "\n",
            "The loss value is :  0.02889096550643444 \n",
            "\n",
            "The loss value is :  0.097037672996521 \n",
            "\n",
            "The loss value is :  0.013591897673904896 \n",
            "\n",
            "The loss value is :  0.14910843968391418 \n",
            "\n",
            "The loss value is :  0.011980565264821053 \n",
            "\n",
            "The loss value is :  0.16898706555366516 \n",
            "\n",
            "The loss value is :  0.011046082712709904 \n",
            "\n",
            "The loss value is :  0.11126375198364258 \n",
            "\n",
            "The loss value is :  0.01925485022366047 \n",
            "\n",
            "The loss value is :  0.10801379382610321 \n",
            "\n",
            "The loss value is :  0.042621634900569916 \n",
            "\n",
            "The loss value is :  0.15178996324539185 \n",
            "\n",
            "The loss value is :  0.04462795704603195 \n",
            "\n",
            "The loss value is :  0.11942616105079651 \n",
            "\n",
            "The minimum loss obtained is: 0.11942616105079651\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b5401c0> and its information value is:  tensor(0.0792)\n",
            "Target node: 64287\n",
            "1-hop neighbors of A: {64284}\n",
            "2-hop neighbors of A: {64296}\n",
            "Out of neighborhood of A: {64289, 64290, 64291, 64292, 64293, 64294, 64295, 64297, 64280, 64281, 64283} \n",
            "\n",
            "The graph number 4304.0 contains: {64289, 64290, 64291, 64298, 64299, 64300, 64301, 64302, 64303, 64304, 64305, 64306, 64307}\n",
            "The loss value is :  0.026750454679131508 \n",
            "\n",
            "The loss value is :  0.1098388135433197 \n",
            "\n",
            "The loss value is :  0.06284284591674805 \n",
            "\n",
            "The loss value is :  0.08037147670984268 \n",
            "\n",
            "The loss value is :  0.05029803141951561 \n",
            "\n",
            "The loss value is :  0.16186586022377014 \n",
            "\n",
            "The loss value is :  0.029050277546048164 \n",
            "\n",
            "The loss value is :  0.08421657234430313 \n",
            "\n",
            "The loss value is :  0.00493178516626358 \n",
            "\n",
            "The loss value is :  0.1102355569601059 \n",
            "\n",
            "The loss value is :  0.013291715644299984 \n",
            "\n",
            "The loss value is :  0.09699556976556778 \n",
            "\n",
            "The loss value is :  0.02324839122593403 \n",
            "\n",
            "The loss value is :  0.14184769988059998 \n",
            "\n",
            "The loss value is :  0.01591327413916588 \n",
            "\n",
            "The loss value is :  0.14903263747692108 \n",
            "\n",
            "The minimum loss obtained is: 0.14903263747692108\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b5435e0> and its information value is:  tensor(0.0653)\n",
            "Target node: 64307\n",
            "1-hop neighbors of A: {64303}\n",
            "2-hop neighbors of A: {64306, 64302}\n",
            "Out of neighborhood of A: {64289, 64290, 64291, 64298, 64299, 64300, 64301, 64304, 64305} \n",
            "\n",
            "The graph number 4305.0 contains: {64304, 64305, 64306, 64307, 64308, 64309, 64310, 64311, 64312}\n",
            "The loss value is :  -0.0018206406384706497 \n",
            "\n",
            "The loss value is :  0.19132907688617706 \n",
            "\n",
            "The loss value is :  -0.010938867926597595 \n",
            "\n",
            "The loss value is :  0.13017430901527405 \n",
            "\n",
            "The loss value is :  -0.0024132393300533295 \n",
            "\n",
            "The loss value is :  0.12863489985466003 \n",
            "\n",
            "The loss value is :  0.10648390650749207 \n",
            "\n",
            "The loss value is :  0.059015244245529175 \n",
            "\n",
            "The loss value is :  0.05283575505018234 \n",
            "\n",
            "The loss value is :  0.10604339838027954 \n",
            "\n",
            "The loss value is :  0.058211565017700195 \n",
            "\n",
            "The loss value is :  0.06757793575525284 \n",
            "\n",
            "The loss value is :  -0.00770132802426815 \n",
            "\n",
            "The loss value is :  0.05204939469695091 \n",
            "\n",
            "The loss value is :  0.001579582691192627 \n",
            "\n",
            "The loss value is :  0.09949659556150436 \n",
            "\n",
            "The minimum loss obtained is: 0.09949659556150436\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b542cb0> and its information value is:  tensor(0.1185)\n",
            "Target node: 64312\n",
            "1-hop neighbors of A: {64305}\n",
            "2-hop neighbors of A: {64310}\n",
            "Out of neighborhood of A: {64304, 64306, 64307, 64308, 64309, 64311} \n",
            "\n",
            "The graph number 4306.0 contains: {64320, 64321, 64322, 64323, 64324, 64325, 64305, 64309, 64310, 64311, 64312, 64313, 64314, 64315, 64316, 64317, 64318, 64319}\n",
            "The loss value is :  0.03974025696516037 \n",
            "\n",
            "The loss value is :  0.10289809852838516 \n",
            "\n",
            "The loss value is :  0.027278080582618713 \n",
            "\n",
            "The loss value is :  0.15421751141548157 \n",
            "\n",
            "The loss value is :  0.024100124835968018 \n",
            "\n",
            "The loss value is :  0.14123578369617462 \n",
            "\n",
            "The loss value is :  0.04257288575172424 \n",
            "\n",
            "The loss value is :  0.15530452132225037 \n",
            "\n",
            "The loss value is :  0.028198212385177612 \n",
            "\n",
            "The loss value is :  0.13523440062999725 \n",
            "\n",
            "The loss value is :  0.02640775591135025 \n",
            "\n",
            "The loss value is :  0.13332849740982056 \n",
            "\n",
            "The loss value is :  0.017583906650543213 \n",
            "\n",
            "The loss value is :  0.13287045061588287 \n",
            "\n",
            "The loss value is :  0.023008376359939575 \n",
            "\n",
            "The loss value is :  0.16023866832256317 \n",
            "\n",
            "The minimum loss obtained is: 0.16023866832256317\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b5436a0> and its information value is:  tensor(0.0479)\n",
            "Target node: 64319\n",
            "1-hop neighbors of A: {64320, 64321, 64316}\n",
            "2-hop neighbors of A: {64322, 64323, 64317, 64318}\n",
            "Out of neighborhood of A: {64324, 64325, 64305, 64309, 64310, 64311, 64312, 64313, 64314, 64315} \n",
            "\n",
            "The graph number 4307.0 contains: {64320, 64321, 64323, 64324, 64325, 64326, 64327, 64328, 64329, 64330, 64331, 64332, 64333, 64334, 64335, 64336, 64337, 64338, 64339, 64340, 64341, 64342, 64343, 64344, 64345, 64346, 64347, 64348, 64349, 64350, 64351, 64352}\n",
            "The loss value is :  0.02632668986916542 \n",
            "\n",
            "The loss value is :  0.16242623329162598 \n",
            "\n",
            "The loss value is :  0.028357384726405144 \n",
            "\n",
            "The loss value is :  0.1747075766324997 \n",
            "\n",
            "The loss value is :  0.04539947956800461 \n",
            "\n",
            "The loss value is :  0.18670518696308136 \n",
            "\n",
            "The loss value is :  0.027933040633797646 \n",
            "\n",
            "The loss value is :  0.1673719435930252 \n",
            "\n",
            "The loss value is :  0.04206936061382294 \n",
            "\n",
            "The loss value is :  0.1655096560716629 \n",
            "\n",
            "The loss value is :  0.03007643111050129 \n",
            "\n",
            "The loss value is :  0.16928112506866455 \n",
            "\n",
            "The loss value is :  0.03981757164001465 \n",
            "\n",
            "The loss value is :  0.17404833436012268 \n",
            "\n",
            "The loss value is :  0.0245816633105278 \n",
            "\n",
            "The loss value is :  0.16587381064891815 \n",
            "\n",
            "The minimum loss obtained is: 0.16587381064891815\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b543370> and its information value is:  tensor(0.0342)\n",
            "Target node: 64352\n",
            "1-hop neighbors of A: {64348}\n",
            "2-hop neighbors of A: {64339}\n",
            "Out of neighborhood of A: {64320, 64321, 64323, 64324, 64325, 64326, 64327, 64328, 64329, 64330, 64331, 64332, 64333, 64334, 64335, 64336, 64337, 64338, 64340, 64341, 64342, 64343, 64344, 64345, 64346, 64347, 64349, 64350, 64351} \n",
            "\n",
            "The graph number 4308.0 contains: {64340, 64341, 64342, 64343, 64344, 64345, 64346, 64347, 64348, 64349, 64350, 64351, 64352, 64353, 64354, 64355, 64356, 64357, 64358, 64359, 64360, 64361, 64362, 64363, 64364, 64365, 64366, 64367, 64368, 64369, 64370, 64371, 64372}\n",
            "The loss value is :  0.025300322100520134 \n",
            "\n",
            "The loss value is :  0.16216234862804413 \n",
            "\n",
            "The loss value is :  0.046392396092414856 \n",
            "\n",
            "The loss value is :  0.16044169664382935 \n",
            "\n",
            "The loss value is :  0.03281966969370842 \n",
            "\n",
            "The loss value is :  0.16692353785037994 \n",
            "\n",
            "The loss value is :  0.0346086323261261 \n",
            "\n",
            "The loss value is :  0.18177393078804016 \n",
            "\n",
            "The loss value is :  0.028456198051571846 \n",
            "\n",
            "The loss value is :  0.18012234568595886 \n",
            "\n",
            "The loss value is :  0.042781081050634384 \n",
            "\n",
            "The loss value is :  0.19342249631881714 \n",
            "\n",
            "The loss value is :  0.03586488217115402 \n",
            "\n",
            "The loss value is :  0.17990891635417938 \n",
            "\n",
            "The loss value is :  0.0365566611289978 \n",
            "\n",
            "The loss value is :  0.16120481491088867 \n",
            "\n",
            "The minimum loss obtained is: 0.16120481491088867\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b542470> and its information value is:  tensor(0.0359)\n",
            "Target node: 64372\n",
            "1-hop neighbors of A: {64369}\n",
            "2-hop neighbors of A: {64368}\n",
            "Out of neighborhood of A: {64340, 64341, 64342, 64343, 64344, 64345, 64346, 64347, 64348, 64349, 64350, 64351, 64352, 64353, 64354, 64355, 64356, 64357, 64358, 64359, 64360, 64361, 64362, 64363, 64364, 64365, 64366, 64367, 64370, 64371} \n",
            "\n",
            "The graph number 4309.0 contains: {64384, 64385, 64386, 64387, 64388, 64389, 64390, 64369, 64370, 64371, 64372, 64373, 64374, 64375, 64376, 64377, 64378, 64379, 64380, 64381, 64382, 64383}\n",
            "The loss value is :  0.02567495033144951 \n",
            "\n",
            "The loss value is :  0.11604365706443787 \n",
            "\n",
            "The loss value is :  0.01768598146736622 \n",
            "\n",
            "The loss value is :  0.16297438740730286 \n",
            "\n",
            "The loss value is :  0.027428722009062767 \n",
            "\n",
            "The loss value is :  0.17655840516090393 \n",
            "\n",
            "The loss value is :  0.02794419415295124 \n",
            "\n",
            "The loss value is :  0.17176960408687592 \n",
            "\n",
            "The loss value is :  0.045182965695858 \n",
            "\n",
            "The loss value is :  0.16926415264606476 \n",
            "\n",
            "The loss value is :  0.02633613534271717 \n",
            "\n",
            "The loss value is :  0.12476938962936401 \n",
            "\n",
            "The loss value is :  0.04446100816130638 \n",
            "\n",
            "The loss value is :  0.16978633403778076 \n",
            "\n",
            "The loss value is :  0.03320217505097389 \n",
            "\n",
            "The loss value is :  0.13729187846183777 \n",
            "\n",
            "The minimum loss obtained is: 0.13729187846183777\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b542080> and its information value is:  tensor(0.0545)\n",
            "Target node: 64383\n",
            "1-hop neighbors of A: {64374}\n",
            "2-hop neighbors of A: {64376, 64370}\n",
            "Out of neighborhood of A: {64384, 64385, 64386, 64387, 64388, 64389, 64390, 64369, 64371, 64372, 64373, 64375, 64377, 64378, 64379, 64380, 64381, 64382} \n",
            "\n",
            "The graph number 4310.0 contains: {64387, 64388, 64389, 64390, 64391, 64392, 64393, 64394, 64395, 64396, 64397, 64398, 64399, 64400, 64401, 64402, 64403, 64404, 64405, 64406, 64408, 64409, 64410, 64411, 64412, 64413, 64414, 64415, 64416}\n",
            "The loss value is :  0.02208603546023369 \n",
            "\n",
            "The loss value is :  0.15071769058704376 \n",
            "\n",
            "The loss value is :  0.02635333128273487 \n",
            "\n",
            "The loss value is :  0.16906322538852692 \n",
            "\n",
            "The loss value is :  0.03522640839219093 \n",
            "\n",
            "The loss value is :  0.1686524748802185 \n",
            "\n",
            "The loss value is :  0.03226550295948982 \n",
            "\n",
            "The loss value is :  0.16163551807403564 \n",
            "\n",
            "The loss value is :  0.030731113627552986 \n",
            "\n",
            "The loss value is :  0.1393842250108719 \n",
            "\n",
            "The loss value is :  0.028727158904075623 \n",
            "\n",
            "The loss value is :  0.15527303516864777 \n",
            "\n",
            "The loss value is :  0.028254181146621704 \n",
            "\n",
            "The loss value is :  0.18164215981960297 \n",
            "\n",
            "The loss value is :  0.023386042565107346 \n",
            "\n",
            "The loss value is :  0.17385680973529816 \n",
            "\n",
            "The minimum loss obtained is: 0.17385680973529816\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b543a60> and its information value is:  tensor(0.0323)\n",
            "Target node: 64416\n",
            "1-hop neighbors of A: {64414}\n",
            "2-hop neighbors of A: {64415}\n",
            "Out of neighborhood of A: {64387, 64388, 64389, 64390, 64391, 64392, 64393, 64394, 64395, 64396, 64397, 64398, 64399, 64400, 64401, 64402, 64403, 64404, 64405, 64406, 64408, 64409, 64410, 64411, 64412, 64413} \n",
            "\n",
            "The graph number 4311.0 contains: {64414, 64415, 64416, 64417, 64418, 64419, 64420, 64421, 64422, 64423, 64424, 64425, 64426, 64427, 64428, 64429, 64430, 64431, 64432}\n",
            "The loss value is :  0.025680866092443466 \n",
            "\n",
            "The loss value is :  0.1219087690114975 \n",
            "\n",
            "The loss value is :  0.0194559283554554 \n",
            "\n",
            "The loss value is :  0.11846290528774261 \n",
            "\n",
            "The loss value is :  0.04508905112743378 \n",
            "\n",
            "The loss value is :  0.13615979254245758 \n",
            "\n",
            "The loss value is :  0.052675675600767136 \n",
            "\n",
            "The loss value is :  0.1401481032371521 \n",
            "\n",
            "The loss value is :  0.04188990592956543 \n",
            "\n",
            "The loss value is :  0.1069318875670433 \n",
            "\n",
            "The loss value is :  0.04928218945860863 \n",
            "\n",
            "The loss value is :  0.12100718915462494 \n",
            "\n",
            "The loss value is :  0.013070453889667988 \n",
            "\n",
            "The loss value is :  0.1452123522758484 \n",
            "\n",
            "The loss value is :  0.04460013285279274 \n",
            "\n",
            "The loss value is :  0.14615043997764587 \n",
            "\n",
            "The minimum loss obtained is: 0.14615043997764587\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b5419c0> and its information value is:  tensor(0.0532)\n",
            "Target node: 64432\n",
            "1-hop neighbors of A: {64425}\n",
            "2-hop neighbors of A: {64427, 64422}\n",
            "Out of neighborhood of A: {64416, 64417, 64418, 64419, 64420, 64421, 64423, 64424, 64426, 64428, 64429, 64430, 64431, 64414, 64415} \n",
            "\n",
            "The graph number 4312.0 contains: {64448, 64449, 64450, 64451, 64452, 64453, 64454, 64427, 64433, 64434, 64435, 64436, 64437, 64438, 64439, 64440, 64441, 64442, 64443, 64444, 64445, 64446, 64447}\n",
            "The loss value is :  0.020750287920236588 \n",
            "\n",
            "The loss value is :  0.13285478949546814 \n",
            "\n",
            "The loss value is :  0.021947020664811134 \n",
            "\n",
            "The loss value is :  0.19195258617401123 \n",
            "\n",
            "The loss value is :  0.02052014134824276 \n",
            "\n",
            "The loss value is :  0.15986455976963043 \n",
            "\n",
            "The loss value is :  0.02918361872434616 \n",
            "\n",
            "The loss value is :  0.1403992623090744 \n",
            "\n",
            "The loss value is :  0.05196136236190796 \n",
            "\n",
            "The loss value is :  0.1515975445508957 \n",
            "\n",
            "The loss value is :  0.025898460298776627 \n",
            "\n",
            "The loss value is :  0.14969205856323242 \n",
            "\n",
            "The loss value is :  0.02563527785241604 \n",
            "\n",
            "The loss value is :  0.1642630249261856 \n",
            "\n",
            "The loss value is :  0.032567404210567474 \n",
            "\n",
            "The loss value is :  0.15102989971637726 \n",
            "\n",
            "The minimum loss obtained is: 0.15102989971637726\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b540850> and its information value is:  tensor(0.0462)\n",
            "Target node: 64447\n",
            "1-hop neighbors of A: {64440}\n",
            "2-hop neighbors of A: {64437, 64446}\n",
            "Out of neighborhood of A: {64448, 64449, 64450, 64451, 64452, 64453, 64454, 64427, 64433, 64434, 64435, 64436, 64438, 64439, 64441, 64442, 64443, 64444, 64445} \n",
            "\n",
            "The graph number 4313.0 contains: {64448, 64449, 64450, 64451, 64452, 64453, 64454, 64455, 64456, 64457, 64458, 64459, 64460, 64461, 64442, 64444, 64445, 64446, 64447}\n",
            "The loss value is :  0.02870536409318447 \n",
            "\n",
            "The loss value is :  0.16973011195659637 \n",
            "\n",
            "The loss value is :  0.01491573452949524 \n",
            "\n",
            "The loss value is :  0.16751456260681152 \n",
            "\n",
            "The loss value is :  0.028299467638134956 \n",
            "\n",
            "The loss value is :  0.12054953724145889 \n",
            "\n",
            "The loss value is :  0.031012900173664093 \n",
            "\n",
            "The loss value is :  0.15367263555526733 \n",
            "\n",
            "The loss value is :  0.011292340233922005 \n",
            "\n",
            "The loss value is :  0.13762813806533813 \n",
            "\n",
            "The loss value is :  0.04758421331644058 \n",
            "\n",
            "The loss value is :  0.14618611335754395 \n",
            "\n",
            "The loss value is :  0.0138554023578763 \n",
            "\n",
            "The loss value is :  0.15044119954109192 \n",
            "\n",
            "The loss value is :  -0.006738860160112381 \n",
            "\n",
            "The loss value is :  0.11075510084629059 \n",
            "\n",
            "The minimum loss obtained is: 0.11075510084629059\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b540e20> and its information value is:  tensor(0.0751)\n",
            "Target node: 64447\n",
            "1-hop neighbors of A: {64459, 64455}\n",
            "2-hop neighbors of A: {64449}\n",
            "Out of neighborhood of A: {64448, 64450, 64451, 64452, 64453, 64454, 64456, 64457, 64458, 64460, 64461, 64442, 64444, 64445, 64446} \n",
            "\n",
            "The graph number 4314.0 contains: {64449, 64450, 64451, 64452, 64453, 64461, 64462, 64463, 64464, 64465}\n",
            "The loss value is :  0.05841563642024994 \n",
            "\n",
            "The loss value is :  0.09338398277759552 \n",
            "\n",
            "The loss value is :  0.07062489539384842 \n",
            "\n",
            "The loss value is :  0.16152901947498322 \n",
            "\n",
            "The loss value is :  0.03460514545440674 \n",
            "\n",
            "The loss value is :  0.11695869266986847 \n",
            "\n",
            "The loss value is :  0.051989175379276276 \n",
            "\n",
            "The loss value is :  0.10815276205539703 \n",
            "\n",
            "The loss value is :  0.020137937739491463 \n",
            "\n",
            "The loss value is :  0.10277294367551804 \n",
            "\n",
            "The loss value is :  0.0355091318488121 \n",
            "\n",
            "The loss value is :  0.11891787499189377 \n",
            "\n",
            "The loss value is :  0.017240753397345543 \n",
            "\n",
            "The loss value is :  0.13534486293792725 \n",
            "\n",
            "The loss value is :  0.025663333013653755 \n",
            "\n",
            "The loss value is :  0.16680359840393066 \n",
            "\n",
            "The minimum loss obtained is: 0.16680359840393066\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b5428c0> and its information value is:  tensor(0.0709)\n",
            "Target node: 64465\n",
            "1-hop neighbors of A: {64453}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {64449, 64450, 64451, 64452, 64461, 64462, 64463, 64464} \n",
            "\n",
            "The graph number 4315.0 contains: {64453, 64454, 64455, 64456, 64457, 64465, 64466, 64467, 64468, 64469, 64470, 64471, 64472, 64473, 64474, 64475, 64476, 64477, 64478, 64479, 64480, 64481, 64482, 64483, 64484, 64485, 64486, 64487, 64488, 64489, 64490}\n",
            "The loss value is :  0.023636365309357643 \n",
            "\n",
            "The loss value is :  0.16109837591648102 \n",
            "\n",
            "The loss value is :  0.0244639553129673 \n",
            "\n",
            "The loss value is :  0.17127618193626404 \n",
            "\n",
            "The loss value is :  0.031246738508343697 \n",
            "\n",
            "The loss value is :  0.16095730662345886 \n",
            "\n",
            "The loss value is :  0.025834769010543823 \n",
            "\n",
            "The loss value is :  0.16722770035266876 \n",
            "\n",
            "The loss value is :  0.04115390405058861 \n",
            "\n",
            "The loss value is :  0.1649404764175415 \n",
            "\n",
            "The loss value is :  0.037613775581121445 \n",
            "\n",
            "The loss value is :  0.18111179769039154 \n",
            "\n",
            "The loss value is :  0.03118712082505226 \n",
            "\n",
            "The loss value is :  0.17035940289497375 \n",
            "\n",
            "The loss value is :  0.0307474285364151 \n",
            "\n",
            "The loss value is :  0.15818078815937042 \n",
            "\n",
            "The minimum loss obtained is: 0.15818078815937042\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b542950> and its information value is:  tensor(0.0380)\n",
            "Target node: 64490\n",
            "1-hop neighbors of A: {64486}\n",
            "2-hop neighbors of A: {64482}\n",
            "Out of neighborhood of A: {64453, 64454, 64455, 64456, 64457, 64465, 64466, 64467, 64468, 64469, 64470, 64471, 64472, 64473, 64474, 64475, 64476, 64477, 64478, 64479, 64480, 64481, 64483, 64484, 64485, 64487, 64488, 64489} \n",
            "\n",
            "The graph number 4316.0 contains: {64470, 64471, 64474, 64476, 64477, 64478, 64487, 64489, 64491, 64492, 64493, 64494, 64495, 64496, 64497, 64498}\n",
            "The loss value is :  -0.0021501630544662476 \n",
            "\n",
            "The loss value is :  0.11636579036712646 \n",
            "\n",
            "The loss value is :  -0.0008237697184085846 \n",
            "\n",
            "The loss value is :  0.11546855419874191 \n",
            "\n",
            "The loss value is :  0.04141075164079666 \n",
            "\n",
            "The loss value is :  0.13867951929569244 \n",
            "\n",
            "The loss value is :  0.019930679351091385 \n",
            "\n",
            "The loss value is :  0.16147109866142273 \n",
            "\n",
            "The loss value is :  0.02230440452694893 \n",
            "\n",
            "The loss value is :  0.1335841715335846 \n",
            "\n",
            "The loss value is :  0.028401143848896027 \n",
            "\n",
            "The loss value is :  0.16617263853549957 \n",
            "\n",
            "The loss value is :  0.052781205624341965 \n",
            "\n",
            "The loss value is :  0.15985271334648132 \n",
            "\n",
            "The loss value is :  0.045557618141174316 \n",
            "\n",
            "The loss value is :  0.16969585418701172 \n",
            "\n",
            "The minimum loss obtained is: 0.16969585418701172\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b5408e0> and its information value is:  tensor(0.0474)\n",
            "Target node: 64498\n",
            "1-hop neighbors of A: {64476}\n",
            "2-hop neighbors of A: {64474}\n",
            "Out of neighborhood of A: {64470, 64471, 64477, 64478, 64487, 64489, 64491, 64492, 64493, 64494, 64495, 64496, 64497} \n",
            "\n",
            "The graph number 4317.0 contains: {64512, 64513, 64476, 64477, 64478, 64479, 64485, 64486, 64489, 64490, 64491, 64499, 64500, 64501, 64502, 64503, 64504, 64505, 64506, 64507, 64508, 64509, 64510, 64511}\n",
            "The loss value is :  0.03093182109296322 \n",
            "\n",
            "The loss value is :  0.16065393388271332 \n",
            "\n",
            "The loss value is :  0.029421024024486542 \n",
            "\n",
            "The loss value is :  0.17128032445907593 \n",
            "\n",
            "The loss value is :  0.04564962536096573 \n",
            "\n",
            "The loss value is :  0.14835970103740692 \n",
            "\n",
            "The loss value is :  0.02017492614686489 \n",
            "\n",
            "The loss value is :  0.18311184644699097 \n",
            "\n",
            "The loss value is :  0.025409959256649017 \n",
            "\n",
            "The loss value is :  0.12988094985485077 \n",
            "\n",
            "The loss value is :  0.03665286302566528 \n",
            "\n",
            "The loss value is :  0.1504816710948944 \n",
            "\n",
            "The loss value is :  0.03592149168252945 \n",
            "\n",
            "The loss value is :  0.15086808800697327 \n",
            "\n",
            "The loss value is :  0.033931463956832886 \n",
            "\n",
            "The loss value is :  0.16087539494037628 \n",
            "\n",
            "The minimum loss obtained is: 0.16087539494037628\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b540f40> and its information value is:  tensor(0.0408)\n",
            "Target node: 64511\n",
            "1-hop neighbors of A: {64490}\n",
            "2-hop neighbors of A: {64509, 64510}\n",
            "Out of neighborhood of A: {64512, 64513, 64476, 64477, 64478, 64479, 64485, 64486, 64489, 64491, 64499, 64500, 64501, 64502, 64503, 64504, 64505, 64506, 64507, 64508} \n",
            "\n",
            "The graph number 4318.0 contains: {64514, 64515, 64516, 64517, 64518, 64519, 64520, 64521, 64522, 64523, 64524, 64525, 64526, 64527, 64528, 64529, 64530, 64531, 64532, 64533, 64491, 64492, 64493}\n",
            "The loss value is :  0.03366904705762863 \n",
            "\n",
            "The loss value is :  0.13590168952941895 \n",
            "\n",
            "The loss value is :  0.02167584002017975 \n",
            "\n",
            "The loss value is :  0.17157913744449615 \n",
            "\n",
            "The loss value is :  0.009534565731883049 \n",
            "\n",
            "The loss value is :  0.1266820877790451 \n",
            "\n",
            "The loss value is :  0.026147793978452682 \n",
            "\n",
            "The loss value is :  0.18173669278621674 \n",
            "\n",
            "The loss value is :  0.034361857920885086 \n",
            "\n",
            "The loss value is :  0.15552301704883575 \n",
            "\n",
            "The loss value is :  0.018613176420331 \n",
            "\n",
            "The loss value is :  0.13033275306224823 \n",
            "\n",
            "The loss value is :  0.047751493752002716 \n",
            "\n",
            "The loss value is :  0.12424233555793762 \n",
            "\n",
            "The loss value is :  0.024171896278858185 \n",
            "\n",
            "The loss value is :  0.1503530889749527 \n",
            "\n",
            "The minimum loss obtained is: 0.1503530889749527\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b540100> and its information value is:  tensor(0.0466)\n",
            "Target node: 64493\n",
            "1-hop neighbors of A: {64516, 64517, 64518}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {64514, 64515, 64519, 64520, 64521, 64522, 64523, 64524, 64525, 64526, 64527, 64528, 64529, 64530, 64531, 64532, 64533, 64491, 64492} \n",
            "\n",
            "The graph number 4319.0 contains: {64522, 64523, 64525, 64526, 64527, 64528, 64530, 64533, 64534, 64535, 64536, 64537, 64538, 64539, 64540, 64541, 64542, 64543, 64544, 64545, 64546, 64547}\n",
            "The loss value is :  0.04606441780924797 \n",
            "\n",
            "The loss value is :  0.14351090788841248 \n",
            "\n",
            "The loss value is :  0.03330064192414284 \n",
            "\n",
            "The loss value is :  0.15343748033046722 \n",
            "\n",
            "The loss value is :  0.03732536733150482 \n",
            "\n",
            "The loss value is :  0.14998196065425873 \n",
            "\n",
            "The loss value is :  0.04311971366405487 \n",
            "\n",
            "The loss value is :  0.162056103348732 \n",
            "\n",
            "The loss value is :  0.020248539745807648 \n",
            "\n",
            "The loss value is :  0.15822654962539673 \n",
            "\n",
            "The loss value is :  0.032271698117256165 \n",
            "\n",
            "The loss value is :  0.18284161388874054 \n",
            "\n",
            "The loss value is :  0.024605106562376022 \n",
            "\n",
            "The loss value is :  0.11941821128129959 \n",
            "\n",
            "The loss value is :  0.021931340917944908 \n",
            "\n",
            "The loss value is :  0.13897182047367096 \n",
            "\n",
            "The minimum loss obtained is: 0.13897182047367096\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b542170> and its information value is:  tensor(0.0535)\n",
            "Target node: 64547\n",
            "1-hop neighbors of A: {64537}\n",
            "2-hop neighbors of A: {64536, 64535}\n",
            "Out of neighborhood of A: {64522, 64523, 64525, 64526, 64527, 64528, 64530, 64533, 64534, 64538, 64539, 64540, 64541, 64542, 64543, 64544, 64545, 64546} \n",
            "\n",
            "The graph number 4320.0 contains: {64548, 64549, 64550, 64551, 64552, 64553, 64554, 64555, 64556, 64557, 64558, 64559, 64560, 64561, 64562}\n",
            "The loss value is :  0.001995014026761055 \n",
            "\n",
            "The loss value is :  0.1328623741865158 \n",
            "\n",
            "The loss value is :  0.030776163563132286 \n",
            "\n",
            "The loss value is :  0.1221884936094284 \n",
            "\n",
            "The loss value is :  0.019987694919109344 \n",
            "\n",
            "The loss value is :  0.1648467481136322 \n",
            "\n",
            "The loss value is :  0.015384652651846409 \n",
            "\n",
            "The loss value is :  0.14997665584087372 \n",
            "\n",
            "The loss value is :  -0.002675490453839302 \n",
            "\n",
            "The loss value is :  0.11846847087144852 \n",
            "\n",
            "The loss value is :  0.030185949057340622 \n",
            "\n",
            "The loss value is :  0.13151435554027557 \n",
            "\n",
            "The loss value is :  0.017240915447473526 \n",
            "\n",
            "The loss value is :  0.10060727596282959 \n",
            "\n",
            "The loss value is :  0.015537673607468605 \n",
            "\n",
            "The loss value is :  0.11217387020587921 \n",
            "\n",
            "The minimum loss obtained is: 0.11217387020587921\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b543340> and its information value is:  tensor(0.0814)\n",
            "Target node: 64562\n",
            "1-hop neighbors of A: {64558}\n",
            "2-hop neighbors of A: {64553}\n",
            "Out of neighborhood of A: {64548, 64549, 64550, 64551, 64552, 64554, 64555, 64556, 64557, 64559, 64560, 64561} \n",
            "\n",
            "The graph number 4321.0 contains: {64560, 64561, 64562, 64563, 64564, 64565, 64566, 64567, 64568, 64569, 64570}\n",
            "The loss value is :  0.020599376410245895 \n",
            "\n",
            "The loss value is :  0.15455976128578186 \n",
            "\n",
            "The loss value is :  0.014680425636470318 \n",
            "\n",
            "The loss value is :  0.030264582484960556 \n",
            "\n",
            "The loss value is :  0.027532465755939484 \n",
            "\n",
            "The loss value is :  0.07719188928604126 \n",
            "\n",
            "The loss value is :  0.023107174783945084 \n",
            "\n",
            "The loss value is :  0.10774349421262741 \n",
            "\n",
            "The loss value is :  0.010635251179337502 \n",
            "\n",
            "The loss value is :  0.11291090399026871 \n",
            "\n",
            "The loss value is :  0.033056799322366714 \n",
            "\n",
            "The loss value is :  0.08182546496391296 \n",
            "\n",
            "The loss value is :  0.020063327625393867 \n",
            "\n",
            "The loss value is :  0.10642135888338089 \n",
            "\n",
            "The loss value is :  0.02933778613805771 \n",
            "\n",
            "The loss value is :  0.1278185099363327 \n",
            "\n",
            "The minimum loss obtained is: 0.1278185099363327\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b542170> and its information value is:  tensor(0.0859)\n",
            "Target node: 64570\n",
            "1-hop neighbors of A: {64563}\n",
            "2-hop neighbors of A: {64560}\n",
            "Out of neighborhood of A: {64561, 64562, 64564, 64565, 64566, 64567, 64568, 64569} \n",
            "\n",
            "The graph number 4322.0 contains: {64576, 64577, 64578, 64579, 64580, 64581, 64582, 64583, 64584, 64585, 64563, 64570, 64571, 64572, 64573, 64574, 64575}\n",
            "The loss value is :  0.028402749449014664 \n",
            "\n",
            "The loss value is :  0.14869830012321472 \n",
            "\n",
            "The loss value is :  0.02105887606739998 \n",
            "\n",
            "The loss value is :  0.13697217404842377 \n",
            "\n",
            "The loss value is :  0.03605615347623825 \n",
            "\n",
            "The loss value is :  0.12587332725524902 \n",
            "\n",
            "The loss value is :  0.0316823311150074 \n",
            "\n",
            "The loss value is :  0.15146934986114502 \n",
            "\n",
            "The loss value is :  0.02151063270866871 \n",
            "\n",
            "The loss value is :  0.14250482618808746 \n",
            "\n",
            "The loss value is :  0.03715363144874573 \n",
            "\n",
            "The loss value is :  0.1382332593202591 \n",
            "\n",
            "The loss value is :  0.04095177352428436 \n",
            "\n",
            "The loss value is :  0.17958608269691467 \n",
            "\n",
            "The loss value is :  0.017880089581012726 \n",
            "\n",
            "The loss value is :  0.11759894341230392 \n",
            "\n",
            "The minimum loss obtained is: 0.11759894341230392\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b540df0> and its information value is:  tensor(0.0734)\n",
            "Target node: 64575\n",
            "1-hop neighbors of A: {64576, 64572}\n",
            "2-hop neighbors of A: {64577, 64578, 64570}\n",
            "Out of neighborhood of A: {64579, 64580, 64581, 64582, 64583, 64584, 64585, 64563, 64571, 64573, 64574} \n",
            "\n",
            "The graph number 4323.0 contains: {64584, 64585, 64586, 64587, 64588, 64589, 64590, 64591, 64592, 64593, 64594, 64595, 64596, 64597, 64598, 64599, 64600, 64601, 64602, 64603, 64554, 64556, 64557, 64558, 64574, 64575}\n",
            "The loss value is :  0.02401534840464592 \n",
            "\n",
            "The loss value is :  0.17500527203083038 \n",
            "\n",
            "The loss value is :  0.02867782674729824 \n",
            "\n",
            "The loss value is :  0.14827348291873932 \n",
            "\n",
            "The loss value is :  0.01920842006802559 \n",
            "\n",
            "The loss value is :  0.15604864060878754 \n",
            "\n",
            "The loss value is :  0.03410680964589119 \n",
            "\n",
            "The loss value is :  0.16901622712612152 \n",
            "\n",
            "The loss value is :  0.024367336183786392 \n",
            "\n",
            "The loss value is :  0.15481041371822357 \n",
            "\n",
            "The loss value is :  0.027165917679667473 \n",
            "\n",
            "The loss value is :  0.15124602615833282 \n",
            "\n",
            "The loss value is :  0.02574678510427475 \n",
            "\n",
            "The loss value is :  0.15277375280857086 \n",
            "\n",
            "The loss value is :  0.026130307465791702 \n",
            "\n",
            "The loss value is :  0.15477919578552246 \n",
            "\n",
            "The minimum loss obtained is: 0.15477919578552246\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b543f40> and its information value is:  tensor(0.0422)\n",
            "Target node: 64575\n",
            "1-hop neighbors of A: {64574}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {64584, 64585, 64586, 64587, 64588, 64589, 64590, 64591, 64592, 64593, 64594, 64595, 64596, 64597, 64598, 64599, 64600, 64601, 64602, 64603, 64554, 64556, 64557, 64558} \n",
            "\n",
            "The graph number 4324.0 contains: {64580, 64581, 64590, 64592, 64594, 64596, 64608, 64609, 64610, 64611, 64612, 64613, 64614, 64551, 64552, 64553, 64615, 64616, 64556, 64559, 64571, 64572, 64574, 64575}\n",
            "The loss value is :  0.043628185987472534 \n",
            "\n",
            "The loss value is :  0.14774636924266815 \n",
            "\n",
            "The loss value is :  0.027564911171793938 \n",
            "\n",
            "The loss value is :  0.15079589188098907 \n",
            "\n",
            "The loss value is :  0.031526241451501846 \n",
            "\n",
            "The loss value is :  0.16106265783309937 \n",
            "\n",
            "The loss value is :  0.021204976364970207 \n",
            "\n",
            "The loss value is :  0.1552843153476715 \n",
            "\n",
            "The loss value is :  0.03294815123081207 \n",
            "\n",
            "The loss value is :  0.1627502292394638 \n",
            "\n",
            "The loss value is :  0.04249186813831329 \n",
            "\n",
            "The loss value is :  0.11653462052345276 \n",
            "\n",
            "The loss value is :  0.0302178543061018 \n",
            "\n",
            "The loss value is :  0.15193410217761993 \n",
            "\n",
            "The loss value is :  0.029286455363035202 \n",
            "\n",
            "The loss value is :  0.14995482563972473 \n",
            "\n",
            "The minimum loss obtained is: 0.14995482563972473\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b542290> and its information value is:  tensor(0.0459)\n",
            "Target node: 64575\n",
            "1-hop neighbors of A: {64574}\n",
            "2-hop neighbors of A: {64616}\n",
            "Out of neighborhood of A: {64580, 64581, 64590, 64592, 64594, 64596, 64608, 64609, 64610, 64611, 64612, 64613, 64614, 64551, 64552, 64553, 64615, 64556, 64559, 64571, 64572} \n",
            "\n",
            "The graph number 4325.0 contains: {64577, 64578, 64633, 64580, 64589, 64591, 64592, 64593, 64594, 64631, 64599, 64603, 64617, 64618, 64619, 64620, 64621, 64622, 64623, 64624, 64625, 64626, 64627, 64628, 64629, 64566, 64630, 64632, 64569, 64634, 64635}\n",
            "The loss value is :  0.02575395628809929 \n",
            "\n",
            "The loss value is :  0.17107634246349335 \n",
            "\n",
            "The loss value is :  0.028112880885601044 \n",
            "\n",
            "The loss value is :  0.1595575362443924 \n",
            "\n",
            "The loss value is :  0.029761480167508125 \n",
            "\n",
            "The loss value is :  0.1662011742591858 \n",
            "\n",
            "The loss value is :  0.041982751339673996 \n",
            "\n",
            "The loss value is :  0.17065183818340302 \n",
            "\n",
            "The loss value is :  0.019764196127653122 \n",
            "\n",
            "The loss value is :  0.1671876460313797 \n",
            "\n",
            "The loss value is :  0.03312082216143608 \n",
            "\n",
            "The loss value is :  0.20643101632595062 \n",
            "\n",
            "The loss value is :  0.041697561740875244 \n",
            "\n",
            "The loss value is :  0.18025203049182892 \n",
            "\n",
            "The loss value is :  0.029402291402220726 \n",
            "\n",
            "The loss value is :  0.16916817426681519 \n",
            "\n",
            "The minimum loss obtained is: 0.16916817426681519\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b540c70> and its information value is:  tensor(0.0333)\n",
            "Target node: 64635\n",
            "1-hop neighbors of A: {64631}\n",
            "2-hop neighbors of A: {64634, 64630}\n",
            "Out of neighborhood of A: {64577, 64578, 64633, 64580, 64589, 64591, 64592, 64593, 64594, 64599, 64603, 64617, 64618, 64619, 64620, 64621, 64622, 64623, 64624, 64625, 64626, 64627, 64628, 64629, 64566, 64632, 64569} \n",
            "\n",
            "The graph number 4326.0 contains: {64640, 64641, 64642, 64643, 64644, 64631, 64632, 64633, 64634, 64635, 64636, 64637, 64638, 64639}\n",
            "The loss value is :  0.0314076766371727 \n",
            "\n",
            "The loss value is :  0.12235810607671738 \n",
            "\n",
            "The loss value is :  0.02461020089685917 \n",
            "\n",
            "The loss value is :  0.12128652632236481 \n",
            "\n",
            "The loss value is :  0.02464822679758072 \n",
            "\n",
            "The loss value is :  0.1383841335773468 \n",
            "\n",
            "The loss value is :  0.027610789984464645 \n",
            "\n",
            "The loss value is :  0.11932224035263062 \n",
            "\n",
            "The loss value is :  0.04826928302645683 \n",
            "\n",
            "The loss value is :  0.13693179190158844 \n",
            "\n",
            "The loss value is :  0.007173851132392883 \n",
            "\n",
            "The loss value is :  0.14114587008953094 \n",
            "\n",
            "The loss value is :  0.004255432635545731 \n",
            "\n",
            "The loss value is :  0.13719025254249573 \n",
            "\n",
            "The loss value is :  0.013690197840332985 \n",
            "\n",
            "The loss value is :  0.08864063769578934 \n",
            "\n",
            "The minimum loss obtained is: 0.08864063769578934\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b540be0> and its information value is:  tensor(0.1020)\n",
            "Target node: 64639\n",
            "1-hop neighbors of A: {64634}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {64640, 64641, 64642, 64643, 64644, 64631, 64632, 64633, 64635, 64636, 64637, 64638} \n",
            "\n",
            "The graph number 4327.0 contains: {64640, 64641, 64642, 64643, 64645, 64646, 64647, 64648, 64649, 64650, 64651, 64652, 64653, 64654, 64655, 64656, 64657, 64658, 64659, 64660, 64661, 64662, 64663, 64664, 64633, 64634, 64636, 64638, 64639}\n",
            "The loss value is :  0.03290354460477829 \n",
            "\n",
            "The loss value is :  0.15726059675216675 \n",
            "\n",
            "The loss value is :  0.04292520135641098 \n",
            "\n",
            "The loss value is :  0.16108889877796173 \n",
            "\n",
            "The loss value is :  0.03400275483727455 \n",
            "\n",
            "The loss value is :  0.15606799721717834 \n",
            "\n",
            "The loss value is :  0.03796154633164406 \n",
            "\n",
            "The loss value is :  0.17236043512821198 \n",
            "\n",
            "The loss value is :  0.029635082930326462 \n",
            "\n",
            "The loss value is :  0.17603616416454315 \n",
            "\n",
            "The loss value is :  0.02519470453262329 \n",
            "\n",
            "The loss value is :  0.16167093813419342 \n",
            "\n",
            "The loss value is :  0.027751699090003967 \n",
            "\n",
            "The loss value is :  0.16203148663043976 \n",
            "\n",
            "The loss value is :  0.018737563863396645 \n",
            "\n",
            "The loss value is :  0.16404178738594055 \n",
            "\n",
            "The minimum loss obtained is: 0.16404178738594055\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b5401c0> and its information value is:  tensor(0.0363)\n",
            "Target node: 64639\n",
            "1-hop neighbors of A: {64653, 64646}\n",
            "2-hop neighbors of A: {64649}\n",
            "Out of neighborhood of A: {64640, 64641, 64642, 64643, 64645, 64647, 64648, 64650, 64651, 64652, 64654, 64655, 64656, 64657, 64658, 64659, 64660, 64661, 64662, 64663, 64664, 64633, 64634, 64636, 64638} \n",
            "\n",
            "The graph number 4328.0 contains: {64663, 64664, 64665, 64666, 64667, 64668, 64669}\n",
            "The loss value is :  0.03461279347538948 \n",
            "\n",
            "The loss value is :  0.07589989900588989 \n",
            "\n",
            "The loss value is :  0.06582949310541153 \n",
            "\n",
            "The loss value is :  0.10736291110515594 \n",
            "\n",
            "The loss value is :  0.04588386416435242 \n",
            "\n",
            "The loss value is :  0.02096264436841011 \n",
            "\n",
            "The loss value is :  -0.05158774182200432 \n",
            "\n",
            "The loss value is :  0.11506028473377228 \n",
            "\n",
            "The loss value is :  -0.025003507733345032 \n",
            "\n",
            "The loss value is :  0.10534988343715668 \n",
            "\n",
            "The loss value is :  0.034079913049936295 \n",
            "\n",
            "The loss value is :  -0.005219228565692902 \n",
            "\n",
            "The loss value is :  0.017944348976016045 \n",
            "\n",
            "The loss value is :  0.2479657679796219 \n",
            "\n",
            "The loss value is :  -0.003397073596715927 \n",
            "\n",
            "The loss value is :  0.07817144691944122 \n",
            "\n",
            "The minimum loss obtained is: 0.07817144691944122\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b5418a0> and its information value is:  tensor(0.1577)\n",
            "Target node: 64669\n",
            "1-hop neighbors of A: {64665}\n",
            "2-hop neighbors of A: {64663}\n",
            "Out of neighborhood of A: {64664, 64666, 64667, 64668} \n",
            "\n",
            "The graph number 4329.0 contains: {64665, 64666, 64667, 64668, 64669, 64670, 64671, 64672, 64673, 64674, 64675, 64676, 64677, 64678, 64679, 64680}\n",
            "The loss value is :  0.02765904739499092 \n",
            "\n",
            "The loss value is :  0.13015173375606537 \n",
            "\n",
            "The loss value is :  0.012424045242369175 \n",
            "\n",
            "The loss value is :  0.14906349778175354 \n",
            "\n",
            "The loss value is :  0.041462644934654236 \n",
            "\n",
            "The loss value is :  0.1943029761314392 \n",
            "\n",
            "The loss value is :  0.0032791532576084137 \n",
            "\n",
            "The loss value is :  0.13972648978233337 \n",
            "\n",
            "The loss value is :  0.015370762906968594 \n",
            "\n",
            "The loss value is :  0.12394225597381592 \n",
            "\n",
            "The loss value is :  0.023070169612765312 \n",
            "\n",
            "The loss value is :  0.09500936418771744 \n",
            "\n",
            "The loss value is :  0.02138407900929451 \n",
            "\n",
            "The loss value is :  0.1395958662033081 \n",
            "\n",
            "The loss value is :  0.003920728340744972 \n",
            "\n",
            "The loss value is :  0.16820326447486877 \n",
            "\n",
            "The minimum loss obtained is: 0.16820326447486877\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b5427a0> and its information value is:  tensor(0.0481)\n",
            "Target node: 64680\n",
            "1-hop neighbors of A: {64673}\n",
            "2-hop neighbors of A: {64669}\n",
            "Out of neighborhood of A: {64665, 64666, 64667, 64668, 64670, 64671, 64672, 64674, 64675, 64676, 64677, 64678, 64679} \n",
            "\n",
            "The graph number 4330.0 contains: {64673, 64680, 64681, 64682, 64683, 64684, 64685, 64686, 64687, 64688}\n",
            "The loss value is :  0.02749674580991268 \n",
            "\n",
            "The loss value is :  0.11921200156211853 \n",
            "\n",
            "The loss value is :  -0.011673975735902786 \n",
            "\n",
            "The loss value is :  0.11196307092905045 \n",
            "\n",
            "The loss value is :  0.04788147658109665 \n",
            "\n",
            "The loss value is :  0.09007134288549423 \n",
            "\n",
            "The loss value is :  0.031327564269304276 \n",
            "\n",
            "The loss value is :  0.07756400853395462 \n",
            "\n",
            "The loss value is :  0.023474780842661858 \n",
            "\n",
            "The loss value is :  0.08304456621408463 \n",
            "\n",
            "The loss value is :  0.023825272917747498 \n",
            "\n",
            "The loss value is :  0.062466662377119064 \n",
            "\n",
            "The loss value is :  0.01126240286976099 \n",
            "\n",
            "The loss value is :  0.07229578495025635 \n",
            "\n",
            "The loss value is :  0.020358944311738014 \n",
            "\n",
            "The loss value is :  0.12299516797065735 \n",
            "\n",
            "The minimum loss obtained is: 0.12299516797065735\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b543700> and its information value is:  tensor(0.0949)\n",
            "Target node: 64688\n",
            "1-hop neighbors of A: {64684}\n",
            "2-hop neighbors of A: {64682}\n",
            "Out of neighborhood of A: {64673, 64680, 64681, 64683, 64685, 64686, 64687} \n",
            "\n",
            "The graph number 4331.0 contains: {64684, 64685, 64686, 64687, 64688, 64689, 64690, 64691, 64692, 64693, 64694, 64695}\n",
            "The loss value is :  -0.00371616892516613 \n",
            "\n",
            "The loss value is :  0.09663990139961243 \n",
            "\n",
            "The loss value is :  0.005583399906754494 \n",
            "\n",
            "The loss value is :  0.12501710653305054 \n",
            "\n",
            "The loss value is :  0.030029989778995514 \n",
            "\n",
            "The loss value is :  0.15438596904277802 \n",
            "\n",
            "The loss value is :  0.020149923861026764 \n",
            "\n",
            "The loss value is :  0.15533456206321716 \n",
            "\n",
            "The loss value is :  0.004157716408371925 \n",
            "\n",
            "The loss value is :  0.08395186066627502 \n",
            "\n",
            "The loss value is :  0.0032945089042186737 \n",
            "\n",
            "The loss value is :  0.12349603325128555 \n",
            "\n",
            "The loss value is :  0.014999523758888245 \n",
            "\n",
            "The loss value is :  0.10096423327922821 \n",
            "\n",
            "The loss value is :  0.04552168399095535 \n",
            "\n",
            "The loss value is :  0.145386204123497 \n",
            "\n",
            "The minimum loss obtained is: 0.145386204123497\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b543cd0> and its information value is:  tensor(0.0711)\n",
            "Target node: 64695\n",
            "1-hop neighbors of A: {64688}\n",
            "2-hop neighbors of A: {64684}\n",
            "Out of neighborhood of A: {64685, 64686, 64687, 64689, 64690, 64691, 64692, 64693, 64694} \n",
            "\n",
            "The graph number 4332.0 contains: {64704, 64689, 64690, 64692, 64693, 64695, 64696, 64697, 64698, 64699, 64700, 64701, 64702, 64703}\n",
            "The loss value is :  0.0313439816236496 \n",
            "\n",
            "The loss value is :  0.1181737557053566 \n",
            "\n",
            "The loss value is :  0.03891044110059738 \n",
            "\n",
            "The loss value is :  0.13755467534065247 \n",
            "\n",
            "The loss value is :  0.011158225126564503 \n",
            "\n",
            "The loss value is :  0.18680405616760254 \n",
            "\n",
            "The loss value is :  0.029147710651159286 \n",
            "\n",
            "The loss value is :  0.106577567756176 \n",
            "\n",
            "The loss value is :  0.016204457730054855 \n",
            "\n",
            "The loss value is :  0.12392759323120117 \n",
            "\n",
            "The loss value is :  0.01797247864305973 \n",
            "\n",
            "The loss value is :  0.13727112114429474 \n",
            "\n",
            "The loss value is :  0.01704002358019352 \n",
            "\n",
            "The loss value is :  0.1441921591758728 \n",
            "\n",
            "The loss value is :  0.03671710193157196 \n",
            "\n",
            "The loss value is :  0.1350395530462265 \n",
            "\n",
            "The minimum loss obtained is: 0.1350395530462265\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b541de0> and its information value is:  tensor(0.0696)\n",
            "Target node: 64703\n",
            "1-hop neighbors of A: {64704, 64701}\n",
            "2-hop neighbors of A: {64697}\n",
            "Out of neighborhood of A: {64689, 64690, 64692, 64693, 64695, 64696, 64698, 64699, 64700, 64702} \n",
            "\n",
            "The graph number 4333.0 contains: {64687, 64688, 64689, 64690, 64692, 64693, 64695, 64696, 64697, 64698, 64699, 64700, 64701, 64702, 64703, 64704, 64705, 64706, 64707, 64708, 64709, 64710, 64711, 64712, 64713, 64714, 64715, 64716, 64717, 64718, 64719, 64720, 64721, 64722, 64723}\n",
            "The loss value is :  0.03498150780797005 \n",
            "\n",
            "The loss value is :  0.17088279128074646 \n",
            "\n",
            "The loss value is :  0.03739731386303902 \n",
            "\n",
            "The loss value is :  0.1932813823223114 \n",
            "\n",
            "The loss value is :  0.024361148476600647 \n",
            "\n",
            "The loss value is :  0.18549808859825134 \n",
            "\n",
            "The loss value is :  0.037347614765167236 \n",
            "\n",
            "The loss value is :  0.17001600563526154 \n",
            "\n",
            "The loss value is :  0.03058733604848385 \n",
            "\n",
            "The loss value is :  0.1695326268672943 \n",
            "\n",
            "The loss value is :  0.03546776622533798 \n",
            "\n",
            "The loss value is :  0.17522507905960083 \n",
            "\n",
            "The loss value is :  0.03252292424440384 \n",
            "\n",
            "The loss value is :  0.18377144634723663 \n",
            "\n",
            "The loss value is :  0.03196362778544426 \n",
            "\n",
            "The loss value is :  0.16516290605068207 \n",
            "\n",
            "The minimum loss obtained is: 0.16516290605068207\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974dac3fa0> and its information value is:  tensor(0.0335)\n",
            "Target node: 64723\n",
            "1-hop neighbors of A: {64704}\n",
            "2-hop neighbors of A: {64722}\n",
            "Out of neighborhood of A: {64687, 64688, 64689, 64690, 64692, 64693, 64695, 64696, 64697, 64698, 64699, 64700, 64701, 64702, 64703, 64705, 64706, 64707, 64708, 64709, 64710, 64711, 64712, 64713, 64714, 64715, 64716, 64717, 64718, 64719, 64720, 64721} \n",
            "\n",
            "The graph number 4334.0 contains: {64704, 64723, 64724, 64725, 64726, 64727, 64728, 64729, 64730, 64731, 64732, 64733, 64734, 64735, 64736, 64737, 64738, 64739, 64740, 64741, 64742, 64743, 64744, 64745, 64746, 64747, 64748, 64749, 64750}\n",
            "The loss value is :  0.016111180186271667 \n",
            "\n",
            "The loss value is :  0.15009106695652008 \n",
            "\n",
            "The loss value is :  0.02608797699213028 \n",
            "\n",
            "The loss value is :  0.15501052141189575 \n",
            "\n",
            "The loss value is :  0.028717007488012314 \n",
            "\n",
            "The loss value is :  0.1777590960264206 \n",
            "\n",
            "The loss value is :  0.03080695867538452 \n",
            "\n",
            "The loss value is :  0.16897250711917877 \n",
            "\n",
            "The loss value is :  0.02573198825120926 \n",
            "\n",
            "The loss value is :  0.17149104177951813 \n",
            "\n",
            "The loss value is :  0.027672164142131805 \n",
            "\n",
            "The loss value is :  0.14856784045696259 \n",
            "\n",
            "The loss value is :  0.03239307552576065 \n",
            "\n",
            "The loss value is :  0.14174142479896545 \n",
            "\n",
            "The loss value is :  0.04427378997206688 \n",
            "\n",
            "The loss value is :  0.16500872373580933 \n",
            "\n",
            "The minimum loss obtained is: 0.16500872373580933\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b542080> and its information value is:  tensor(0.0359)\n",
            "Target node: 64750\n",
            "1-hop neighbors of A: {64747}\n",
            "2-hop neighbors of A: {64744, 64748, 64749}\n",
            "Out of neighborhood of A: {64704, 64723, 64724, 64725, 64726, 64727, 64728, 64729, 64730, 64731, 64732, 64733, 64734, 64735, 64736, 64737, 64738, 64739, 64740, 64741, 64742, 64743, 64745, 64746} \n",
            "\n",
            "The graph number 4335.0 contains: {64732, 64733, 64734, 64735, 64736, 64737, 64738, 64739, 64740, 64741, 64742, 64743, 64745, 64746, 64752, 64753, 64754, 64755, 64756, 64757, 64758, 64759, 64760, 64761, 64762, 64763}\n",
            "The loss value is :  0.027720153331756592 \n",
            "\n",
            "The loss value is :  0.15483516454696655 \n",
            "\n",
            "The loss value is :  0.021258797496557236 \n",
            "\n",
            "The loss value is :  0.16444255411624908 \n",
            "\n",
            "The loss value is :  0.035050585865974426 \n",
            "\n",
            "The loss value is :  0.12754234671592712 \n",
            "\n",
            "The loss value is :  0.031081201508641243 \n",
            "\n",
            "The loss value is :  0.13485778868198395 \n",
            "\n",
            "The loss value is :  0.020735494792461395 \n",
            "\n",
            "The loss value is :  0.15899313986301422 \n",
            "\n",
            "The loss value is :  0.029474599286913872 \n",
            "\n",
            "The loss value is :  0.1583501398563385 \n",
            "\n",
            "The loss value is :  0.04246163368225098 \n",
            "\n",
            "The loss value is :  0.1563732922077179 \n",
            "\n",
            "The loss value is :  0.034921806305646896 \n",
            "\n",
            "The loss value is :  0.15538030862808228 \n",
            "\n",
            "The minimum loss obtained is: 0.15538030862808228\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b541660> and its information value is:  tensor(0.0419)\n",
            "Target node: 64763\n",
            "1-hop neighbors of A: {64745}\n",
            "2-hop neighbors of A: {64746}\n",
            "Out of neighborhood of A: {64732, 64733, 64734, 64735, 64736, 64737, 64738, 64739, 64740, 64741, 64742, 64743, 64752, 64753, 64754, 64755, 64756, 64757, 64758, 64759, 64760, 64761, 64762} \n",
            "\n",
            "The graph number 4336.0 contains: {64768, 64769, 64770, 64771, 64772, 64773, 64774, 64775, 64776, 64777, 64778, 64779, 64780, 64781, 64782, 64783, 64745, 64746, 64750, 64763, 64764, 64765, 64766, 64767}\n",
            "The loss value is :  0.03147615119814873 \n",
            "\n",
            "The loss value is :  0.1617717742919922 \n",
            "\n",
            "The loss value is :  0.023186074569821358 \n",
            "\n",
            "The loss value is :  0.15338365733623505 \n",
            "\n",
            "The loss value is :  0.010182689875364304 \n",
            "\n",
            "The loss value is :  0.14943927526474 \n",
            "\n",
            "The loss value is :  0.02317633107304573 \n",
            "\n",
            "The loss value is :  0.13453705608844757 \n",
            "\n",
            "The loss value is :  0.01940150186419487 \n",
            "\n",
            "The loss value is :  0.13993613421916962 \n",
            "\n",
            "The loss value is :  0.015918761491775513 \n",
            "\n",
            "The loss value is :  0.16019617021083832 \n",
            "\n",
            "The loss value is :  0.03756718337535858 \n",
            "\n",
            "The loss value is :  0.13463576138019562 \n",
            "\n",
            "The loss value is :  0.0345088429749012 \n",
            "\n",
            "The loss value is :  0.1424957811832428 \n",
            "\n",
            "The minimum loss obtained is: 0.1424957811832428\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b543820> and its information value is:  tensor(0.0498)\n",
            "Target node: 64767\n",
            "1-hop neighbors of A: {64770, 64766}\n",
            "2-hop neighbors of A: {64768, 64769, 64774, 64775}\n",
            "Out of neighborhood of A: {64771, 64772, 64773, 64776, 64777, 64778, 64779, 64780, 64781, 64782, 64783, 64745, 64746, 64750, 64763, 64764, 64765} \n",
            "\n",
            "The graph number 4337.0 contains: {64777, 64779, 64781, 64782, 64783, 64784, 64785, 64786, 64787, 64788, 64789, 64790, 64791, 64792, 64793, 64794, 64795, 64796, 64797}\n",
            "The loss value is :  0.03360974043607712 \n",
            "\n",
            "The loss value is :  0.1628578156232834 \n",
            "\n",
            "The loss value is :  0.04387432709336281 \n",
            "\n",
            "The loss value is :  0.12910300493240356 \n",
            "\n",
            "The loss value is :  0.0451589934527874 \n",
            "\n",
            "The loss value is :  0.15926970541477203 \n",
            "\n",
            "The loss value is :  0.013318890705704689 \n",
            "\n",
            "The loss value is :  0.14586730301380157 \n",
            "\n",
            "The loss value is :  0.025669069960713387 \n",
            "\n",
            "The loss value is :  0.15470841526985168 \n",
            "\n",
            "The loss value is :  0.012962144799530506 \n",
            "\n",
            "The loss value is :  0.1576654464006424 \n",
            "\n",
            "The loss value is :  0.027480551972985268 \n",
            "\n",
            "The loss value is :  0.14607146382331848 \n",
            "\n",
            "The loss value is :  0.053200457245111465 \n",
            "\n",
            "The loss value is :  0.12816134095191956 \n",
            "\n",
            "The minimum loss obtained is: 0.12816134095191956\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The best model is:  <__main__.NCM object at 0x7f974b541f60> and its information value is:  tensor(0.0634)\n",
            "Target node: 64797\n",
            "1-hop neighbors of A: {64792}\n",
            "2-hop neighbors of A: {64788}\n",
            "Out of neighborhood of A: {64777, 64779, 64781, 64782, 64783, 64784, 64785, 64786, 64787, 64789, 64790, 64791, 64793, 64794, 64795, 64796} \n",
            "\n",
            "[0.12816134095191956, 0.12816134095191956]\n",
            "The time of the calculation was :  9700.296605587006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AIDS first graph results"
      ],
      "metadata": {
        "id": "LOy1osWboYkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import torch\n",
        "from itertools import product\n",
        "\n",
        "AIDS = \"/content/drive/MyDrive/data/AIDS/\"\n",
        "AIDS_df = pd.read_csv(AIDS + 'AIDS_A.txt', sep=',', header=None, names=['from', 'to'])\n",
        "AIDS_graph_indicator = pd.read_csv(AIDS + 'AIDS_graph_indicator.txt', header=None, names=['graph_id'])\n",
        "AIDS_node_labels = pd.read_csv(AIDS + 'AIDS_node_labels.txt', header=None, names=['node_label'])\n",
        "AIDS_df['graph_id'] = AIDS_graph_indicator\n",
        "AIDS_df['node_label'] = AIDS_node_labels\n",
        "\n",
        "grouped = AIDS_df.groupby('graph_id')\n",
        "AIDS_causal_graphs = {}\n",
        "for graph_id, group in grouped:\n",
        "    V = set(group['from']).union(set(group['to']))\n",
        "    edges = list(zip(group['from'], group['to'])) + list(zip(group['to'], group['from']))\n",
        "    AIDS_causal_graphs[graph_id] = CausalGraph(V=V, path=edges)\n",
        "\n",
        "cg = AIDS_causal_graphs[1.0]\n",
        "print(f\"The graph contains: {cg.set_v}\",'\\n')\n",
        "cg.plot()\n",
        "\n",
        "# hyperparameters\n",
        "num_epochs = 2\n",
        "learning_rates = [0.001, 0.002, 0.005,0.01]\n",
        "hidden_sizes = [32, 64, 128, 256]\n",
        "num_layers = [1, 2, 3, 4]\n",
        "lambdas = [0.01, 0.05, .1,.2,.3]\n",
        "hyperparameters = product(learning_rates, hidden_sizes, num_layers, lambdas)\n",
        "total_loss = []\n",
        "min_loss = float('inf')\n",
        "best_hyperparams = None\n",
        "for i, hyperparams in enumerate(hyperparameters):\n",
        "    learning_rate, h_size, h_layers, lambdas = hyperparams\n",
        "    print(f'Training with learning rate: {learning_rate}, h_size: {h_size}, h_layers: {h_layers}, lambdas: {lambdas}','\\n')\n",
        "    causal_loss,best_ncm_model,p_do = train(cg, lambdas, learning_rate, h_size, h_layers, num_epochs)\n",
        "    total_loss.append(causal_loss)\n",
        "    if causal_loss < min_loss:\n",
        "        best_model = best_ncm_model\n",
        "        best_intervention = p_do\n",
        "        min_loss = causal_loss\n",
        "        best_hyperparams = hyperparams\n",
        "\n",
        "print('The minimum loss obtained is:', min_loss)\n",
        "print('The best hyperparameters are:',f'Training with learning rate: {best_hyperparams[0]}, h_size: {best_hyperparams[1]}, h_layers: {best_hyperparams[2]}, lambdas: {best_hyperparams[3]}','\\n')\n",
        "print('The best model is: ', best_model,'and its identified query value is: ',best_intervention.data)\n",
        "print(f\"Target node: {best_model.graph.target_node}\")\n",
        "print(f\"1-hop neighbors of A: {best_model.graph.one_hop_neighbors}\")\n",
        "print(f\"2-hop neighbors of A: {best_model.graph.two_hop_neighbors}\")\n",
        "print(f\"Out of neighborhood of A: {best_model.graph.out_of_neighborhood}\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(total_loss)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss over epochs')\n",
        "plt.savefig('AIDS NCM Loss over epochs.png')\n",
        "# plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PFXlaRxcom2t",
        "outputId": "f26b344d-a4be-4340-999c-fc3c611c9e62"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The graph contains: {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 21, 25, 27, 28, 38, 46} \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with learning rate: 0.001, h_size: 32, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.01831599324941635 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.14511851966381073 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.30958062410354614 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.5822744965553284 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9973894357681274 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.020395437255501747 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.12863008677959442 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.3202058970928192 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.601820170879364 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9699006080627441 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.031863801181316376 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.17673680186271667 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.2602268159389496 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.597999095916748 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9458838701248169 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.0259169302880764 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.14471279084682465 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.2996518313884735 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.5317598581314087 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.8431691527366638 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.034048523753881454 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.12798970937728882 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.36911654472351074 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6592638492584229 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9345654845237732 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.018518738448619843 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.18919041752815247 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.31070882081985474 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.652928352355957 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9694850444793701 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.04140632227063179 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.17709974944591522 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.333757221698761 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6079005002975464 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9191088676452637 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.03240467235445976 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.17424431443214417 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.29908615350723267 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.600086510181427 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  1.0221827030181885 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.022029288113117218 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.10892991721630096 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.27774810791015625 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6127024292945862 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9211698174476624 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.015831392258405685 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.14348693192005157 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.29820945858955383 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6710896492004395 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9539681077003479 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.0283628199249506 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.1421850323677063 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.30472126603126526 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6530511975288391 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.902344822883606 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.022503692656755447 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.14222529530525208 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.31921884417533875 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.5568037033081055 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.8333690166473389 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.026053326204419136 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.160384863615036 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.3052758276462555 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6656327247619629 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9374920129776001 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.02826104685664177 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.14551734924316406 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.2956206500530243 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6226545572280884 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9167375564575195 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.028513338416814804 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.15295463800430298 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.3103979527950287 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6199057102203369 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.8513242602348328 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.032386306673288345 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.1584237515926361 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.3235195577144623 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6049237251281738 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  1.0250213146209717 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.019826054573059082 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.16731853783130646 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.3407261371612549 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.5903330445289612 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9805870056152344 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.011187789961695671 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.1435769945383072 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.32532835006713867 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.5974872708320618 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.89201420545578 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.015696890652179718 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.16670985519886017 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.31513431668281555 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.5786166787147522 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.8944500088691711 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.019817139953374863 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.1728011816740036 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.2554371953010559 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6270325183868408 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9117499589920044 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.016467221081256866 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.1415247768163681 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.32332637906074524 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.5950026512145996 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.8067432641983032 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.03680163249373436 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.14479272067546844 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.27735403180122375 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6864022612571716 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.8799582719802856 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.039941370487213135 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.14156614243984222 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.26134905219078064 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6825541853904724 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9315066933631897 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.0042822156101465225 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.15045756101608276 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.2885052263736725 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6056705117225647 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.8492719531059265 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.025220083072781563 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.12429788708686829 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.29766398668289185 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.5983484387397766 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.8955358862876892 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.03326832875609398 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.1519167423248291 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.29910242557525635 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.5916388034820557 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9153688549995422 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.031827859580516815 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.16053073108196259 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.30817824602127075 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.5690749287605286 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9054828882217407 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.03196737542748451 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.1694650799036026 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.27661463618278503 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6167717576026917 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9741131067276001 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.03170938044786453 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.142585888504982 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.29399535059928894 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.5552969574928284 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9261085391044617 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.011235473677515984 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.14887653291225433 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.2947176992893219 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6163290143013 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9605726599693298 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.02984272502362728 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.14860323071479797 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.2988809049129486 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6008649468421936 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9776421189308167 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.03617407754063606 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.16488850116729736 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.3228745758533478 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6371989250183105 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9311226606369019 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.0386241190135479 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.16518954932689667 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.2838997542858124 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6700239777565002 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9179518818855286 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.02818281389772892 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.11542890220880508 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.33364951610565186 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.5838531255722046 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9507878422737122 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.031345706433057785 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.12010415643453598 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.29545682668685913 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.5671457052230835 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9697701930999756 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.03388529270887375 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.1666252464056015 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.30792179703712463 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6043373346328735 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.8901507258415222 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.0340220183134079 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.15104937553405762 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.3423912525177002 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6042457818984985 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9288444519042969 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.037147004157304764 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.1415959596633911 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.3198164701461792 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.698107898235321 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9330431818962097 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.027151895686984062 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.14905127882957458 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.293705016374588 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.5830408334732056 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  1.039361834526062 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.035252589732408524 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.1553417295217514 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.32344892621040344 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6591238975524902 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.8687935471534729 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.030445599928498268 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.1532556265592575 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.30535659193992615 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6069744825363159 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9827386140823364 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.022684386000037193 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.14028555154800415 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.33652976155281067 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6776198148727417 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.969577968120575 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.029474034905433655 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.14474959671497345 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.3225190043449402 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6096579432487488 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9690297245979309 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.03404248505830765 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.13870291411876678 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.277297705411911 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6528752446174622 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.888358473777771 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.028405608609318733 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.15539398789405823 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.27214136719703674 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.5830798149108887 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9918908476829529 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.029433803632855415 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.1793525665998459 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.2979837656021118 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.656692624092102 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9468823671340942 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.012226535007357597 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.1429632157087326 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.32333245873451233 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.5736140012741089 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.8922337889671326 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.022707046940922737 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.165693461894989 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.2920272946357727 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.5519964694976807 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9414774179458618 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.02604501321911812 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.12304678559303284 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.33124840259552 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.683076798915863 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9564939737319946 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.027102403342723846 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.13258911669254303 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.2928946316242218 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6275125741958618 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9135218858718872 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.020744655281305313 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.10617950558662415 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.3283836841583252 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6200746297836304 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9219100475311279 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.028610622510313988 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.16374148428440094 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.2766526937484741 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6215810179710388 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9208360314369202 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.03369942307472229 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.14533263444900513 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.3003987967967987 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6556823253631592 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9877679347991943 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.02398063801229 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.14051823318004608 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.28597110509872437 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.584941565990448 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9577679634094238 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.028507299721240997 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.1571412831544876 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.2542698383331299 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.5667251348495483 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9702596664428711 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.027241408824920654 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.12360230088233948 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.30534031987190247 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6236653327941895 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.8930027484893799 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.03524679318070412 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.1655387282371521 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.27916616201400757 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.5887472629547119 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.8758177757263184 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.028764527291059494 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.1508225053548813 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.301324725151062 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6387341022491455 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9226755499839783 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.026724856346845627 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.1476747691631317 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.3212341368198395 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6274368762969971 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9587737321853638 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.024556737393140793 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.13867107033729553 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.3216097950935364 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6000614762306213 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9188240170478821 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.03138504922389984 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.14710736274719238 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.3305363655090332 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.5984741449356079 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.8992593884468079 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.02336183562874794 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.15101152658462524 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.3431260287761688 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6850817799568176 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  1.1005467176437378 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.01882370375096798 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.17104142904281616 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.34876549243927 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.5767321586608887 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.928912341594696 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.027489788830280304 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.1672586351633072 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.3459419906139374 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.58925461769104 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9907001256942749 \n",
            "\n",
            "The minimum loss obtained is: 0.0042822156101465225\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 64, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The best model is:  <__main__.NCM object at 0x7faa90b5ded0> and its identified query value is:  tensor(0.0721)\n",
            "Target node: 46\n",
            "1-hop neighbors of A: {9}\n",
            "2-hop neighbors of A: {8, 10}\n",
            "Out of neighborhood of A: {1, 2, 3, 4, 5, 6, 7, 11, 12, 13, 14, 15, 16, 17, 21, 25, 27, 28, 38}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MUTAG first graph results"
      ],
      "metadata": {
        "id": "1AMoRKQkoket"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "import pandas as pd\n",
        "from itertools import product\n",
        "import pandas as pd\n",
        "\n",
        "Mutagenicity = \"/content/drive/MyDrive/data/Mutagenicity/\"\n",
        "# Load the data\n",
        "Mutagenicity_df = pd.read_csv(Mutagenicity + 'Mutagenicity_A.txt', sep=',', header=None, names=['from', 'to'])\n",
        "Mutagenicity_graph_indicator = pd.read_csv(Mutagenicity + 'Mutagenicity_graph_indicator.txt', header=None, names=['graph_id'])\n",
        "Mutagenicity_node_labels = pd.read_csv(Mutagenicity + 'Mutagenicity_node_labels.txt', header=None, names=['node_label'])\n",
        "# print(Mutagenicity_edges_df,Mutagenicity_graph_indicator,Mutagenicity_node_labels)\n",
        "Mutagenicity_df['graph_id'] = Mutagenicity_graph_indicator\n",
        "Mutagenicity_df['node_label'] = Mutagenicity_node_labels\n",
        "\n",
        "# Group edges by graph id\n",
        "grouped = Mutagenicity_df.groupby('graph_id')\n",
        "# Dictionary to hold each graph\n",
        "Mutagenicity_causal_graphs = {}\n",
        "for graph_id, group in grouped:\n",
        "    # Create a set of vertices for each group\n",
        "    V = set(group['from']).union(set(group['to']))\n",
        "    # Create a list of edges for each group\n",
        "    edges = list(zip(group['from'], group['to'])) + list(zip(group['to'], group['from']))\n",
        "    # Create a CausalGraph for each group\n",
        "    Mutagenicity_causal_graphs[graph_id] = CausalGraph(V=V, path=edges)\n",
        "    # Mutagenicity_causal_graphs[graph_id].plot()\n",
        "\n",
        "cg = Mutagenicity_causal_graphs[1.0]\n",
        "print(f\"The graph contains: {cg.set_v}\",'\\n')\n",
        "cg.plot()\n",
        "\n",
        "# hyperparameters\n",
        "num_epochs = 2\n",
        "learning_rates = [0.001, 0.002, 0.005,0.01]\n",
        "hidden_sizes = [32, 64, 128, 256]\n",
        "num_layers = [1, 2, 3, 4]\n",
        "lambdas = [0.01, 0.05, .1,.2,.3]\n",
        "hyperparameters = product(learning_rates, hidden_sizes, num_layers, lambdas)\n",
        "total_loss = []\n",
        "min_loss = float('inf')\n",
        "best_hyperparams = None\n",
        "for i, hyperparams in enumerate(hyperparameters):\n",
        "    learning_rate, h_size, h_layers, lambdas = hyperparams\n",
        "    print(f'Training with learning rate: {learning_rate}, h_size: {h_size}, h_layers: {h_layers}, lambdas: {lambdas}','\\n')\n",
        "    causal_loss,best_ncm_model,p_do = train(cg, lambdas, learning_rate, h_size, h_layers, num_epochs)\n",
        "    total_loss.append(causal_loss)\n",
        "    if causal_loss < min_loss:\n",
        "        best_model = best_ncm_model\n",
        "        best_intervention = p_do\n",
        "        min_loss = causal_loss\n",
        "        best_hyperparams = hyperparams\n",
        "\n",
        "print('The minimum loss obtained is:', min_loss)\n",
        "print('The best hyperparameters are:',f'Training with learning rate: {best_hyperparams[0]}, h_size: {best_hyperparams[1]}, h_layers: {best_hyperparams[2]}, lambdas: {best_hyperparams[3]}','\\n')\n",
        "print('The best model is: ', best_model,'and its identified query value is: ',best_intervention.data)\n",
        "print(f\"Target node: {best_model.graph.target_node}\")\n",
        "print(f\"1-hop neighbors of A: {best_model.graph.one_hop_neighbors}\")\n",
        "print(f\"2-hop neighbors of A: {best_model.graph.two_hop_neighbors}\")\n",
        "print(f\"Out of neighborhood of A: {best_model.graph.out_of_neighborhood}\")\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(total_loss)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss over epochs')\n",
        "plt.savefig('Mutag NCM Loss over epochs.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "obGG43xgodQk",
        "outputId": "eb0a536a-125b-4408-f0ff-a1b08581b5ec"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The graph contains: {1, 2, 3, 4, 5, 6, 7, 8, 9} \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with learning rate: 0.001, h_size: 32, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.0026984065771102905 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  -0.009418107569217682 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.21651853621006012 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.36176666617393494 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.735058605670929 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  -0.001188766211271286 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.08079919219017029 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.18753278255462646 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.426863431930542 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.5563783049583435 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.009813938289880753 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.09639905393123627 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.1523537039756775 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.31580400466918945 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.6135721802711487 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  -0.012686097994446754 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.010689757764339447 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.11683520674705505 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.31851401925086975 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.6692156791687012 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.00964527577161789 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.1276310533285141 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.24473963677883148 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.5073640942573547 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.9082962274551392 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.005848029628396034 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.054880540817976 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.23684470355510712 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.3984440565109253 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.590738832950592 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.03221224620938301 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.10182195156812668 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.14046873152256012 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.4754592478275299 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.41550126671791077 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.05987007915973663 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.10744680464267731 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.213334321975708 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.33718404173851013 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.8196516036987305 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.04782850667834282 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.11753947287797928 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.25599923729896545 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.4365709722042084 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.6724443435668945 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  -0.01338377594947815 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.21148982644081116 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.24505984783172607 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.49864253401756287 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.7298451662063599 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.021064456552267075 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.1240527331829071 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.15572935342788696 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.3832447826862335 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.8128128051757812 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.01592659391462803 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.09150755405426025 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.23076248168945312 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.5619691610336304 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.627829372882843 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.01580330729484558 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.1119433119893074 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.21206015348434448 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.47960710525512695 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.44771113991737366 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.03919501602649689 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.13170699775218964 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.25797319412231445 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.3557383716106415 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.5620518326759338 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.06337497383356094 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.12564855813980103 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.18040674924850464 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.38451042771339417 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.7092443704605103 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  -0.017441434785723686 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.10136828571557999 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.09891146421432495 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.3917428255081177 \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.8890897631645203 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.016374316066503525 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.09498381614685059 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.2039756029844284 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.4186912477016449 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.5691756010055542 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  -0.05897645279765129 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.09680530428886414 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.23122018575668335 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6288836002349854 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.7780920267105103 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  -0.016556981950998306 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.1988302320241928 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.24802455306053162 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.41738954186439514 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.6446365714073181 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  -0.015729760751128197 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.1675276905298233 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.3322236239910126 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.2922113239765167 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.5676343441009521 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.03437568619847298 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.11219790577888489 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.17622573673725128 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.32695502042770386 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.6774366497993469 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  -0.005255337804555893 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.12231029570102692 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.33603042364120483 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.3881540298461914 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.5229427218437195 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.03311855345964432 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.08084559440612793 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.21589423716068268 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.3910231292247772 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.7169445753097534 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.006004120223224163 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.1396007537841797 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.1950613558292389 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.32678988575935364 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.887715220451355 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  -0.07062405347824097 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.15199030935764313 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.28586018085479736 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.4622023105621338 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.7191066741943359 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.02651003561913967 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.15084072947502136 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.13800469040870667 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.45494067668914795 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.7056959271430969 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.03171302378177643 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.14616243541240692 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.22210639715194702 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.34880098700523376 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.6306947469711304 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.0306688342243433 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.06447651982307434 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.2924272418022156 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.45267510414123535 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.7359065413475037 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.05718096345663071 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.0772668868303299 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.2515828311443329 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.2971554398536682 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.42984461784362793 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.030297476798295975 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.0345706082880497 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.27474886178970337 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.4211353659629822 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.5333443284034729 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  -0.02886616811156273 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.13342615962028503 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.2002781480550766 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.5041467547416687 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.44737508893013 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.02905777283012867 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.10568885505199432 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.18026202917099 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.4671315848827362 \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.7977645397186279 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  -0.012110034003853798 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.12745173275470734 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.23981143534183502 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.41092783212661743 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.7056160569190979 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.07513999938964844 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.10299951583147049 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.24620476365089417 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.5035156011581421 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.6289068460464478 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  -0.011690890416502953 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.1151902824640274 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.26034095883369446 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.4046003520488739 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.6977980732917786 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  -0.007070537656545639 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.10681094229221344 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.2677948772907257 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.45061221718788147 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.8201047778129578 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.002468632534146309 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.06886572390794754 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.14702579379081726 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.4983794391155243 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.7193765044212341 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.05094650387763977 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.12898088991641998 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.28234654664993286 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.5101097822189331 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.4758804142475128 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.04333660006523132 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.12359452247619629 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.14314055442810059 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.3186672329902649 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.5845575332641602 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.061672840267419815 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.09196316450834274 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.34885120391845703 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.34874165058135986 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.6623312830924988 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  -0.0017350483685731888 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.13487131893634796 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.31892773509025574 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.5069963932037354 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.595872163772583 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.02240237407386303 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.06723465025424957 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.22090913355350494 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.44786515831947327 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.5673940777778625 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.08753320574760437 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.11979014426469803 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.26805379986763 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.41591718792915344 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.7040932178497314 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.03902619332075119 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.12670598924160004 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.14234037697315216 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.33680155873298645 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.5697970986366272 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.038306817412376404 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.07616705447435379 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.23342055082321167 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.561125636100769 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.8296289443969727 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  -0.01668088324368 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.1355469524860382 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.20298908650875092 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.6833173036575317 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.7725754380226135 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  -0.053141653537750244 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.13737094402313232 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.22207270562648773 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.31772351264953613 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.635653555393219 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.025335073471069336 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.14086633920669556 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.284959614276886 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.34005677700042725 \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.7401713132858276 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  -0.022883271798491478 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.07962530851364136 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.239674910902977 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.4335852861404419 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.6398135423660278 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.0267499890178442 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.10611051321029663 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.28001493215560913 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.4735942780971527 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.6205940842628479 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.024141397327184677 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.19553333520889282 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.22094687819480896 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.41367170214653015 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.5041176080703735 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  -0.0386943444609642 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.117318294942379 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.12060156464576721 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.5336821675300598 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.5640692114830017 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  -0.00790872611105442 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.043565694242715836 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.279019832611084 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.5752516388893127 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.6365408897399902 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  -0.031081413850188255 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.12932853400707245 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.26533278822898865 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.49039527773857117 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.6076611876487732 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.052061013877391815 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.10421445220708847 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.17853903770446777 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.48912370204925537 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.65687096118927 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  -0.007755693048238754 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.10390427708625793 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.2895810902118683 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.49531033635139465 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.504986047744751 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  -0.008222682401537895 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.16099222004413605 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.30675962567329407 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.400073379278183 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.8025545477867126 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.009322486817836761 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.13182471692562103 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.28025689721107483 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.49977341294288635 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.7111532688140869 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  -0.003417186439037323 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.10787444561719894 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.19090908765792847 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.4033565819263458 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.5694071650505066 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.004409521818161011 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.03994310274720192 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.3170488178730011 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.40517809987068176 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.48092320561408997 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.012872486375272274 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.09127625823020935 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.2082674205303192 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.5153751969337463 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.46774178743362427 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.007878895848989487 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.6501898765563965 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.03745139390230179 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.09726738929748535 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.19964702427387238 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.4393632113933563 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.6022957563400269 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  0.014862390235066414 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  0.13169927895069122 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  0.2115647792816162 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  0.5011230111122131 \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  0.6696826219558716 \n",
            "\n",
            "The minimum loss obtained is: -0.07062405347824097\n",
            "The best hyperparameters are: Training with learning rate: 0.002, h_size: 128, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The best model is:  <__main__.NCM object at 0x7faa90195d80> and its identified query value is:  tensor(0.2387)\n",
            "Target node: 9\n",
            "1-hop neighbors of A: {7}\n",
            "2-hop neighbors of A: {3}\n",
            "Out of neighborhood of A: {1, 2, 4, 5, 6, 8}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}
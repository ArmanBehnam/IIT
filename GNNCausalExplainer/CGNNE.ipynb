{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Causal graph"
      ],
      "metadata": {
        "id": "uSlJcP_Bm7XP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class CausalGraph:\n",
        "    def __init__(self, V, path=[], unobserved_edges=[]):\n",
        "        self.v = list(V)\n",
        "        self.set_v = set(V)\n",
        "        self.fn = {v: set() for v in V}  # First neighborhood\n",
        "        self.sn = {v: set() for v in V}  # Second neighborhood\n",
        "        self.on = {v: set() for v in V}  # Out of neighborhood\n",
        "        self.p = set(map(tuple, map(sorted, path)))  # Path to First neighborhood\n",
        "        self.ue = set(map(tuple, map(sorted, unobserved_edges)))  # Unobserved edges\n",
        "\n",
        "        for v1, v2 in path:\n",
        "            self.fn[v1].add(v2)\n",
        "            self.fn[v2].add(v1)\n",
        "            self.p.add(tuple(sorted((v1, v2))))\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.v)\n",
        "\n",
        "    def categorize_neighbors(self,target_node):\n",
        "        # centrality = {v: len(self.fn[v]) for v in self.v}\n",
        "        # target_node = max(centrality, key=centrality.get)\n",
        "        if target_node not in self.set_v:\n",
        "            return\n",
        "\n",
        "        one_hop_neighbors = self.fn[target_node]\n",
        "        two_hop_neighbors = set()\n",
        "\n",
        "        for neighbor in one_hop_neighbors:\n",
        "            two_hop_neighbors |= self.fn[neighbor]\n",
        "\n",
        "        two_hop_neighbors -= one_hop_neighbors\n",
        "        two_hop_neighbors.discard(target_node)\n",
        "        out_of_neighborhood = self.set_v - (one_hop_neighbors | two_hop_neighbors | {target_node})\n",
        "\n",
        "        self.sn[target_node] = two_hop_neighbors\n",
        "        self.on[target_node] = out_of_neighborhood\n",
        "        return target_node, one_hop_neighbors, two_hop_neighbors, out_of_neighborhood\n",
        "\n",
        "    def plot(self):\n",
        "        G = nx.Graph()\n",
        "        G.add_nodes_from(self.v)\n",
        "        G.add_edges_from(self.p)\n",
        "\n",
        "        pos = nx.spring_layout(G)\n",
        "        nx.draw(G, pos, with_labels=True, node_size=200, font_size=10, font_weight='bold', node_color=\"lightblue\", edge_color=\"grey\")\n",
        "        plt.savefig('causal.png')\n",
        "        plt.show()\n",
        "\n",
        "    def sort(self):\n",
        "        sorted_nodes = sorted(list(self.set_v))\n",
        "        return sorted_nodes\n",
        "\n",
        "    def graph_search(self,cg, v1, v2=None, edge_type=\"path\",target_node = None):\n",
        "        assert edge_type in [\"path\", \"unobserved\"]\n",
        "        assert v1 in cg.set_v\n",
        "        assert v2 in cg.set_v or v2 is None\n",
        "\n",
        "        target, one_hop_neighbors, two_hop_neighbors, out_of_neighborhood = cg.categorize_neighbors(target_node)\n",
        "\n",
        "        q = deque([v1])\n",
        "        seen = {v1}\n",
        "        while len(q) > 0:\n",
        "            cur = q.popleft()\n",
        "            cur_fn = cg.fn[cur]\n",
        "            cur_sn = cg.sn[target_node]\n",
        "            cur_on = cg.on[target_node]\n",
        "\n",
        "            cur_neighbors = cur_fn if edge_type == \"path\" else (cur_sn | cur_on)\n",
        "\n",
        "            for neighbor in cur_neighbors:\n",
        "                if neighbor not in seen:\n",
        "                    if v2 is not None:\n",
        "                        if (neighbor == v2 and edge_type == \"path\" and neighbor in one_hop_neighbors) or \\\n",
        "                                (neighbor == v2 and edge_type == \"unobserved\" and neighbor in (\n",
        "                                        two_hop_neighbors | out_of_neighborhood)):\n",
        "                            return True\n",
        "                    seen.add(neighbor)\n",
        "                    q.append(neighbor)\n",
        "\n",
        "        if v2 is None:\n",
        "            return seen\n",
        "\n",
        "        return False\n",
        "\n",
        "    def generate_binary_values(self, cg, num_samples):\n",
        "        dataset = []\n",
        "        for _ in range(num_samples):\n",
        "            binary_values = {node: random.choice([0, 1]) for node in cg}\n",
        "            dataset.append(binary_values)\n",
        "        return dataset\n",
        "\n",
        "    def calculate_probabilities(self, dataset):\n",
        "        node_counts = {node: 0 for node in self.v}\n",
        "        total_samples = len(dataset)\n",
        "\n",
        "        for i in dataset:\n",
        "            for node, value in i.items():\n",
        "                if value == 1:\n",
        "                    node_counts[node] += 1\n",
        "\n",
        "        node_probabilities = {node: count / total_samples for node, count in node_counts.items()}\n",
        "        return node_probabilities\n",
        "\n",
        "    def calculate_joint_probabilities(self, dataset):\n",
        "        joint_counts = {(node_i, node_j): 0 for node_i in self.v for node_j in self.v if node_i != node_j}\n",
        "        total_samples = len(dataset)\n",
        "\n",
        "        for sample in dataset:\n",
        "            for node_i, node_j in joint_counts.keys():\n",
        "                if sample[node_i] == 1 and sample[node_j] == 1:\n",
        "                    joint_counts[(node_i, node_j)] += 1\n",
        "\n",
        "        joint_probabilities = {}\n",
        "        min_prob = 1  # initialize the min_prob to 1\n",
        "\n",
        "        # First, calculate the probabilities for the existing links\n",
        "        for (node_i, node_j), count in joint_counts.items():\n",
        "            if (node_i, node_j) in self.p or (node_j, node_i) in self.p:\n",
        "                prob = count / total_samples\n",
        "                joint_probabilities[(node_i, node_j)] = prob\n",
        "                joint_probabilities[(node_j, node_i)] = prob  # update for bidirectional link\n",
        "                if prob < min_prob:\n",
        "                    min_prob = prob  # update the minimum probability\n",
        "\n",
        "        # Now, calculate the probabilities for the non-existing links using the Gumbel distribution\n",
        "        for (node_i, node_j), count in joint_counts.items():\n",
        "            if (node_i, node_j) not in self.p and (node_j, node_i) not in self.p:\n",
        "                # generate a random value from a Gumbel distribution\n",
        "                gumbel_noise = np.random.gumbel()\n",
        "                # rescale the gumbel noise to be in [0, min_prob)\n",
        "                # scaled_gumbel_noise = min_prob * (gumbel_noise - np.min(gumbel_noise)) / (np.max(gumbel_noise) - np.min(gumbel_noise))\n",
        "                joint_probabilities[(node_i, node_j)] = gumbel_noise\n",
        "                joint_probabilities[(node_j, node_i)] = gumbel_noise  # update for bidirectional link\n",
        "\n",
        "        return joint_probabilities\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    cg = CausalGraph(['A', 'B', 'C', 'D'], [('A', 'B'), ('A', 'C'), ('B', 'D')])\n",
        "    datasets = cg.generate_binary_values(cg,num_samples=10)\n",
        "    p_v = cg.calculate_probabilities(datasets)\n",
        "    print(datasets)\n",
        "    p_v_joint = cg.calculate_joint_probabilities(datasets)\n",
        "    print(p_v)\n",
        "    print(p_v_joint)\n",
        "    print(cg.p)\n",
        "\n",
        "    target_node, one_hop_neighbors, two_hop_neighbors, out_of_neighborhood = cg.categorize_neighbors(target_node = cg.sort()[0])\n",
        "    print(f\"Target node: {target_node}\")\n",
        "    print(f\"1-hop neighbors of A: {one_hop_neighbors}\")\n",
        "    print(f\"2-hop neighbors of A: {two_hop_neighbors}\")\n",
        "    print(f\"Out of neighborhood of A: {out_of_neighborhood}\")\n",
        "    # Example usage of graph_search\n",
        "    result1 = cg.graph_search(cg, 'A', 'D', edge_type=\"path\",target_node = 'A')\n",
        "    print(f\"Is there a path from A to D? {result1}\")\n",
        "    result2 = cg.graph_search(cg, 'A', 'D', edge_type=\"unobserved\",target_node = 'A')\n",
        "    print(f\"Can there be an unobserved path from A to D? {result2}\")\n",
        "    result3 = cg.graph_search(cg, 'A', edge_type=\"path\",target_node = 'A')\n",
        "    print(f\"All nodes reachable from A via paths: {result3}\")\n",
        "    cg.plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 736
        },
        "id": "vxp7ciGUm8E3",
        "outputId": "e4fb31f3-c119-4519-f41c-58ead1399d8a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'A': 0, 'B': 0, 'C': 1, 'D': 0}, {'A': 0, 'B': 0, 'C': 0, 'D': 1}, {'A': 1, 'B': 0, 'C': 0, 'D': 0}, {'A': 0, 'B': 0, 'C': 1, 'D': 0}, {'A': 0, 'B': 0, 'C': 1, 'D': 0}, {'A': 0, 'B': 0, 'C': 0, 'D': 0}, {'A': 1, 'B': 1, 'C': 1, 'D': 1}, {'A': 1, 'B': 1, 'C': 0, 'D': 1}, {'A': 0, 'B': 1, 'C': 0, 'D': 1}, {'A': 0, 'B': 0, 'C': 1, 'D': 0}]\n",
            "{'A': 0.3, 'B': 0.3, 'C': 0.5, 'D': 0.4}\n",
            "{('A', 'B'): 0.2, ('B', 'A'): 0.2, ('A', 'C'): 0.1, ('C', 'A'): 0.1, ('B', 'D'): 0.3, ('D', 'B'): 0.3, ('A', 'D'): 0.11338496208569517, ('D', 'A'): 0.11338496208569517, ('B', 'C'): 3.9178734724334032, ('C', 'B'): 3.9178734724334032, ('C', 'D'): 3.2542712891302465, ('D', 'C'): 3.2542712891302465}\n",
            "{('A', 'B'), ('A', 'C'), ('B', 'D')}\n",
            "Target node: A\n",
            "1-hop neighbors of A: {'B', 'C'}\n",
            "2-hop neighbors of A: {'D'}\n",
            "Out of neighborhood of A: set()\n",
            "Is there a path from A to D? False\n",
            "Can there be an unobserved path from A to D? True\n",
            "All nodes reachable from A via paths: {'B', 'C', 'D', 'A'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAApQAAAHzCAYAAACe1o1DAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAa4klEQVR4nO3dfWxd933f8e+hKUqUaYqy/MA4lmXGcZzHxjGT5QHQhHXe0rXJ2tFtsyevS4dtCPrHsKEY0mZI1xZoggxY/hj2R7YBw1p3a9xY2RJjdZZ1pqbBduMyXdY6yeYmlGzXph1TpGhGlyIpnv3BB4vkpXjJc++55+H1AgLLFPnDsQEhb9/PPecmaZqmAQAA+9TT7QsAAKDcBCUAAJkISgAAMhGUAABkIigBAMhEUAIAkImgBAAgE0EJAEAmghIAgEwEJQAAmQhKAAAyEZQAAGQiKAEAyERQAgCQiaAEACATQQkAQCaCEgCATAQlAACZCEoAADIRlAAAZCIoAQDIRFACAJCJoAQAIBNBCQBAJoISAIBMBCUAAJkISgAAMhGUAABkIigBAMhEUAIAkImgBAAgE0EJAEAmghIAgEwEJQAAmQhKAAAyEZQAAGQiKAEAyERQAgCQiaAEACATQbkmTdNuXwIAQCn1dvsCumVmYSnOX7wU043FmLu8HGlEJBExeLA3jvX3xYkjh+PooQPdvkwAgMJL0pq9NDe/uBwTU7Mx3ViKJCKa/cOvf/1Y/4EYHR6Kgb7adjcAwK5qFZTPzzViYmo20rR5SG6VRESSRIwOD8Xxwf5OXx4AQCnV5qW35+ca8fRLs3v6mTQi0jQ2fk5UAgBsV4ubctZn7iwmpmZjfnG5PRcEAFAhtZi8zzz3alxoLG2buRcvL8Rj//E/xBO/95V44XvPxpUry3Hs1tvine//UPzU3/+FGD5+YuN7k4i4sf9AnLrjplyvHQCg6Co/ec8sLMV0Y2nb1+cvzsY///jPxuS3/yQiIvqvH4jh43fGqy/9WXz94YfiLfeObgrKNCKmG0sxs7Dk7m8AgKtUPijPX7zU9G7uf/frn9qIyZ/8e5+Iv/WPfymu61391/HM009Fb+/2fzXJ2nlHDx3p7EUDAJRI5YNyurG4LSZ/+NpcPPHYVyMi4s63vj0e/MV/FkmSbPz+O973gaZnpWvnAQDwusrflDN3efuNNC+d+35cWV79+ttG378pJvdzHgBAnVU6KNM0bfq8yavvQ9pLTEasP0qo8vcxAQC0rNJBmSRJNMvF20bu2ni/5He++Y09BWISe49QAIAqq3RQRqx+NvdW198wGB/6sY9GRMTkt/8kfvtffmZjAo+I+NYT/zO++82nWz4PAKDOKv8cyv/98sWYnL20bfp+bXYmfvXjPxuT33kmIiIOD9wQN7/xeExPvRjzF2fjF37j8/GjYx/b9DNJRIwMHY57b3WXNwDAusq/QnniyOGm76O8Yeho/MbvfDV+7p9+Ot78rntjZWUlXpz8Xlw/eCTu/5m/GW9vcqd3unYeAACvq/wrlBE7f1LOnqRpHOvvi1MnfFIOAMDVKv8KZUTE6PBQZLqPJk1jZWUlnnvq8Zifn2/bdQEAVEEtgnKgrzdGh4f2f0CSxEjflXjlhefiC1/4QkxOTrbt2gAAyq4Wk/e65+caMTE1G2m6/aMYm1l9RNDqK5zHB/tjfn4+Tp8+HefOnYtTp07FyZMno6enFk0OALCjWgVlRMT84nJMTM3GdGOp6Wd8R8TG12/q74v7ho/EQN/rjwpaWVmJs2fPxvj4eIyMjMTY2FgMDAzkdPUAAMVTu6BcN7OwFOcvXorpxmLMXV6ONFZDcvBgbxzr74sTRw7H0UMHdvz5ycnJOH36dEREjI2NxcjISD4XDgBQMLUNyq3SNN3zJ+CYwAEABGVmJnAAoO4EZZuYwAGAuhKUbWQCBwDqSFC2mQkcAKgbQdkhJnAAoC4EZQeZwAGAOhCUHWYCBwCqTlDmxAQOAFSVoMyRCRwAqCJBmTMTOABQNYKyS0zgAEBVCMouMoEDAFUgKLvMBA4AlJ2gLAgTOABQVoKyQEzgAEAZCcqCMYEDAGUjKAvKBA4AlIWgLDATOABQBoKy4EzgAEDRCcqSMIEDAEUlKEvEBA4AFJGgLBkTOABQNIKypEzgAEBRCMoSM4EDAEUgKEvOBA4AdJugrAgTOADQLYKyQkzgAEA3CMqKMYEDAHkTlBVlAgcA8iIoK8wEDgDkQVBWnAkcAOg0QVkTJnAAoFMEZY2YwAGAThCUNWMCBwDaTVDWlAkcAGgXQVljJnAAoB0EZc2ZwAGArAQlEWECBwD2T1CywQQOAOyHoGQTEzgAsFeCkqZM4ABAqwQlOzKBAwCtEJRckwkcANiNoKQlJnAAYCeCkpaZwAGAZgQle2ICBwC2EpTsiwkcAFgnKNk3EzgAECEoycgEDgAIStrCBA4A9SUoaRsTOADUk6CkrUzgAFA/gpKOMIEDQH0ISjrGBA4A9SAo6SgTOABUn6AkFyZwAKguQUluTOAAUE2CklyZwAGgegQlXWECB4DqEJR0jQkcAKpBUNJVJnAAKD9BSSGYwAGgvAQlhWECB4ByEpQUigkcAMpHUFJIJnAAKA9BSWGZwAGgHAQlhWYCB4DiE5SUggkcAIpLUFIaJnAAKCZBSamYwAGgeAQlpWQCB4DiEJSUlgkcAIpBUFJqJnAA6D5BSSWYwAGgewQllWECB4DuEJRUigkcAPInKKkkEzgA5EdQUlkmcADIh6Ck0kzgANB5gpJaMIEDQOcISmrDBA4AnSEoqRUTOAC0n6CklkzgANA+gpLaMoEDQHsISmrNBA4A2QlKCBM4AGQhKGGNCRwA9kdQwlVM4ACwd4ISmjCBA0DrBCXswAQOAK0RlHANJnAA2J2ghBaYwAFgZ4ISWmQCB4DmBCXsgQkcALYTlLAPJnAAeJ2ghH0ygQPAKkEJGZjAAUBQQluYwAGoM0EJbWICB6CuBCW0kQkcgDoSlNABJnAA6kRQQoeYwAGoC0EJHWQCB6AOBCXkwAQOQJUJSshJOybwNE0jSZIOXSEA7I+ghBztdQKfWViK8xcvxXRjMeYuL0caEUlEDB7sjWP9fXHiyOE4euhAbtcPAM0ISuiC3Sbw+cXlmJiajenGUiQR0ewP6frXj/UfiNHhoRjo6+30ZQNAU4ISumSnCfz5uUZMTM1GmjYPya2SiEiSiNHhoTg+2N/pywaAbQQldNHWCfyDP/bR+OOZhX2f9743iEoA8icooQAmJyfjK1/7etz+5388kut6YvV1x73rSSLuv/Nm8zcAufKUZSiAkZGReNeHf2q1CLfE5P84/cV44K23xQNvvS1+5u23x6sv/dmO56RpxMTUbEevFQC2EpRQADMLSzG7eCWSZPsfyfEvP7zx65WVlRj/z7+74zlpREw3lmJmYakTlwkATQlKKIDzFy81HblffuG5+PYfPhUREXe9890REfH4NYIyYvX1zfMXL7X5CgFgZ4ISCmC6sdj0ju7xLz8caZrG0M23xCd+/V9ERMTU+cn4zsQf7HhWunYeAORFUEIBzF1e3va1NE1j/L98KSIiTv7EX4uRt70zTtzz9oiIePyqGbzV8wCgUwQldFmapk1fnXzmG0/GKy88FxERp37ygdW//tXVvz752KNxubHzrJ2unQsAefBsEeiyJEmafhrO1a9Cfvrv/HRERKxcWX3l8dL8a/HU139vIzC3nbl2LgDkwSuUUACDBzf/t13jhz+Mp/7boxt/f+m1ubj02lwsXHr9VcnHv/zFls8DgE4SlFAAx/r7Nt3l/eTXHt2Ix89/9fF45Lsvbvzv47/8axER8cwfPNH0mZTJ2nkAkBdBCQVw4sjhTZP3+rMnb7vzTXHH3fds+t4P/KW/EhE7P5MyXTsPAPLioxehIM4892pcaCw1vUGnVUlE3Nh/IE7dcVO7LgsAduUVSiiI0eGhyHIfTZqmsXLlSty6PN++iwKAFghKKIiBvt4YHR7a988nSRKXJ78dDz/0m3HmzJlYWVlp38UBwDW4FRQK5Phgf0RETEzNRppuf5RQM6uPCFp9hfONd98fZwcOxvj4eJw/fz7GxsZiYGCgo9cMAN5DCQU0v7gcE1OzMd1YavqMyojY+PpN/X1x3/CRGOh7/b8PJycn4/Tp0xERMTY2FiMjI3lcNgA1JSihwGYWluL8xUsx3ViMucvLkcZqSA4e7I1j/X1x4sjhOHroQNOfnZ+fj9OnT8e5c+fi1KlTcfLkyejp8S4XANpPUEKJpGm6p0/AWVlZibNnz8b4+HiMjIyYwAHoCEEJNWACB6CTBCXUhAkcgE4RlFAjJnAAOkFQQg2ZwAFoJ0EJNWUCB6BdBCXUmAkcgHYQlIAJHIBMBCUQESZwAPZPUAIbTOAA7IegBLYxgQOwF4ISaMoEDkCrBCWwIxM4AK0QlMCuTOAAXIugBFpiAgdgJ4ISaJkJHIBmBCWwZyZwAK4mKIF9MYEDsE5QAvtmAgcgQlACbWACB6g3QQm0hQkcoL4EJdA2JnCAehKUQNuZwAHqRVACHWECB6gPQQl0jAkcoB4EJdBxJnCAahOUQC5M4ADVJSiB3JjAAapJUAK5M4EDVIugBLrCBA5QHYIS6BoTOEA1CEqg60zgAOUmKIFCMIEDlJegBArDBA5QToISKBwTOEC5CEqgkEzgAOUhKIHCMoEDlIOgBArPBA5QbIISKAUTOEBxCUqgNEzgAMUkKIHSMYEDFIugBErJBA5QHIISKC0TOEAxCEqg9EzgAN0lKIFKMIEDdI+gBCrDBA7QHYISqBwTOEC+BCVQSSZwgPwISqCyTOAA+RCUQOWZwAE6S1ACtWACB+gcQQnUhgkcoDMEJVA7JnCA9hKUQC2ZwAHaR1ACtWUCB2gPQQnUngkcIBtBCRAmcIAsBCXAGhM4wP4ISoAtTOAAeyMoAZowgQO0TlAC7MAEDtAaQQmwCxM4wLUJSoAWmMABdiYoAVpkAgdoTlAC7JEJHGAzQQmwDyZwgNcJSoB9MoEDrBKUABmZwIG6E5QAbWACB+pMUAK0iQkcqCtBCdBmJnCgbgQlQAeYwIE6EZQAHWICB+pCUAJ0mAkcqDpBCZADEzhQZYISICcmcKCqBCVAzkzgQNUISoAuMIEDVSIoAbrEBA5UhaAE6DITOFB2ghKgAEzgQJkJSoCCMIEDZSUoAQrGBA6UjaAEKCATOFAmghKgoEzgQFkISoCCM4EDRScoAUrABA4UmaAEKAkTOFBUghKgZEzgQNEISoASMoEDRSIoAUrKBA4UhaAEKDkTONBtghKgAkzgQDcJSoCKMIED3SIoASrGBA7kTVACVJAJHMiToASoKBM4kBdBCVBxJnCg0wQlQA2YwIFOEpQANWECBzpFUALUjAkcaDdBCVBDJnCgnQQlQE2ZwIF2EZQANWcCB7ISlACYwIFMBCUAEWECB/ZPUAKwiQkc2CtBCcA2JnBgLwQlAE2ZwIFWCUoArskEDuxGUAKwKxM4cC2CEoCWmMCBnQhKAPbEBA5sJSgB2DMTOHA1QQnAvpjAgXWCEoBMTOCAoAQgMxM41JugBKAtTOBQX4ISgLYygUP9CEoA2s4EDvUiKAHoiKwTeJqmkSRJB68QaBdBCUBHtTqBzywsxfmLl2K6sRhzl5cjjYgkIgYP9sax/r44ceRwHD10IL8LB1omKAHouGtN4POLyzExNRvTjaVIIqLZ/ymtf/1Y/4EYHR6Kgb7eHK8e2I2gBCAXzSbwmZXrYmJqNtK0eUhulUREkkSMDg/F8cH+Tl8y0CJBCUCu1ifw6287Ebfc96FYzcS9e98bRCUUhaAEIHcvz87F/3rptYgk2feNNz1JxP133mz+hgLwpxCA3H13bjF6eno2zdyffvCBeObpJzf+/rre3rjh6I3xttH3x4O/+Km49fY7Np2RphETU7Nx6o6bcrpqYCceCgZArmYWlmK6sbTjeyZ7D/TF3e++L974prtj9gevxJOPfTU+84mf2/Z9aURMN5ZiZmGpo9cL7E5QApCr8xcvXfNdk0dvviU++8VH4/Nf+f34iz/9NyIi4vln/2+8NnNh2/cma+cB3SUoAcjVdGOxpTu6LzcuxYWXpyIiYvDGY9E/cMO270nXzgO6y3soAcjV3OXla/7+D158IR54620bf997oC/+0ef+VfQeaP5Q893OAzrPK5QA5CZN011fnVx/D+Vd7/iR6Dt0KJaXFuNf//I/iempF5ufuXYu0D2CEoDcJEmy61Mn199D+blHHovPfemxiIi48MpUfO13fqv5mWvnAt0jKAHI1eDB/b3baunyQlvPA9rHn0IAcnWsvy/mLi/vOH3P/OCV+OTHPhIry8vx/Pf+X0RE9PT0xHv/wl/e9r3J2nlAdwlKAHJ14sjh+P7szo/6WV5ajGe/9c2IiOi/fiDuuXc0Pvp3/2G84899cNv3pmvnAd3loxcByN2Z516NC9d4uHkr0pWVGLgujQ+/5fa2XRewP95DCUDuRoeHoh330fzRo1+KM2fOxMrKSvbDgH0TlADkbqCvN0aHhzKd8b7bhuKDo++J8fHxeOihh2J+fr49FwfsmckbgK55fq4RE1OzkabR0vy9+oig1Vc4jw/2R0TE5ORknD59OiIixsbGYmRkpHMXDDQlKAHoqvnF5ZiYmo3pxlIk0Tws179+U39f3Dd8JAb6Nt9TOj8/H6dPn45z587FqVOn4uTJk9HTY4SDvAhKAAphZmEpzl+8FNONxY3HCiWx+pzJY/19ceLI4Th6qPnHL0ZErKysxNmzZ2N8fDxGRkZibGwsBgYGcrt+qDNBCUAhpWm6r0/AMYFD/gQlAJVjAod8CUoAKskEDvkRlABUmgkcOk9QAlB5JnDoLEEJQC2YwKFzBCUAtWICh/YTlADUjgkc2ktQAlBLJnBoH0EJQK2ZwCE7QQlA7ZnAIRtBCQBhAocsBCUAXMUEDnsnKAFgCxM47I2gBIAmTODQOkEJANdgAofdCUoA2IUJHK5NUAJAC0zgsDNBCQB7YAKH7QQlAOyRCRw2E5QAsA8mcHidoASADEzgICgBIDMTOHUnKAGgDUzg1JmgBIA2MoFTR4ISANrMBE7dCEoA6AATOHUiKAGgg0zg1IGgBIAOM4FTdYISAHJgAqfKBCUA5MgEThUJSgDImQmcqhGUANAFJnCqRFACQBeZwKkCQQkAXWYCp+wEJQAUgAmcMhOUAFAgJnDKSFACQMGYwCkbQQkABWQCp0wEJQAUmAmcMhCUAFBwJnCKTlACQAmYwCkyQQkAJWICp4gEJQCUjAmcohGUAFBCJnCKRFACQImZwCkCQQkAJWcCp9sEJQBUgAmcbhKUAFAhJnC6QVACQMWYwMmboASACjKBkydBCQAVZgInD4ISACrOBE6nCUoAqAETOJ0kKAGgRiYnJ+ORRx6JJElM4LSNoASAmjGB026CEgBqyAROOwlKAKgxEzjtICgBoOZM4GQlKAEAEziZCEoAYIMJnP0QlADAJiZw9kpQAgDbmMDZC0EJAOzIBE4rBCUAcE0mcHYjKAGAXZnAuRZBCQC0zAROM4ISANgTEzhbCUoAYM9M4FxNUAIA+2YCJ0JQAgAZmcARlABAZibwehOUAEDbmMDrSVACAG1lAq8fQQkAtJ0JvF4EJQDQMSbwehCUAEBHmcCrT1ACAB1nAq82QQkA5MYEXk2CEgDIlQm8egQlAJA7E3i1CEoAoGtM4NUgKAGArjKBl5+gBAC6zgReboISACgME3g5CUoAoFBM4OUjKAGAwjGBl4ugBAAKywReDoISACg0E3jxCUoAoPBM4MUmKAGA0jCBF5OgBABKxQRePIISACgdE3ixCEoAoLRM4MUgKAGAUjOBd5+gBABKzwTeXYISAKgME3h3CEoAoFJM4PkTlABA5ZjA8yUoAYDKMoHnQ1ACAJVmAu88QQkAVJ4JvLMEJQBQGybwzhCUAECtmMDbT1ACALVjAm8vQQkA1FbWCTxN00iSpENXVx6CEgCotb1M4DMLS3H+4qWYbizG3OXlSCMiiYjBg71xrL8vThw5HEcPHcj1+otAUAIAtbfbBD6/uBwTU7Mx3ViKJCKaxdP614/1H4jR4aEY6OvN6eq7T1ACAKxpNoE/P9eIianZSNPmIblVEhFJEjE6PBTHB/s7fcmFICgBAK5y9QT+gQ9/JOaH3rDvs973hnpEpaAEANhiZWUlxp94Ki7ceEckPT37vvGmJ4m4/86bKz9/C0oAgCbOPPdqTDcWY3XEXvXpBx+IZ55+MiIienp6ou9Qf9x4y61xz3veGz/+t38+3vSOH9l0RhIRN/YfiFN33JTjlefPUzwBALaYWViK6cZSXB2TV+s90Bd3veveOHzDYLx0fjIe//LD8cmPfST+++/+9qbvSyNiurEUMwtLnb/oLhKUAABbnL94aYeUXHX05lvis198NP7tmYn47MP/NW6+7fa4srwc/+ZXfyle+P6zm743WTuvygQlAMAW043Flu7ojoh487veHT//qV+LiIgry8vx+1/6T5t+P107r8oEJQDAFnOXl/f0/W8bff/Gr1/43rPbfn+v55WNoAQAuEqapi2/Ovn6z6xc+/fXzq0qQQkAcJUkSa75/slmvvOH39j49e133b39zLVzq0pQAgBsMXiw9edG/ukffyv+/Wd+JSIieq67Ln507K9nOq+Mqv1PBwCwD8f6+2Lu8vKO0/fMD16JT37sI3Hh5ZfiwstTkaZpXNfbG//gVz4Tx9/8lk3fm6ydV2WCEgBgixNHDsf3Z3d+1M/y0mL86f/5ozjYfziG7xiJe97z3viJB7c/2Dxi9f2TJ44c7uDVdp9PygEAaOLMc6/GhcbSnm/QuZpPygEAqLHR4aHIeh9NkqyeU3WCEgCgiYG+3swxODo8FAN91X+HYfX/CQEA9un4YH9ERExMzUaaRkvz9+ojglZjcv3nq857KAEAdjG/uBwTU7Mx3ViKJJqH5frXb+rvi/uGj9Tilcl1ghIAoEUzC0tx/uKlmG4sbjxWKInV50we6++LE0cOx9FDB7p9mbkTlAAA+5SmaaU/AadVbsoBANgnMblKUAIAkImgBAAgE0EJAEAmghIAgEwEJQAAmQhKAAAyEZQAAGQiKAEAyERQAgCQiaAEACATQQkAQCaCEgCATAQlAACZCEoAADIRlAAAZCIoAQDIRFACAJCJoAQAIBNBCQBAJoISAIBMBCUAAJkISgAAMhGUAABkIigBAMhEUAIAkImgBAAgE0EJAEAmghIAgEwEJQAAmQhKAAAyEZQAAGQiKAEAyERQAgCQiaAEACATQQkAQCaCEgCATAQlAACZCEoAADIRlAAAZPL/ASgdsAwlWwzaAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm 1"
      ],
      "metadata": {
        "id": "DJd_Oa2FnEuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Uniform, Gumbel, Bernoulli\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import product\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "class NNModel(nn.Module):\n",
        "    def __init__(self, u, input_size, output_size, h_size, h_layers):\n",
        "        super(NNModel, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.h_size = h_size\n",
        "        self.h_layers = h_layers\n",
        "        self.u = u\n",
        "        layers = [nn.Linear(self.input_size, self.h_size)]\n",
        "        for l in range(h_layers - 1):\n",
        "            layers.append(nn.Linear(self.h_size, self.h_size))\n",
        "            layers.append(nn.ReLU())\n",
        "        layers.append(nn.Linear(self.h_size, self.output_size))\n",
        "        self.nn = nn.Sequential(*layers)\n",
        "        self.nn.apply(self.init_weights)\n",
        "\n",
        "    def init_weights(self, m):\n",
        "        if type(m) == nn.Linear:\n",
        "            torch.nn.init.xavier_normal_(m.weight, gain=torch.nn.init.calculate_gain('relu'))\n",
        "\n",
        "    def nn_forward(self):\n",
        "        out = self.nn(self.u)\n",
        "        return torch.sigmoid(out)\n",
        "\n",
        "class NCM:\n",
        "    def __init__(self, graph, lambda_reg, learning_rate,h_size, h_layers):\n",
        "        self.graph = graph\n",
        "        self.h_size = h_size\n",
        "        self.h_layers = h_layers\n",
        "        self.lambda_reg = lambda_reg\n",
        "        self.learning_rate = learning_rate\n",
        "        self.states = {graph.target_node: Bernoulli(0.5).sample((1,))}\n",
        "        self.u_i = {v: Uniform(0, 1).sample((1,)) for v in graph.one_hop_neighbors | graph.two_hop_neighbors}\n",
        "        self.u_ij = {v: Uniform(0, 1).sample((1,)) for v in graph.one_hop_neighbors}\n",
        "        self.u_do = {v: Uniform(0, 1).sample((1,)) for v in graph.one_hop_neighbors}\n",
        "        self.u = torch.cat(list(self.states.values()) + list(self.u_i.values()) + list(self.u_ij.values()), dim=0)\n",
        "        self.model_v = NNModel(u = self.u, input_size=len(self.u), output_size=1,h_size = self.h_size, h_layers = self.h_layers)\n",
        "        self.model_do = NNModel(u = self.u, input_size=len(self.u), output_size=1,h_size = self.h_size, h_layers = self.h_layers)\n",
        "\n",
        "    def ncm_forward(self):\n",
        "        G_i = Gumbel(loc=torch.tensor([0.0]), scale=torch.tensor([1.0])).sample((1,))\n",
        "        f = self.model_v.nn_forward()\n",
        "        # print(self.states)\n",
        "        if len(self.u_ij) > 0:\n",
        "            return G_i + torch.log(f)\n",
        "        else:\n",
        "            return G_i + torch.log(1 - f)\n",
        "\n",
        "def train(cg,lambdas,learning_rate,h_size,h_layers,num_epochs):\n",
        "    causal_loss = []\n",
        "    losses = []\n",
        "    for i in range(num_epochs):\n",
        "        # print('epoch : ', i)\n",
        "        sum_f = 0\n",
        "        for node in cg.set_v:\n",
        "            # print(node)\n",
        "            cg.target_node, cg.one_hop_neighbors, cg.two_hop_neighbors, cg.out_of_neighborhood = cg.categorize_neighbors(target_node=node)\n",
        "            ncm = NCM(cg, lambda_reg=lambdas, learning_rate=learning_rate, h_size=h_size, h_layers=h_layers)\n",
        "            optimizer = optim.Adam(ncm.model_v.parameters(), lr=ncm.learning_rate)\n",
        "            optimizer.zero_grad()\n",
        "            f = ncm.ncm_forward()\n",
        "            # print(f)\n",
        "            sum_f += f\n",
        "        # print(sum_f)\n",
        "        p_L1 = sum_f / len(cg.set_v)\n",
        "        p_L2 = torch.abs(torch.prod(p_L1) / len(cg.set_v))\n",
        "        # loss = -np.log(torch.sum(p_L1))\n",
        "        loss = ((1 / len(cg.set_v)) * -torch.log(torch.sum(p_L1))) - (lambdas * torch.log(torch.sum(p_L2)))\n",
        "        # print(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(float(loss))\n",
        "        dir_path = \"./model\"\n",
        "        os.makedirs(dir_path, exist_ok=True)\n",
        "        torch.save(ncm.model_v.state_dict(), os.path.join(dir_path, f'model_{i}.pth'))\n",
        "    # print(losses)\n",
        "    if math.isnan(losses[0]):\n",
        "        causal_loss.append(losses[1])\n",
        "    else:\n",
        "        causal_loss.append(losses[0])\n",
        "    print(\"The loss value is : \", causal_loss, '\\n')\n",
        "    return causal_loss\n",
        "\n",
        "def kl_divergence(p, q):\n",
        "    return np.sum(np.where(p != 0, p * np.log(p / q), 0))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    cg = CausalGraph(['A', 'B', 'C', 'D'], [('A', 'B'), ('A', 'C'), ('B', 'D')])\n",
        "    cg.target_node, cg.one_hop_neighbors, cg.two_hop_neighbors, cg.out_of_neighborhood = cg.categorize_neighbors(target_node=cg.sort()[0])\n",
        "    datasets = cg.generate_binary_values(cg, num_samples=10)\n",
        "    p_v = cg.calculate_probabilities(datasets)\n",
        "    p_v_joint = cg.calculate_joint_probabilities(datasets)\n",
        "    print(datasets,'\\n',p_v,'\\n',p_v_joint)\n",
        "\n",
        "    # hyperparameters\n",
        "    num_epochs = 2\n",
        "    learning_rates = [0.001, 0.002, 0.005,0.01]\n",
        "    hidden_sizes = [32, 64, 128, 256]\n",
        "    num_layers = [1, 2, 3, 4]\n",
        "    lambdas = [0.01, 0.05, .1,.2,.3]\n",
        "    hyperparameters = product(learning_rates, hidden_sizes, num_layers, lambdas)\n",
        "    total_loss = []\n",
        "    for i, hyperparams in enumerate(hyperparameters):\n",
        "        learning_rate, h_size, h_layers, lambdas = hyperparams\n",
        "        print(f'Training with learning rate: {learning_rate}, h_size: {h_size}, h_layers: {h_layers}, lambdas: {lambdas}')\n",
        "        causal_loss = train(cg, lambdas, learning_rate, h_size, h_layers, num_epochs)\n",
        "        total_loss.append(causal_loss)\n",
        "    total_loss = [x for x in total_loss if not math.isnan(x[0])]\n",
        "    print(total_loss)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(total_loss)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss over epochs')\n",
        "    plt.savefig('syn Loss over epochs.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BoIHm_K_nEJC",
        "outputId": "57db874a-46f2-4fec-e04c-9cf7b216d350"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'A': 0, 'B': 1, 'C': 1, 'D': 0}, {'A': 1, 'B': 0, 'C': 1, 'D': 0}, {'A': 1, 'B': 1, 'C': 1, 'D': 0}, {'A': 1, 'B': 0, 'C': 1, 'D': 0}, {'A': 1, 'B': 0, 'C': 0, 'D': 1}, {'A': 0, 'B': 0, 'C': 0, 'D': 1}, {'A': 0, 'B': 1, 'C': 0, 'D': 1}, {'A': 0, 'B': 1, 'C': 1, 'D': 1}, {'A': 0, 'B': 0, 'C': 1, 'D': 0}, {'A': 0, 'B': 0, 'C': 1, 'D': 1}] \n",
            " {'A': 0.4, 'B': 0.4, 'C': 0.7, 'D': 0.5} \n",
            " {('A', 'B'): 0.1, ('B', 'A'): 0.1, ('A', 'C'): 0.3, ('C', 'A'): 0.3, ('B', 'D'): 0.2, ('D', 'B'): 0.2, ('A', 'D'): 0.2878856291267668, ('D', 'A'): 0.2878856291267668, ('B', 'C'): 0.49441359569410936, ('C', 'B'): 0.49441359569410936, ('C', 'D'): 1.1064401463716462, ('D', 'C'): 1.1064401463716462}\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  [0.33541932702064514] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  [0.10991257429122925] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  [-0.0669974759221077] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  [0.021590907126665115] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  [0.14756163954734802] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  [1.2256481647491455] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  [0.5244036912918091] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  [0.19994035363197327] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  [0.5676528215408325] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  [0.888565719127655] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  [0.6905744671821594] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  [0.5906212329864502] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  [0.8600257635116577] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  [1.1814051866531372] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  [0.29956987500190735] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  [0.18421372771263123] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  [0.4954540729522705] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  [0.6459929943084717] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  [1.0797885656356812] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  [0.1545363962650299] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  [0.26269084215164185] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  [0.6627377271652222] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  [3.0973644256591797] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  [0.36355385184288025] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  [0.8502606153488159] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  [1.6831293106079102] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  [0.7561489343643188] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  [0.3589423894882202] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  [0.06539864838123322] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  [0.8489151000976562] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  [0.3382546305656433] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  [0.6903176307678223] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  [0.5565525889396667] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  [0.20461505651474] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  [0.9488097429275513] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  [0.2925914525985718] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  [0.46664243936538696] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  [0.4782058596611023] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  [1.7307329177856445] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  [0.0030982140451669693] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  [0.8186614513397217] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  [1.21034836769104] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  [1.2437385320663452] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  [0.9089294672012329] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  [0.002500852569937706] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  [0.6323800683021545] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  [0.07163432985544205] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  [0.3918461799621582] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  [1.4042302370071411] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  [0.39639216661453247] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  [0.6826862096786499] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  [0.5526245832443237] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  [0.4703490734100342] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  [0.06726594269275665] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  [1.3359204530715942] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  [0.9726402163505554] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  [0.8105023503303528] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  [2.0453908443450928] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  [0.3517402410507202] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  [1.5782265663146973] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  [0.3761124312877655] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  [0.7182496786117554] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  [0.286577045917511] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  [1.4165959358215332] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  [0.36723583936691284] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  [0.07200595736503601] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  [0.26482093334198] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  [0.3644406199455261] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  [0.4618965983390808] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  [0.8522759675979614] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  [0.3951294422149658] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  [0.5082359313964844] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  [0.6563813090324402] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  [0.5163465142250061] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  [0.4061627984046936] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  [0.1792130321264267] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  [0.1646595597267151] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  [2.435232162475586] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  [0.21164946258068085] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  [0.39075231552124023] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  [0.47207415103912354] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  [0.6365631818771362] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  [0.0751800537109375] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  [0.5149400234222412] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  [0.11581844091415405] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  [0.8838902711868286] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  [0.9547608494758606] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  [0.6790652275085449] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  [-0.012107029557228088] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  [0.27593475580215454] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  [0.4217117130756378] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  [0.542888879776001] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  [0.44224274158477783] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  [1.7708244323730469] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  [0.1558300405740738] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  [-0.0532408282160759] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  [0.3900465667247772] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  [1.0756399631500244] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  [1.631990909576416] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  [0.2578141391277313] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  [0.04681042581796646] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  [0.9665305614471436] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  [0.8088449239730835] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  [0.9710745215415955] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  [0.1791502684354782] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  [0.2580713927745819] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  [0.8091526031494141] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  [1.607452392578125] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  [0.26558735966682434] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  [0.42535215616226196] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  [0.4389955997467041] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  [1.1796278953552246] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  [0.6374965906143188] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  [0.8435908555984497] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  [0.5386030673980713] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  [-0.054670799523591995] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  [0.7583366632461548] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  [0.4271273612976074] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  [0.1447514444589615] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  [0.0028325729072093964] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  [1.005521297454834] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  [0.5356059074401855] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  [1.3069672584533691] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  [-0.08073379099369049] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  [0.38437536358833313] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  [0.5884637236595154] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  [0.3046473264694214] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  [0.7650153040885925] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  [1.133865237236023] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  [0.5870016813278198] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  [0.25732657313346863] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  [0.14384092390537262] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  [0.055729471147060394] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  [0.9818052649497986] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  [0.17506898939609528] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  [0.5086444616317749] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  [0.13954642415046692] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  [0.027649827301502228] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  [-0.052505090832710266] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  [1.4814540147781372] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  [0.11804424226284027] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  [0.8417627215385437] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  [1.5247197151184082] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  [0.3551541864871979] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  [1.6239771842956543] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  [0.013775087893009186] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  [0.8526049852371216] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  [-0.04417979717254639] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  [0.153986394405365] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  [0.026583485305309296] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  [0.5253838300704956] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  [0.5814460515975952] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  [0.026536792516708374] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  [0.4169594645500183] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  [0.06569942831993103] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  [0.2900663912296295] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  [0.8614532947540283] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  [0.49817192554473877] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  [0.4159550070762634] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  [0.4804060757160187] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  [0.5794458389282227] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  [0.4480404257774353] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  [1.937148094177246] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  [0.7305901646614075] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  [0.049469366669654846] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  [0.7343547344207764] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  [0.5601871609687805] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  [0.6573054790496826] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  [0.5093348026275635] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  [0.6564600467681885] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  [0.2813343107700348] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  [0.3947528898715973] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  [0.4230596423149109] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  [0.19322948157787323] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  [0.47624844312667847] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  [0.38454166054725647] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  [0.9502915740013123] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 1, lambdas: 0.01\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 1, lambdas: 0.05\n",
            "The loss value is :  [0.5567618608474731] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 1, lambdas: 0.1\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 1, lambdas: 0.2\n",
            "The loss value is :  [0.06320030242204666] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 1, lambdas: 0.3\n",
            "The loss value is :  [1.2545959949493408] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 2, lambdas: 0.01\n",
            "The loss value is :  [-0.0962982326745987] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 2, lambdas: 0.05\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 2, lambdas: 0.1\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 2, lambdas: 0.2\n",
            "The loss value is :  [1.915682077407837] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 2, lambdas: 0.3\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 3, lambdas: 0.01\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 3, lambdas: 0.05\n",
            "The loss value is :  [0.1477603316307068] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 3, lambdas: 0.1\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 3, lambdas: 0.2\n",
            "The loss value is :  [0.5938067436218262] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 3, lambdas: 0.3\n",
            "The loss value is :  [0.7029964923858643] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 4, lambdas: 0.01\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 4, lambdas: 0.05\n",
            "The loss value is :  [0.1710951328277588] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 4, lambdas: 0.1\n",
            "The loss value is :  [0.02344168722629547] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 4, lambdas: 0.2\n",
            "The loss value is :  [0.29008761048316956] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 4, lambdas: 0.3\n",
            "The loss value is :  [nan] \n",
            "\n",
            "[[0.33541932702064514], [0.10991257429122925], [-0.0669974759221077], [0.021590907126665115], [0.14756163954734802], [1.2256481647491455], [0.5244036912918091], [0.19994035363197327], [0.5676528215408325], [0.888565719127655], [0.6905744671821594], [0.5906212329864502], [0.8600257635116577], [1.1814051866531372], [0.29956987500190735], [0.18421372771263123], [0.4954540729522705], [0.6459929943084717], [1.0797885656356812], [0.1545363962650299], [0.26269084215164185], [0.6627377271652222], [3.0973644256591797], [0.36355385184288025], [0.8502606153488159], [1.6831293106079102], [0.7561489343643188], [0.3589423894882202], [0.06539864838123322], [0.8489151000976562], [0.3382546305656433], [0.6903176307678223], [0.5565525889396667], [0.20461505651474], [0.9488097429275513], [0.2925914525985718], [0.46664243936538696], [0.4782058596611023], [1.7307329177856445], [0.0030982140451669693], [0.8186614513397217], [1.21034836769104], [1.2437385320663452], [0.9089294672012329], [0.002500852569937706], [0.6323800683021545], [0.07163432985544205], [0.3918461799621582], [1.4042302370071411], [0.39639216661453247], [0.6826862096786499], [0.5526245832443237], [0.4703490734100342], [0.06726594269275665], [1.3359204530715942], [0.9726402163505554], [0.8105023503303528], [2.0453908443450928], [0.3517402410507202], [1.5782265663146973], [0.3761124312877655], [0.7182496786117554], [0.286577045917511], [1.4165959358215332], [0.36723583936691284], [0.07200595736503601], [0.26482093334198], [0.3644406199455261], [0.4618965983390808], [0.8522759675979614], [0.3951294422149658], [0.5082359313964844], [0.6563813090324402], [0.5163465142250061], [0.4061627984046936], [0.1792130321264267], [0.1646595597267151], [2.435232162475586], [0.21164946258068085], [0.39075231552124023], [0.47207415103912354], [0.6365631818771362], [0.0751800537109375], [0.5149400234222412], [0.11581844091415405], [0.8838902711868286], [0.9547608494758606], [0.6790652275085449], [-0.012107029557228088], [0.27593475580215454], [0.4217117130756378], [0.542888879776001], [0.44224274158477783], [1.7708244323730469], [0.1558300405740738], [-0.0532408282160759], [0.3900465667247772], [1.0756399631500244], [1.631990909576416], [0.2578141391277313], [0.04681042581796646], [0.9665305614471436], [0.8088449239730835], [0.9710745215415955], [0.1791502684354782], [0.2580713927745819], [0.8091526031494141], [1.607452392578125], [0.26558735966682434], [0.42535215616226196], [0.4389955997467041], [1.1796278953552246], [0.6374965906143188], [0.8435908555984497], [0.5386030673980713], [-0.054670799523591995], [0.7583366632461548], [0.4271273612976074], [0.1447514444589615], [0.0028325729072093964], [1.005521297454834], [0.5356059074401855], [1.3069672584533691], [-0.08073379099369049], [0.38437536358833313], [0.5884637236595154], [0.3046473264694214], [0.7650153040885925], [1.133865237236023], [0.5870016813278198], [0.25732657313346863], [0.14384092390537262], [0.055729471147060394], [0.9818052649497986], [0.17506898939609528], [0.5086444616317749], [0.13954642415046692], [0.027649827301502228], [-0.052505090832710266], [1.4814540147781372], [0.11804424226284027], [0.8417627215385437], [1.5247197151184082], [0.3551541864871979], [1.6239771842956543], [0.013775087893009186], [0.8526049852371216], [-0.04417979717254639], [0.153986394405365], [0.026583485305309296], [0.5253838300704956], [0.5814460515975952], [0.026536792516708374], [0.4169594645500183], [0.06569942831993103], [0.2900663912296295], [0.8614532947540283], [0.49817192554473877], [0.4159550070762634], [0.4804060757160187], [0.5794458389282227], [0.4480404257774353], [1.937148094177246], [0.7305901646614075], [0.049469366669654846], [0.7343547344207764], [0.5601871609687805], [0.6573054790496826], [0.5093348026275635], [0.6564600467681885], [0.2813343107700348], [0.3947528898715973], [0.4230596423149109], [0.19322948157787323], [0.47624844312667847], [0.38454166054725647], [0.9502915740013123], [0.5567618608474731], [0.06320030242204666], [1.2545959949493408], [-0.0962982326745987], [1.915682077407837], [0.1477603316307068], [0.5938067436218262], [0.7029964923858643], [0.1710951328277588], [0.02344168722629547], [0.29008761048316956]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm 3"
      ],
      "metadata": {
        "id": "gGJNIY9Xnd-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "import time\n",
        "\n",
        "def GNNCausalExplanation(dataset, lambdas, learning_rate, h_size, h_layers, num_epochs, delta):\n",
        "    total_loss = []\n",
        "    for graph_id in dataset:\n",
        "        cg = dataset[graph_id]\n",
        "        v_star, one_hop_neighbors, two_hop_neighbors, out_of_neighborhood = cg.categorize_neighbors(target_node=cg.sort()[0])\n",
        "        print(f\"This is group: {graph_id} and this is first subgraph \")\n",
        "        print(f\"Target node: {v_star}\")\n",
        "        print(f\"1-hop neighbors of A: {one_hop_neighbors}\")\n",
        "        print(f\"2-hop neighbors of A: {two_hop_neighbors}\")\n",
        "        print(f\"Out of neighborhood of A: {out_of_neighborhood}\")\n",
        "        causal_loss = train(cg, lambdas = lambdas, learning_rate = learning_rate, h_size = h_size, h_layers = h_layers, num_epochs = num_epochs)\n",
        "        print(\"The loss value is : \",causal_loss,'\\n')\n",
        "        total_loss.append(causal_loss)\n",
        "    total_loss = [x for x in total_loss if not math.isnan(x[0])]\n",
        "    print(total_loss)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(total_loss)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss over epochs')\n",
        "    plt.savefig('alg3 NCM Loss over epochs.png')\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start_time1 = time.time()\n",
        "    AIDS = \"/content/drive/MyDrive/data/AIDS/\"\n",
        "    AIDS_df = pd.read_csv(AIDS + 'AIDS_A.txt', sep=',', header=None, names=['from', 'to'])\n",
        "    AIDS_graph_indicator = pd.read_csv(AIDS + 'AIDS_graph_indicator.txt', header=None, names=['graph_id'])\n",
        "    AIDS_node_labels = pd.read_csv(AIDS + 'AIDS_node_labels.txt', header=None, names=['node_label'])\n",
        "    AIDS_df['graph_id'] = AIDS_graph_indicator\n",
        "    AIDS_df['node_label'] = AIDS_node_labels\n",
        "    grouped = AIDS_df.groupby('graph_id')\n",
        "    AIDS_causal_graphs = {}\n",
        "    for graph_id, group in grouped:\n",
        "        V = set(group['from']).union(set(group['to']))\n",
        "        edges = list(zip(group['from'], group['to'])) + list(zip(group['to'], group['from']))\n",
        "        AIDS_causal_graphs[graph_id] = CausalGraph(V=V, path=edges)\n",
        "\n",
        "    Mutagenicity = \"/content/drive/MyDrive/data/Mutagenicity/\"\n",
        "    Mutagenicity_df = pd.read_csv(Mutagenicity + 'Mutagenicity_A.txt', sep=',', header=None, names=['from', 'to'])\n",
        "    Mutagenicity_graph_indicator = pd.read_csv(Mutagenicity + 'Mutagenicity_graph_indicator.txt', header=None,names=['graph_id'])\n",
        "    Mutagenicity_node_labels = pd.read_csv(Mutagenicity + 'Mutagenicity_node_labels.txt', header=None,names=['node_label'])\n",
        "    Mutagenicity_df['graph_id'] = Mutagenicity_graph_indicator\n",
        "    Mutagenicity_df['node_label'] = Mutagenicity_node_labels\n",
        "    grouped = Mutagenicity_df.groupby('graph_id')\n",
        "    Mutagenicity_causal_graphs = {}\n",
        "    for graph_id, group in grouped:\n",
        "        V = set(group['from']).union(set(group['to']))\n",
        "        edges = list(zip(group['from'], group['to'])) + list(zip(group['to'], group['from']))\n",
        "        Mutagenicity_causal_graphs[graph_id] = CausalGraph(V=V, path=edges)\n",
        "    end_time1 = time.time()\n",
        "    t1 = end_time1 - start_time1\n",
        "    print('The time of the data preprocessing was : ', t1)\n",
        "\n",
        "    num_subgraph_limit = 5\n",
        "    delta = 0.01\n",
        "    num_nodes_density = 5\n",
        "    start_time2 = time.time()\n",
        "    GNNCausalExplanation(dataset= AIDS_causal_graphs, lambdas = 0.1, learning_rate = 0.001, h_size = 128, h_layers = 2, num_epochs = 2, delta=0.01)\n",
        "    end_time2 = time.time()\n",
        "    t2 = end_time2 - start_time2\n",
        "    print('The time of the calculation was : ',t2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zxqOWfkQnNY0",
        "outputId": "9c78a8eb-9aa8-47aa-f300-7c4d920c9123"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "The loss value is :  [0.3878481686115265] \n",
            "\n",
            "The loss value is :  [0.3878481686115265] \n",
            "\n",
            "This is group: 1446.0 and this is first subgraph \n",
            "Target node: 10836\n",
            "1-hop neighbors of A: {10841}\n",
            "2-hop neighbors of A: {10842}\n",
            "Out of neighborhood of A: {10840, 10837, 10838, 10839}\n",
            "The loss value is :  [0.4989898204803467] \n",
            "\n",
            "The loss value is :  [0.4989898204803467] \n",
            "\n",
            "This is group: 1447.0 and this is first subgraph \n",
            "Target node: 10840\n",
            "1-hop neighbors of A: {10841, 10844}\n",
            "2-hop neighbors of A: {10842, 10843}\n",
            "Out of neighborhood of A: {10848, 10849, 10850, 10851, 10852, 10853, 10854, 10855, 10856, 10857, 10858, 10859, 10845, 10846, 10847}\n",
            "The loss value is :  [0.5768148899078369] \n",
            "\n",
            "The loss value is :  [0.5768148899078369] \n",
            "\n",
            "This is group: 1448.0 and this is first subgraph \n",
            "Target node: 10859\n",
            "1-hop neighbors of A: {10860, 10861, 10862}\n",
            "2-hop neighbors of A: {10864, 10865, 10863}\n",
            "Out of neighborhood of A: set()\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1449.0 and this is first subgraph \n",
            "Target node: 10861\n",
            "1-hop neighbors of A: {10862}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {10863, 10864, 10865, 10866, 10867, 10868}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1450.0 and this is first subgraph \n",
            "Target node: 10869\n",
            "1-hop neighbors of A: {10873, 10870}\n",
            "2-hop neighbors of A: {10874, 10871}\n",
            "Out of neighborhood of A: {10872, 10875, 10876}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1451.0 and this is first subgraph \n",
            "Target node: 10873\n",
            "1-hop neighbors of A: {10879}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {10880, 10881, 10875, 10876, 10877, 10878}\n",
            "The loss value is :  [0.2892671823501587] \n",
            "\n",
            "The loss value is :  [0.2892671823501587] \n",
            "\n",
            "This is group: 1452.0 and this is first subgraph \n",
            "Target node: 10871\n",
            "1-hop neighbors of A: {10884}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {10881, 10882, 10883, 10875, 10876, 10878}\n",
            "The loss value is :  [0.2640497386455536] \n",
            "\n",
            "The loss value is :  [0.2640497386455536] \n",
            "\n",
            "This is group: 1453.0 and this is first subgraph \n",
            "Target node: 10871\n",
            "1-hop neighbors of A: {10884}\n",
            "2-hop neighbors of A: {10885}\n",
            "Out of neighborhood of A: {10880, 10886, 10887, 10888, 10889, 10890, 10891, 10892, 10893, 10894, 10895, 10896, 10897, 10898, 10899, 10900, 10901, 10902, 10903, 10904, 10905, 10906, 10907, 10908, 10909, 10910, 10872, 10873, 10874, 10878, 10879}\n",
            "The loss value is :  [0.5825565457344055] \n",
            "\n",
            "The loss value is :  [0.5825565457344055] \n",
            "\n",
            "This is group: 1454.0 and this is first subgraph \n",
            "Target node: 10910\n",
            "1-hop neighbors of A: {10911}\n",
            "2-hop neighbors of A: {10912}\n",
            "Out of neighborhood of A: {10913, 10914, 10915, 10916}\n",
            "The loss value is :  [0.781345009803772] \n",
            "\n",
            "The loss value is :  [0.781345009803772] \n",
            "\n",
            "This is group: 1455.0 and this is first subgraph \n",
            "Target node: 10895\n",
            "1-hop neighbors of A: {10896}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {10901, 10902, 10903, 10913, 10915, 10916, 10917, 10918, 10919, 10920, 10921, 10922, 10923, 10924, 10925, 10926}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1456.0 and this is first subgraph \n",
            "Target node: 10923\n",
            "1-hop neighbors of A: {10924}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {10927, 10928, 10929, 10935, 10936, 10937}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1457.0 and this is first subgraph \n",
            "Target node: 10928\n",
            "1-hop neighbors of A: {10937}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {10929, 10930, 10931, 10932, 10934, 10936, 10943}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1458.0 and this is first subgraph \n",
            "Target node: 10931\n",
            "1-hop neighbors of A: {10936}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {10944, 10945, 10946, 10947, 10948, 10949, 10950, 10951, 10952, 10953, 10954, 10955, 10956, 10957, 10958, 10959, 10960, 10961, 10932, 10933, 10937, 10938, 10939, 10940, 10941, 10942, 10943}\n",
            "The loss value is :  [0.5046219825744629] \n",
            "\n",
            "The loss value is :  [0.5046219825744629] \n",
            "\n",
            "This is group: 1459.0 and this is first subgraph \n",
            "Target node: 10960\n",
            "1-hop neighbors of A: {10961}\n",
            "2-hop neighbors of A: {10962}\n",
            "Out of neighborhood of A: {10963, 10964, 10965}\n",
            "The loss value is :  [0.20804496109485626] \n",
            "\n",
            "The loss value is :  [0.20804496109485626] \n",
            "\n",
            "This is group: 1460.0 and this is first subgraph \n",
            "Target node: 10957\n",
            "1-hop neighbors of A: {10958}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {10960, 10961, 10965, 10966, 10967, 10968, 10969, 10970}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1461.0 and this is first subgraph \n",
            "Target node: 10965\n",
            "1-hop neighbors of A: {10967}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {10976, 10971, 10972, 10973, 10974, 10975}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1462.0 and this is first subgraph \n",
            "Target node: 10974\n",
            "1-hop neighbors of A: {10980}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {10976, 10977, 10978, 10979, 10981, 10975}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1463.0 and this is first subgraph \n",
            "Target node: 10977\n",
            "1-hop neighbors of A: {10984}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {10981, 10982, 10983, 10985, 10986, 10987}\n",
            "The loss value is :  [0.28609949350357056] \n",
            "\n",
            "The loss value is :  [0.28609949350357056] \n",
            "\n",
            "This is group: 1464.0 and this is first subgraph \n",
            "Target node: 10981\n",
            "1-hop neighbors of A: {10992}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {10983, 10986, 10988, 10989, 10990, 10991}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1465.0 and this is first subgraph \n",
            "Target node: 10972\n",
            "1-hop neighbors of A: {10996}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {10976, 10979, 10993, 10994, 10995, 10997, 10998, 10973, 10974}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1466.0 and this is first subgraph \n",
            "Target node: 10974\n",
            "1-hop neighbors of A: {10975}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11008, 11009, 11010, 11011, 11012, 11013, 11014, 11015, 11016, 11017, 11018, 11019, 11020, 11021, 11022, 10979, 10980, 10983, 10984, 10989, 10997, 10998, 10999, 11000, 11001, 11002, 11003, 11004, 11005, 11006, 11007}\n",
            "The loss value is :  [0.5704163908958435] \n",
            "\n",
            "The loss value is :  [0.5704163908958435] \n",
            "\n",
            "This is group: 1467.0 and this is first subgraph \n",
            "Target node: 11023\n",
            "1-hop neighbors of A: {11024, 11025, 11026}\n",
            "2-hop neighbors of A: {11027, 11028}\n",
            "Out of neighborhood of A: {11029}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1468.0 and this is first subgraph \n",
            "Target node: 11027\n",
            "1-hop neighbors of A: {11029}\n",
            "2-hop neighbors of A: {11031}\n",
            "Out of neighborhood of A: {11028, 11030, 11032, 11033, 11034, 11035}\n",
            "The loss value is :  [1.0569618940353394] \n",
            "\n",
            "The loss value is :  [1.0569618940353394] \n",
            "\n",
            "This is group: 1469.0 and this is first subgraph \n",
            "Target node: 11033\n",
            "1-hop neighbors of A: {11035, 11036}\n",
            "2-hop neighbors of A: {11039}\n",
            "Out of neighborhood of A: {11040, 11041, 11034, 11037, 11038}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1470.0 and this is first subgraph \n",
            "Target node: 11039\n",
            "1-hop neighbors of A: {11041}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11042, 11043, 11044, 11045, 11046, 11047}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1471.0 and this is first subgraph \n",
            "Target node: 11046\n",
            "1-hop neighbors of A: {11048, 11049}\n",
            "2-hop neighbors of A: {11051}\n",
            "Out of neighborhood of A: {11050, 11052, 11053, 11047}\n",
            "The loss value is :  [0.5463386178016663] \n",
            "\n",
            "The loss value is :  [0.5463386178016663] \n",
            "\n",
            "This is group: 1472.0 and this is first subgraph \n",
            "Target node: 11050\n",
            "1-hop neighbors of A: {11051}\n",
            "2-hop neighbors of A: {11053}\n",
            "Out of neighborhood of A: {11054, 11055, 11056, 11057, 11058, 11059}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1473.0 and this is first subgraph \n",
            "Target node: 11057\n",
            "1-hop neighbors of A: {11059}\n",
            "2-hop neighbors of A: {11060, 11061}\n",
            "Out of neighborhood of A: {11064, 11065, 11062, 11063}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1474.0 and this is first subgraph \n",
            "Target node: 11064\n",
            "1-hop neighbors of A: {11065}\n",
            "2-hop neighbors of A: {11066, 11067}\n",
            "Out of neighborhood of A: {11072, 11073, 11074, 11075, 11076, 11068, 11069, 11070, 11071}\n",
            "The loss value is :  [0.5802364349365234] \n",
            "\n",
            "The loss value is :  [0.5802364349365234] \n",
            "\n",
            "This is group: 1475.0 and this is first subgraph \n",
            "Target node: 11073\n",
            "1-hop neighbors of A: {11077, 11079}\n",
            "2-hop neighbors of A: {11078}\n",
            "Out of neighborhood of A: {11075, 11076}\n",
            "The loss value is :  [0.5496959090232849] \n",
            "\n",
            "The loss value is :  [0.5496959090232849] \n",
            "\n",
            "This is group: 1476.0 and this is first subgraph \n",
            "Target node: 11079\n",
            "1-hop neighbors of A: {11080}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11081, 11082, 11083, 11084, 11085, 11086}\n",
            "The loss value is :  [0.597266674041748] \n",
            "\n",
            "The loss value is :  [0.597266674041748] \n",
            "\n",
            "This is group: 1477.0 and this is first subgraph \n",
            "Target node: 11083\n",
            "1-hop neighbors of A: {11086}\n",
            "2-hop neighbors of A: {11088, 11089}\n",
            "Out of neighborhood of A: {11090, 11084, 11087}\n",
            "The loss value is :  [1.4731383323669434] \n",
            "\n",
            "The loss value is :  [1.4731383323669434] \n",
            "\n",
            "This is group: 1478.0 and this is first subgraph \n",
            "Target node: 11085\n",
            "1-hop neighbors of A: {11087}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11089, 11090, 11091, 11092, 11093, 11094, 11095}\n",
            "The loss value is :  [0.33102118968963623] \n",
            "\n",
            "The loss value is :  [0.33102118968963623] \n",
            "\n",
            "This is group: 1479.0 and this is first subgraph \n",
            "Target node: 11094\n",
            "1-hop neighbors of A: {11096, 11095}\n",
            "2-hop neighbors of A: {11097, 11098, 11099, 11100}\n",
            "Out of neighborhood of A: set()\n",
            "The loss value is :  [0.7165900468826294] \n",
            "\n",
            "The loss value is :  [0.7165900468826294] \n",
            "\n",
            "This is group: 1480.0 and this is first subgraph \n",
            "Target node: 11098\n",
            "1-hop neighbors of A: {11101}\n",
            "2-hop neighbors of A: {11100}\n",
            "Out of neighborhood of A: {11102, 11103, 11104, 11105, 11106, 11107, 11108, 11109, 11110, 11111, 11112, 11113, 11114, 11115, 11116, 11117}\n",
            "The loss value is :  [0.559980571269989] \n",
            "\n",
            "The loss value is :  [0.559980571269989] \n",
            "\n",
            "This is group: 1481.0 and this is first subgraph \n",
            "Target node: 11114\n",
            "1-hop neighbors of A: {11120, 11119}\n",
            "2-hop neighbors of A: {11121}\n",
            "Out of neighborhood of A: {11122, 11123, 11117, 11118}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1482.0 and this is first subgraph \n",
            "Target node: 11118\n",
            "1-hop neighbors of A: {11119}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11136, 11137, 11138, 11139, 11140, 11141, 11142, 11143, 11144, 11145, 11146, 11147, 11148, 11122, 11123, 11124, 11125, 11126, 11127, 11128, 11129, 11130, 11131, 11132, 11133, 11134, 11135}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1483.0 and this is first subgraph \n",
            "Target node: 11146\n",
            "1-hop neighbors of A: {11148}\n",
            "2-hop neighbors of A: {11150}\n",
            "Out of neighborhood of A: {11147, 11149}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1484.0 and this is first subgraph \n",
            "Target node: 11149\n",
            "1-hop neighbors of A: {11151}\n",
            "2-hop neighbors of A: {11152, 11153}\n",
            "Out of neighborhood of A: {11154, 11155}\n",
            "The loss value is :  [0.3493131697177887] \n",
            "\n",
            "The loss value is :  [0.3493131697177887] \n",
            "\n",
            "This is group: 1485.0 and this is first subgraph \n",
            "Target node: 11149\n",
            "1-hop neighbors of A: {11150}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11153, 11154, 11155, 11156, 11164, 11165, 11166}\n",
            "The loss value is :  [0.5353091955184937] \n",
            "\n",
            "The loss value is :  [0.5353091955184937] \n",
            "\n",
            "This is group: 1486.0 and this is first subgraph \n",
            "Target node: 11165\n",
            "1-hop neighbors of A: {11166}\n",
            "2-hop neighbors of A: {11167}\n",
            "Out of neighborhood of A: {11168, 11169, 11170, 11171, 11172}\n",
            "The loss value is :  [0.5654123425483704] \n",
            "\n",
            "The loss value is :  [0.5654123425483704] \n",
            "\n",
            "This is group: 1487.0 and this is first subgraph \n",
            "Target node: 11169\n",
            "1-hop neighbors of A: {11173}\n",
            "2-hop neighbors of A: {11174}\n",
            "Out of neighborhood of A: {11171, 11172, 11175, 11176, 11177}\n",
            "The loss value is :  [0.6678757071495056] \n",
            "\n",
            "The loss value is :  [0.6678757071495056] \n",
            "\n",
            "This is group: 1488.0 and this is first subgraph \n",
            "Target node: 11158\n",
            "1-hop neighbors of A: {11182}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11172, 11176, 11177, 11178, 11179, 11180, 11181, 11183}\n",
            "The loss value is :  [0.41733038425445557] \n",
            "\n",
            "The loss value is :  [0.41733038425445557] \n",
            "\n",
            "This is group: 1489.0 and this is first subgraph \n",
            "Target node: 11157\n",
            "1-hop neighbors of A: {11189}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11200, 11158, 11159, 11160, 11161, 11162, 11163, 11179, 11180, 11183, 11184, 11185, 11186, 11187, 11188, 11190, 11191, 11192, 11193, 11194, 11195, 11196, 11197, 11198, 11199}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1490.0 and this is first subgraph \n",
            "Target node: 11160\n",
            "1-hop neighbors of A: {11200}\n",
            "2-hop neighbors of A: {11201}\n",
            "Out of neighborhood of A: {11192, 11202}\n",
            "The loss value is :  [0.5635221004486084] \n",
            "\n",
            "The loss value is :  [0.5635221004486084] \n",
            "\n",
            "This is group: 1491.0 and this is first subgraph \n",
            "Target node: 11173\n",
            "1-hop neighbors of A: {11207}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11202, 11203, 11204, 11205, 11206, 11181, 11183, 11192}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1492.0 and this is first subgraph \n",
            "Target node: 11164\n",
            "1-hop neighbors of A: {11211, 11212}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11168, 11173, 11207, 11208, 11209, 11210, 11165}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1493.0 and this is first subgraph \n",
            "Target node: 11164\n",
            "1-hop neighbors of A: {11212}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11172, 11173, 11176, 11177, 11181, 11182, 11183, 11185, 11186, 11187}\n",
            "The loss value is :  [0.6146576404571533] \n",
            "\n",
            "The loss value is :  [0.6146576404571533] \n",
            "\n",
            "This is group: 1494.0 and this is first subgraph \n",
            "Target node: 11158\n",
            "1-hop neighbors of A: {11189}\n",
            "2-hop neighbors of A: {11198}\n",
            "Out of neighborhood of A: {11200, 11213, 11214, 11215, 11216, 11217, 11218, 11219, 11220, 11221, 11159, 11222, 11161, 11162, 11223, 11183, 11187, 11188, 11190, 11192, 11193, 11194, 11195, 11196}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1495.0 and this is first subgraph \n",
            "Target node: 11216\n",
            "1-hop neighbors of A: {11224}\n",
            "2-hop neighbors of A: {11227}\n",
            "Out of neighborhood of A: {11219, 11220, 11223, 11225, 11226, 11228, 11229}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1496.0 and this is first subgraph \n",
            "Target node: 11217\n",
            "1-hop neighbors of A: {11218}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11232, 11233, 11234, 11227, 11228, 11229, 11230, 11231}\n",
            "The loss value is :  [0.46057048439979553] \n",
            "\n",
            "The loss value is :  [0.46057048439979553] \n",
            "\n",
            "This is group: 1497.0 and this is first subgraph \n",
            "Target node: 11217\n",
            "1-hop neighbors of A: {11218}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11235, 11236, 11237, 11222, 11223, 11225, 11227}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1498.0 and this is first subgraph \n",
            "Target node: 11236\n",
            "1-hop neighbors of A: {11238}\n",
            "2-hop neighbors of A: {11242}\n",
            "Out of neighborhood of A: {11240, 11241, 11237, 11239}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1499.0 and this is first subgraph \n",
            "Target node: 11238\n",
            "1-hop neighbors of A: {11243}\n",
            "2-hop neighbors of A: {11241}\n",
            "Out of neighborhood of A: {11244, 11245, 11246, 11247, 11248, 11249, 11250, 11251, 11252, 11253}\n",
            "The loss value is :  [0.874178946018219] \n",
            "\n",
            "The loss value is :  [0.874178946018219] \n",
            "\n",
            "This is group: 1500.0 and this is first subgraph \n",
            "Target node: 11247\n",
            "1-hop neighbors of A: {11248}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11253, 11254, 11255, 11256, 11257, 11258, 11259}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1501.0 and this is first subgraph \n",
            "Target node: 11255\n",
            "1-hop neighbors of A: {11259}\n",
            "2-hop neighbors of A: {11258}\n",
            "Out of neighborhood of A: {11264, 11265, 11260, 11261, 11262, 11263}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1502.0 and this is first subgraph \n",
            "Target node: 11264\n",
            "1-hop neighbors of A: {11265, 11269, 11270}\n",
            "2-hop neighbors of A: {11266, 11271}\n",
            "Out of neighborhood of A: {11267, 11268}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1503.0 and this is first subgraph \n",
            "Target node: 11267\n",
            "1-hop neighbors of A: {11274}\n",
            "2-hop neighbors of A: {11273}\n",
            "Out of neighborhood of A: {11268, 11269, 11270, 11271, 11272, 11275, 11276}\n",
            "The loss value is :  [0.4987049698829651] \n",
            "\n",
            "The loss value is :  [0.4987049698829651] \n",
            "\n",
            "This is group: 1504.0 and this is first subgraph \n",
            "Target node: 11276\n",
            "1-hop neighbors of A: {11277, 11278, 11279}\n",
            "2-hop neighbors of A: {11280, 11281}\n",
            "Out of neighborhood of A: {11285, 11286}\n",
            "The loss value is :  [0.47399988770484924] \n",
            "\n",
            "The loss value is :  [0.47399988770484924] \n",
            "\n",
            "This is group: 1505.0 and this is first subgraph \n",
            "Target node: 11285\n",
            "1-hop neighbors of A: {11290, 11286}\n",
            "2-hop neighbors of A: {11291, 11287}\n",
            "Out of neighborhood of A: {11288, 11289}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1506.0 and this is first subgraph \n",
            "Target node: 11286\n",
            "1-hop neighbors of A: {11294}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11289, 11290, 11291, 11292, 11293, 11295, 11296, 11297, 11298, 11299, 11300, 11301, 11302, 11303, 11304, 11305, 11306, 11307, 11308, 11309, 11310, 11311, 11312, 11313, 11314, 11315, 11316, 11317, 11318, 11319, 11320, 11321, 11322}\n",
            "The loss value is :  [0.5560839176177979] \n",
            "\n",
            "The loss value is :  [0.5560839176177979] \n",
            "\n",
            "This is group: 1507.0 and this is first subgraph \n",
            "Target node: 11320\n",
            "1-hop neighbors of A: {11323, 11324}\n",
            "2-hop neighbors of A: {11327}\n",
            "Out of neighborhood of A: {11321, 11325, 11326}\n",
            "The loss value is :  [0.4189555048942566] \n",
            "\n",
            "The loss value is :  [0.4189555048942566] \n",
            "\n",
            "This is group: 1508.0 and this is first subgraph \n",
            "Target node: 11328\n",
            "1-hop neighbors of A: {11329, 11333}\n",
            "2-hop neighbors of A: {11330, 11334}\n",
            "Out of neighborhood of A: {11331, 11332}\n",
            "The loss value is :  [0.49682506918907166] \n",
            "\n",
            "The loss value is :  [0.49682506918907166] \n",
            "\n",
            "This is group: 1509.0 and this is first subgraph \n",
            "Target node: 11332\n",
            "1-hop neighbors of A: {11339, 11333}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11334, 11335, 11336, 11337, 11338}\n",
            "The loss value is :  [0.45674896240234375] \n",
            "\n",
            "The loss value is :  [0.45674896240234375] \n",
            "\n",
            "This is group: 1510.0 and this is first subgraph \n",
            "Target node: 11332\n",
            "1-hop neighbors of A: {11333}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11337, 11339, 11340, 11341, 11342}\n",
            "The loss value is :  [0.4443758726119995] \n",
            "\n",
            "The loss value is :  [0.4443758726119995] \n",
            "\n",
            "This is group: 1511.0 and this is first subgraph \n",
            "Target node: 11340\n",
            "1-hop neighbors of A: {11346}\n",
            "2-hop neighbors of A: {11348}\n",
            "Out of neighborhood of A: {11341, 11342, 11343, 11344, 11345, 11347}\n",
            "The loss value is :  [0.4658746123313904] \n",
            "\n",
            "The loss value is :  [0.4658746123313904] \n",
            "\n",
            "This is group: 1512.0 and this is first subgraph \n",
            "Target node: 11344\n",
            "1-hop neighbors of A: {11347}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11345, 11346, 11348, 11349, 11350, 11351, 11352, 11353}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1513.0 and this is first subgraph \n",
            "Target node: 11349\n",
            "1-hop neighbors of A: {11354}\n",
            "2-hop neighbors of A: {11353, 11355}\n",
            "Out of neighborhood of A: {11360, 11350, 11352, 11356, 11357, 11358, 11359}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1514.0 and this is first subgraph \n",
            "Target node: 11359\n",
            "1-hop neighbors of A: {11369}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11392, 11360, 11361, 11362, 11363, 11364, 11365, 11366, 11367, 11368, 11370, 11371, 11372, 11373, 11374, 11375, 11376, 11377, 11378, 11379, 11380, 11381, 11382, 11383, 11384, 11385, 11386, 11387, 11388, 11389, 11390, 11391}\n",
            "The loss value is :  [0.5230194330215454] \n",
            "\n",
            "The loss value is :  [0.5230194330215454] \n",
            "\n",
            "This is group: 1515.0 and this is first subgraph \n",
            "Target node: 11388\n",
            "1-hop neighbors of A: {11393}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11394, 11395, 11396, 11397, 11398, 11389, 11390}\n",
            "The loss value is :  [0.35553890466690063] \n",
            "\n",
            "The loss value is :  [0.35553890466690063] \n",
            "\n",
            "This is group: 1516.0 and this is first subgraph \n",
            "Target node: 11383\n",
            "1-hop neighbors of A: {11384}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11392, 11393, 11394, 11398, 11399, 11400, 11401, 11402, 11403, 11404}\n",
            "The loss value is :  [0.4510223865509033] \n",
            "\n",
            "The loss value is :  [0.4510223865509033] \n",
            "\n",
            "This is group: 1517.0 and this is first subgraph \n",
            "Target node: 11405\n",
            "1-hop neighbors of A: {11409, 11410, 11406}\n",
            "2-hop neighbors of A: {11408, 11411, 11415, 11416, 11407}\n",
            "Out of neighborhood of A: {11412, 11413}\n",
            "The loss value is :  [0.4443405270576477] \n",
            "\n",
            "The loss value is :  [0.4443405270576477] \n",
            "\n",
            "This is group: 1518.0 and this is first subgraph \n",
            "Target node: 11408\n",
            "1-hop neighbors of A: {11409}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11415, 11416, 11417, 11418, 11419, 11420, 11421, 11422}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1519.0 and this is first subgraph \n",
            "Target node: 11418\n",
            "1-hop neighbors of A: {11422, 11423}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11424, 11425, 11426, 11427, 11428, 11419}\n",
            "The loss value is :  [0.66070556640625] \n",
            "\n",
            "The loss value is :  [0.66070556640625] \n",
            "\n",
            "This is group: 1520.0 and this is first subgraph \n",
            "Target node: 11426\n",
            "1-hop neighbors of A: {11429, 11430}\n",
            "2-hop neighbors of A: {11431, 11432, 11433}\n",
            "Out of neighborhood of A: set()\n",
            "The loss value is :  [0.4231348931789398] \n",
            "\n",
            "The loss value is :  [0.4231348931789398] \n",
            "\n",
            "This is group: 1521.0 and this is first subgraph \n",
            "Target node: 11431\n",
            "1-hop neighbors of A: {11432}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11433, 11434, 11435, 11436, 11437, 11438, 11439}\n",
            "The loss value is :  [0.5231825709342957] \n",
            "\n",
            "The loss value is :  [0.5231825709342957] \n",
            "\n",
            "This is group: 1522.0 and this is first subgraph \n",
            "Target node: 11440\n",
            "1-hop neighbors of A: {11441, 11442, 11443, 11444, 11445}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11448, 11447}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1523.0 and this is first subgraph \n",
            "Target node: 11447\n",
            "1-hop neighbors of A: {11449, 11450}\n",
            "2-hop neighbors of A: {11451, 11459}\n",
            "Out of neighborhood of A: {11448, 11452, 11453, 11454, 11455, 11456, 11457, 11458, 11460, 11461, 11462, 11463, 11464, 11465, 11466, 11467, 11468, 11469, 11470, 11471, 11472, 11473, 11474, 11475, 11476, 11490, 11491, 11492, 11493, 11494, 11495, 11499}\n",
            "The loss value is :  [0.39900144934654236] \n",
            "\n",
            "The loss value is :  [0.39900144934654236] \n",
            "\n",
            "This is group: 1524.0 and this is first subgraph \n",
            "Target node: 11474\n",
            "1-hop neighbors of A: {11476}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11475, 11477, 11478, 11479, 11480, 11484, 11485}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1525.0 and this is first subgraph \n",
            "Target node: 11479\n",
            "1-hop neighbors of A: {11485}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11480, 11481, 11482, 11483, 11484, 11486, 11487, 11488, 11489, 11496, 11497, 11498, 11499, 11500, 11501, 11502, 11503, 11504}\n",
            "The loss value is :  [0.6222559213638306] \n",
            "\n",
            "The loss value is :  [0.6222559213638306] \n",
            "\n",
            "This is group: 1526.0 and this is first subgraph \n",
            "Target node: 11504\n",
            "1-hop neighbors of A: {11505, 11506, 11507}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11508, 11509, 11510}\n",
            "The loss value is :  [0.4320249557495117] \n",
            "\n",
            "The loss value is :  [0.4320249557495117] \n",
            "\n",
            "This is group: 1527.0 and this is first subgraph \n",
            "Target node: 11508\n",
            "1-hop neighbors of A: {11510}\n",
            "2-hop neighbors of A: {11512, 11511}\n",
            "Out of neighborhood of A: {11513, 11514}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1528.0 and this is first subgraph \n",
            "Target node: 11512\n",
            "1-hop neighbors of A: {11515}\n",
            "2-hop neighbors of A: {11516}\n",
            "Out of neighborhood of A: {11520, 11521, 11522, 11523, 11524, 11525, 11526, 11527, 11528, 11529, 11530, 11531, 11532, 11514, 11517, 11518, 11519}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1529.0 and this is first subgraph \n",
            "Target node: 11530\n",
            "1-hop neighbors of A: {11532, 11533}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11534, 11535, 11536, 11537, 11538, 11539}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1530.0 and this is first subgraph \n",
            "Target node: 11537\n",
            "1-hop neighbors of A: {11539}\n",
            "2-hop neighbors of A: {11538}\n",
            "Out of neighborhood of A: {11540, 11541, 11542, 11543, 11544}\n",
            "The loss value is :  [0.7371317744255066] \n",
            "\n",
            "The loss value is :  [0.7371317744255066] \n",
            "\n",
            "This is group: 1531.0 and this is first subgraph \n",
            "Target node: 11543\n",
            "1-hop neighbors of A: {11545, 11546}\n",
            "2-hop neighbors of A: {11549, 11550}\n",
            "Out of neighborhood of A: {11544, 11547, 11548}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1532.0 and this is first subgraph \n",
            "Target node: 11545\n",
            "1-hop neighbors of A: {11548}\n",
            "2-hop neighbors of A: {11550}\n",
            "Out of neighborhood of A: {11546, 11551, 11552, 11553, 11554, 11555, 11556, 11557, 11558, 11559, 11560, 11561, 11562, 11563, 11564, 11565, 11566}\n",
            "The loss value is :  [0.40542837977409363] \n",
            "\n",
            "The loss value is :  [0.40542837977409363] \n",
            "\n",
            "This is group: 1533.0 and this is first subgraph \n",
            "Target node: 11564\n",
            "1-hop neighbors of A: {11567}\n",
            "2-hop neighbors of A: {11569}\n",
            "Out of neighborhood of A: {11565, 11568, 11570, 11571, 11572, 11573}\n",
            "The loss value is :  [1.0545767545700073] \n",
            "\n",
            "The loss value is :  [1.0545767545700073] \n",
            "\n",
            "This is group: 1534.0 and this is first subgraph \n",
            "Target node: 11572\n",
            "1-hop neighbors of A: {11574}\n",
            "2-hop neighbors of A: {11576, 11575}\n",
            "Out of neighborhood of A: {11577, 11578}\n",
            "The loss value is :  [0.7248049974441528] \n",
            "\n",
            "The loss value is :  [0.7248049974441528] \n",
            "\n",
            "This is group: 1535.0 and this is first subgraph \n",
            "Target node: 11576\n",
            "1-hop neighbors of A: {11579}\n",
            "2-hop neighbors of A: {11577, 11582}\n",
            "Out of neighborhood of A: {11578, 11580, 11581, 11583, 11584, 11585, 11586, 11587, 11588, 11589, 11590, 11591, 11592, 11593, 11594, 11595, 11596, 11597, 11598, 11599, 11600, 11601, 11602, 11603, 11604, 11605, 11606, 11607, 11608, 11609, 11610, 11611, 11612, 11613, 11614, 11615}\n",
            "The loss value is :  [1.0445870161056519] \n",
            "\n",
            "The loss value is :  [1.0445870161056519] \n",
            "\n",
            "This is group: 1536.0 and this is first subgraph \n",
            "Target node: 11615\n",
            "1-hop neighbors of A: {11616}\n",
            "2-hop neighbors of A: {11617, 11618}\n",
            "Out of neighborhood of A: {11619, 11620}\n",
            "The loss value is :  [0.33126360177993774] \n",
            "\n",
            "The loss value is :  [0.33126360177993774] \n",
            "\n",
            "This is group: 1537.0 and this is first subgraph \n",
            "Target node: 11619\n",
            "1-hop neighbors of A: {11620, 11621}\n",
            "2-hop neighbors of A: {11622}\n",
            "Out of neighborhood of A: {11624, 11623}\n",
            "The loss value is :  [0.9000295996665955] \n",
            "\n",
            "The loss value is :  [0.9000295996665955] \n",
            "\n",
            "This is group: 1538.0 and this is first subgraph \n",
            "Target node: 11625\n",
            "1-hop neighbors of A: {11626, 11629}\n",
            "2-hop neighbors of A: {11627, 11638}\n",
            "Out of neighborhood of A: {11648, 11649, 11650, 11651, 11652, 11628, 11630, 11631, 11632, 11633, 11634, 11635, 11636, 11637, 11639, 11640, 11641, 11642, 11643, 11644, 11645, 11646, 11647}\n",
            "The loss value is :  [0.6206889748573303] \n",
            "\n",
            "The loss value is :  [0.6206889748573303] \n",
            "\n",
            "This is group: 1539.0 and this is first subgraph \n",
            "Target node: 11628\n",
            "1-hop neighbors of A: {11629}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11650, 11652, 11653, 11654, 11655, 11656, 11657, 11658, 11659, 11646}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1540.0 and this is first subgraph \n",
            "Target node: 11628\n",
            "1-hop neighbors of A: {11629}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11660, 11661, 11662, 11663, 11664, 11634, 11636}\n",
            "The loss value is :  [0.2215808629989624] \n",
            "\n",
            "The loss value is :  [0.2215808629989624] \n",
            "\n",
            "This is group: 1541.0 and this is first subgraph \n",
            "Target node: 11660\n",
            "1-hop neighbors of A: {11664}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11663, 11665, 11666, 11667, 11668}\n",
            "The loss value is :  [0.39169344305992126] \n",
            "\n",
            "The loss value is :  [0.39169344305992126] \n",
            "\n",
            "This is group: 1542.0 and this is first subgraph \n",
            "Target node: 11663\n",
            "1-hop neighbors of A: {11664}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11667, 11668, 11669, 11670, 11671, 11672, 11673, 11674, 11675, 11676, 11677, 11678, 11679, 11680, 11681, 11682, 11683, 11684}\n",
            "The loss value is :  [0.49335381388664246] \n",
            "\n",
            "The loss value is :  [0.49335381388664246] \n",
            "\n",
            "This is group: 1543.0 and this is first subgraph \n",
            "Target node: 11682\n",
            "1-hop neighbors of A: {11685, 11686}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11683, 11687, 11688, 11689, 11692, 11693}\n",
            "The loss value is :  [0.4457487463951111] \n",
            "\n",
            "The loss value is :  [0.4457487463951111] \n",
            "\n",
            "This is group: 1544.0 and this is first subgraph \n",
            "Target node: 11690\n",
            "1-hop neighbors of A: {11696, 11694}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11697, 11691, 11693, 11695}\n",
            "The loss value is :  [0.4379982352256775] \n",
            "\n",
            "The loss value is :  [0.4379982352256775] \n",
            "\n",
            "This is group: 1545.0 and this is first subgraph \n",
            "Target node: 11691\n",
            "1-hop neighbors of A: {11696}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11693, 11694, 11695, 11698, 11699, 11700}\n",
            "The loss value is :  [0.8134589195251465] \n",
            "\n",
            "The loss value is :  [0.8134589195251465] \n",
            "\n",
            "This is group: 1546.0 and this is first subgraph \n",
            "Target node: 11698\n",
            "1-hop neighbors of A: {11700}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11701, 11705, 11707, 11708, 11709, 11710}\n",
            "The loss value is :  [0.4272876977920532] \n",
            "\n",
            "The loss value is :  [0.4272876977920532] \n",
            "\n",
            "This is group: 1547.0 and this is first subgraph \n",
            "Target node: 11706\n",
            "1-hop neighbors of A: {11707}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11712, 11713, 11714, 11715, 11716, 11711}\n",
            "The loss value is :  [0.4274950921535492] \n",
            "\n",
            "The loss value is :  [0.4274950921535492] \n",
            "\n",
            "This is group: 1548.0 and this is first subgraph \n",
            "Target node: 11715\n",
            "1-hop neighbors of A: {11717}\n",
            "2-hop neighbors of A: {11719}\n",
            "Out of neighborhood of A: {11720, 11716, 11718}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1549.0 and this is first subgraph \n",
            "Target node: 11718\n",
            "1-hop neighbors of A: {11719}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11721, 11722, 11723, 11724, 11725, 11726}\n",
            "The loss value is :  [0.692564070224762] \n",
            "\n",
            "The loss value is :  [0.692564070224762] \n",
            "\n",
            "This is group: 1550.0 and this is first subgraph \n",
            "Target node: 11723\n",
            "1-hop neighbors of A: {11727}\n",
            "2-hop neighbors of A: {11728, 11729}\n",
            "Out of neighborhood of A: {11730, 11731, 11726}\n",
            "The loss value is :  [0.660405158996582] \n",
            "\n",
            "The loss value is :  [0.660405158996582] \n",
            "\n",
            "This is group: 1551.0 and this is first subgraph \n",
            "Target node: 11725\n",
            "1-hop neighbors of A: {11726}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11730, 11731, 11732, 11733, 11734, 11735, 11736, 11737}\n",
            "The loss value is :  [0.638478696346283] \n",
            "\n",
            "The loss value is :  [0.638478696346283] \n",
            "\n",
            "This is group: 1552.0 and this is first subgraph \n",
            "Target node: 11733\n",
            "1-hop neighbors of A: {11737}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11734, 11735, 11736, 11738, 11739, 11740, 11741, 11742, 11743, 11744, 11745, 11746, 11747, 11748, 11749, 11750, 11751, 11752, 11753, 11754, 11755, 11756, 11757, 11758, 11759, 11760, 11761}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1553.0 and this is first subgraph \n",
            "Target node: 11747\n",
            "1-hop neighbors of A: {11748}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11749, 11751, 11753, 11756, 11757, 11758, 11759, 11762}\n",
            "The loss value is :  [0.7378987073898315] \n",
            "\n",
            "The loss value is :  [0.7378987073898315] \n",
            "\n",
            "This is group: 1554.0 and this is first subgraph \n",
            "Target node: 11758\n",
            "1-hop neighbors of A: {11759}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11763, 11764, 11765, 11766, 11767, 11768}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1555.0 and this is first subgraph \n",
            "Target node: 11764\n",
            "1-hop neighbors of A: {11771}\n",
            "2-hop neighbors of A: {11772}\n",
            "Out of neighborhood of A: {11766, 11767, 11769, 11770, 11773, 11774}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1556.0 and this is first subgraph \n",
            "Target node: 11773\n",
            "1-hop neighbors of A: {11780}\n",
            "2-hop neighbors of A: {11781, 11779, 11782}\n",
            "Out of neighborhood of A: {11776, 11777, 11778, 11783, 11784, 11785, 11786, 11787, 11788, 11789, 11790, 11791, 11792, 11793, 11794, 11795, 11796, 11797, 11798, 11799, 11800, 11801, 11802, 11803, 11804, 11805, 11806, 11807, 11808, 11809, 11810, 11811, 11774, 11775}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1557.0 and this is first subgraph \n",
            "Target node: 11805\n",
            "1-hop neighbors of A: {11829}\n",
            "2-hop neighbors of A: {11830}\n",
            "Out of neighborhood of A: {11806, 11808, 11809, 11810, 11811, 11812, 11813, 11814, 11815, 11816, 11817, 11818, 11819, 11820, 11821, 11822, 11823, 11824, 11825, 11826, 11827, 11828}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1558.0 and this is first subgraph \n",
            "Target node: 11811\n",
            "1-hop neighbors of A: {11834}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11821, 11829, 11830, 11831, 11832, 11833, 11835}\n",
            "The loss value is :  [0.5103705525398254] \n",
            "\n",
            "The loss value is :  [0.5103705525398254] \n",
            "\n",
            "This is group: 1559.0 and this is first subgraph \n",
            "Target node: 11819\n",
            "1-hop neighbors of A: {11836}\n",
            "2-hop neighbors of A: {11837}\n",
            "Out of neighborhood of A: {11840, 11841, 11842, 11843, 11844, 11845, 11846, 11847, 11848, 11849, 11821, 11835, 11838, 11839}\n",
            "The loss value is :  [0.7424955368041992] \n",
            "\n",
            "The loss value is :  [0.7424955368041992] \n",
            "\n",
            "This is group: 1560.0 and this is first subgraph \n",
            "Target node: 11807\n",
            "1-hop neighbors of A: {11808}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11812, 11813, 11848, 11849, 11850, 11851, 11852, 11853, 11825}\n",
            "The loss value is :  [0.4852176904678345] \n",
            "\n",
            "The loss value is :  [0.4852176904678345] \n",
            "\n",
            "This is group: 1561.0 and this is first subgraph \n",
            "Target node: 11812\n",
            "1-hop neighbors of A: {11813}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11815, 11816, 11818, 11819, 11821, 11822, 11824, 11825, 11833, 11834}\n",
            "The loss value is :  [0.5262861251831055] \n",
            "\n",
            "The loss value is :  [0.5262861251831055] \n",
            "\n",
            "This is group: 1562.0 and this is first subgraph \n",
            "Target node: 11854\n",
            "1-hop neighbors of A: {11859, 11855}\n",
            "2-hop neighbors of A: {11856}\n",
            "Out of neighborhood of A: {11857, 11858, 11860}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1563.0 and this is first subgraph \n",
            "Target node: 11858\n",
            "1-hop neighbors of A: {11859, 11862}\n",
            "2-hop neighbors of A: {11861, 11863}\n",
            "Out of neighborhood of A: {11872, 11864, 11865, 11866, 11867, 11868, 11869, 11870, 11871}\n",
            "The loss value is :  [0.7287817597389221] \n",
            "\n",
            "The loss value is :  [0.7287817597389221] \n",
            "\n",
            "This is group: 1564.0 and this is first subgraph \n",
            "Target node: 11869\n",
            "1-hop neighbors of A: {11872}\n",
            "2-hop neighbors of A: {11870}\n",
            "Out of neighborhood of A: {11871, 11873, 11874, 11875, 11876, 11877, 11878, 11879, 11880, 11881, 11882, 11883, 11884, 11885, 11886, 11887, 11888, 11889, 11890, 11891, 11892, 11893, 11894, 11895, 11896, 11897, 11898, 11899, 11900, 11901, 11902, 11903}\n",
            "The loss value is :  [0.45752573013305664] \n",
            "\n",
            "The loss value is :  [0.45752573013305664] \n",
            "\n",
            "This is group: 1565.0 and this is first subgraph \n",
            "Target node: 11899\n",
            "1-hop neighbors of A: {11904}\n",
            "2-hop neighbors of A: {11905}\n",
            "Out of neighborhood of A: {11906, 11907, 11908}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1566.0 and this is first subgraph \n",
            "Target node: 11895\n",
            "1-hop neighbors of A: {11916}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11904, 11906, 11907, 11908, 11909, 11910, 11911, 11912, 11913, 11914, 11915, 11917, 11918, 11919, 11920, 11921, 11922, 11923, 11924, 11925, 11900, 11902}\n",
            "The loss value is :  [1.1462711095809937] \n",
            "\n",
            "The loss value is :  [1.1462711095809937] \n",
            "\n",
            "This is group: 1567.0 and this is first subgraph \n",
            "Target node: 11927\n",
            "1-hop neighbors of A: {11928}\n",
            "2-hop neighbors of A: {11929, 11930}\n",
            "Out of neighborhood of A: {11931, 11932, 11933}\n",
            "The loss value is :  [0.3553928732872009] \n",
            "\n",
            "The loss value is :  [0.3553928732872009] \n",
            "\n",
            "This is group: 1568.0 and this is first subgraph \n",
            "Target node: 11927\n",
            "1-hop neighbors of A: {11937, 11934}\n",
            "2-hop neighbors of A: {11936, 11933, 11935}\n",
            "Out of neighborhood of A: {11938, 11939}\n",
            "The loss value is :  [0.5419064164161682] \n",
            "\n",
            "The loss value is :  [0.5419064164161682] \n",
            "\n",
            "This is group: 1569.0 and this is first subgraph \n",
            "Target node: 11938\n",
            "1-hop neighbors of A: {11939, 11940, 11941}\n",
            "2-hop neighbors of A: {11942, 11943, 11944}\n",
            "Out of neighborhood of A: {11945, 11946, 11947, 11948, 11949, 11950, 11951, 11952, 11953, 11954, 11955, 11956}\n",
            "The loss value is :  [0.7830026149749756] \n",
            "\n",
            "The loss value is :  [0.7830026149749756] \n",
            "\n",
            "This is group: 1570.0 and this is first subgraph \n",
            "Target node: 11953\n",
            "1-hop neighbors of A: {11956}\n",
            "2-hop neighbors of A: {11957}\n",
            "Out of neighborhood of A: {11960, 11958, 11959}\n",
            "The loss value is :  [0.1877630054950714] \n",
            "\n",
            "The loss value is :  [0.1877630054950714] \n",
            "\n",
            "This is group: 1571.0 and this is first subgraph \n",
            "Target node: 11959\n",
            "1-hop neighbors of A: {11960}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11961, 11962, 11963, 11964, 11965, 11966, 11967}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1572.0 and this is first subgraph \n",
            "Target node: 11962\n",
            "1-hop neighbors of A: {11971}\n",
            "2-hop neighbors of A: {11972}\n",
            "Out of neighborhood of A: {11968, 11969, 11970, 11973, 11974, 11975, 11976, 11977, 11965, 11966, 11967}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1573.0 and this is first subgraph \n",
            "Target node: 11973\n",
            "1-hop neighbors of A: {11977, 11978}\n",
            "2-hop neighbors of A: {11981}\n",
            "Out of neighborhood of A: {11974, 11976, 11979, 11980, 11982}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1574.0 and this is first subgraph \n",
            "Target node: 11976\n",
            "1-hop neighbors of A: {11977}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {11982, 11983, 11984, 11985, 11986, 11987}\n",
            "The loss value is :  [0.27376675605773926] \n",
            "\n",
            "The loss value is :  [0.27376675605773926] \n",
            "\n",
            "This is group: 1575.0 and this is first subgraph \n",
            "Target node: 11985\n",
            "1-hop neighbors of A: {11988, 11989}\n",
            "2-hop neighbors of A: {11992}\n",
            "Out of neighborhood of A: {11986, 11987, 11990, 11991, 11993}\n",
            "The loss value is :  [0.34554797410964966] \n",
            "\n",
            "The loss value is :  [0.34554797410964966] \n",
            "\n",
            "This is group: 1576.0 and this is first subgraph \n",
            "Target node: 11988\n",
            "1-hop neighbors of A: {11990}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12000, 12001, 12002, 12003, 11992, 11993, 11994, 11995, 11996, 11997, 11998, 11999}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1577.0 and this is first subgraph \n",
            "Target node: 12002\n",
            "1-hop neighbors of A: {12003}\n",
            "2-hop neighbors of A: {12008, 12004}\n",
            "Out of neighborhood of A: {12005, 12006, 12007}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1578.0 and this is first subgraph \n",
            "Target node: 12007\n",
            "1-hop neighbors of A: {12009}\n",
            "2-hop neighbors of A: {12010, 12011}\n",
            "Out of neighborhood of A: {12012, 12013}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1579.0 and this is first subgraph \n",
            "Target node: 11998\n",
            "1-hop neighbors of A: {12016, 11999}\n",
            "2-hop neighbors of A: {12017}\n",
            "Out of neighborhood of A: {12006, 12012, 12013, 12014, 12015}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1580.0 and this is first subgraph \n",
            "Target node: 12007\n",
            "1-hop neighbors of A: {12008}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12018, 12019, 12020, 12023, 12024, 12025}\n",
            "The loss value is :  [0.5214986205101013] \n",
            "\n",
            "The loss value is :  [0.5214986205101013] \n",
            "\n",
            "This is group: 1581.0 and this is first subgraph \n",
            "Target node: 12019\n",
            "1-hop neighbors of A: {12025}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12035, 12020, 12021, 12022, 12023, 12026}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1582.0 and this is first subgraph \n",
            "Target node: 12026\n",
            "1-hop neighbors of A: {12027, 12028}\n",
            "2-hop neighbors of A: {12042, 12029}\n",
            "Out of neighborhood of A: set()\n",
            "The loss value is :  [0.7951940298080444] \n",
            "\n",
            "The loss value is :  [0.7951940298080444] \n",
            "\n",
            "This is group: 1583.0 and this is first subgraph \n",
            "Target node: 12028\n",
            "1-hop neighbors of A: {12030}\n",
            "2-hop neighbors of A: {12031}\n",
            "Out of neighborhood of A: {12032, 12033, 12034, 12029}\n",
            "The loss value is :  [0.7238110899925232] \n",
            "\n",
            "The loss value is :  [0.7238110899925232] \n",
            "\n",
            "This is group: 1584.0 and this is first subgraph \n",
            "Target node: 12033\n",
            "1-hop neighbors of A: {12041, 12034}\n",
            "2-hop neighbors of A: {12038}\n",
            "Out of neighborhood of A: {12035, 12036, 12037, 12039, 12040, 12042, 12043, 12044, 12045, 12046, 12047, 12048, 12049, 12050, 12051, 12052, 12053, 12054, 12055, 12056, 12057, 12058, 12059, 12060, 12061, 12062, 12063, 12064}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1585.0 and this is first subgraph \n",
            "Target node: 12063\n",
            "1-hop neighbors of A: {12064}\n",
            "2-hop neighbors of A: {12065}\n",
            "Out of neighborhood of A: {12066, 12067, 12068, 12069, 12070, 12072, 12073, 12074, 12075, 12076, 12077, 12078, 12079, 12080, 12081, 12082, 12083, 12084, 12085, 12086, 12087, 12088, 12089, 12090, 12091, 12092, 12093, 12094, 12095, 12096}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1586.0 and this is first subgraph \n",
            "Target node: 12093\n",
            "1-hop neighbors of A: {12097}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12096, 12098, 12099, 12100, 12101, 12102, 12103, 12104, 12105, 12106, 12107, 12108, 12109, 12110, 12094}\n",
            "The loss value is :  [0.3748164176940918] \n",
            "\n",
            "The loss value is :  [0.3748164176940918] \n",
            "\n",
            "This is group: 1587.0 and this is first subgraph \n",
            "Target node: 12106\n",
            "1-hop neighbors of A: {12110}\n",
            "2-hop neighbors of A: {12115}\n",
            "Out of neighborhood of A: {12108, 12109, 12111, 12112, 12113, 12114}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1588.0 and this is first subgraph \n",
            "Target node: 12110\n",
            "1-hop neighbors of A: {12115}\n",
            "2-hop neighbors of A: {12118}\n",
            "Out of neighborhood of A: {12114, 12116, 12117, 12119, 12120}\n",
            "The loss value is :  [0.31959351897239685] \n",
            "\n",
            "The loss value is :  [0.31959351897239685] \n",
            "\n",
            "This is group: 1589.0 and this is first subgraph \n",
            "Target node: 12079\n",
            "1-hop neighbors of A: {12080}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12082, 12085, 12118, 12120, 12121}\n",
            "The loss value is :  [0.3967057466506958] \n",
            "\n",
            "The loss value is :  [0.3967057466506958] \n",
            "\n",
            "This is group: 1590.0 and this is first subgraph \n",
            "Target node: 12095\n",
            "1-hop neighbors of A: {12098}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12102, 12105, 12107, 12109, 12115, 12117, 12122, 12123}\n",
            "The loss value is :  [0.6515189409255981] \n",
            "\n",
            "The loss value is :  [0.6515189409255981] \n",
            "\n",
            "This is group: 1591.0 and this is first subgraph \n",
            "Target node: 12122\n",
            "1-hop neighbors of A: {12128, 12127}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12123, 12124, 12125, 12126}\n",
            "The loss value is :  [1.253796935081482] \n",
            "\n",
            "The loss value is :  [1.253796935081482] \n",
            "\n",
            "This is group: 1592.0 and this is first subgraph \n",
            "Target node: 12124\n",
            "1-hop neighbors of A: {12130}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12129, 12131, 12132, 12126, 12127}\n",
            "The loss value is :  [0.25961434841156006] \n",
            "\n",
            "The loss value is :  [0.25961434841156006] \n",
            "\n",
            "This is group: 1593.0 and this is first subgraph \n",
            "Target node: 12126\n",
            "1-hop neighbors of A: {12127}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12128, 12129, 12133, 12134, 12135, 12136, 12137, 12138, 12139, 12140, 12141, 12142, 12143}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1594.0 and this is first subgraph \n",
            "Target node: 12137\n",
            "1-hop neighbors of A: {12144, 12143}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12138, 12139, 12140, 12145, 12146, 12147, 12148}\n",
            "The loss value is :  [0.518990159034729] \n",
            "\n",
            "The loss value is :  [0.518990159034729] \n",
            "\n",
            "This is group: 1595.0 and this is first subgraph \n",
            "Target node: 12140\n",
            "1-hop neighbors of A: {12149}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12160, 12161, 12162, 12163, 12164, 12165, 12166, 12167, 12168, 12169, 12170, 12171, 12172, 12173, 12174, 12175, 12176, 12141, 12142, 12143, 12144, 12145, 12146, 12147, 12148, 12150, 12151, 12152, 12153, 12154, 12155, 12156, 12157, 12158, 12159}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1596.0 and this is first subgraph \n",
            "Target node: 12176\n",
            "1-hop neighbors of A: {12177, 12178}\n",
            "2-hop neighbors of A: {12179}\n",
            "Out of neighborhood of A: {12180, 12181, 12182}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1597.0 and this is first subgraph \n",
            "Target node: 12173\n",
            "1-hop neighbors of A: {12186}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12179, 12182, 12183, 12184, 12185, 12187, 12188}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1598.0 and this is first subgraph \n",
            "Target node: 12174\n",
            "1-hop neighbors of A: {12175}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12192, 12193, 12187, 12189, 12190, 12191}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1599.0 and this is first subgraph \n",
            "Target node: 12183\n",
            "1-hop neighbors of A: {12184}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12192, 12193, 12194, 12195, 12196, 12197, 12198}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1600.0 and this is first subgraph \n",
            "Target node: 12194\n",
            "1-hop neighbors of A: {12202, 12198}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12195, 12196, 12199, 12200, 12201, 12203}\n",
            "The loss value is :  [0.4021719694137573] \n",
            "\n",
            "The loss value is :  [0.4021719694137573] \n",
            "\n",
            "This is group: 1601.0 and this is first subgraph \n",
            "Target node: 12197\n",
            "1-hop neighbors of A: {12198}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12200, 12201, 12204, 12205, 12206, 12207}\n",
            "The loss value is :  [0.46945616602897644] \n",
            "\n",
            "The loss value is :  [0.46945616602897644] \n",
            "\n",
            "This is group: 1602.0 and this is first subgraph \n",
            "Target node: 12205\n",
            "1-hop neighbors of A: {12208, 12209}\n",
            "2-hop neighbors of A: {12212, 12213}\n",
            "Out of neighborhood of A: {12206, 12207, 12210, 12211, 12214, 12215, 12217, 12218, 12219, 12222, 12223, 12224, 12225, 12226, 12228, 12229, 12230, 12231, 12232, 12233, 12234, 12235, 12236, 12237, 12238, 12239, 12240, 12241, 12242, 12243, 12244, 12245, 12246, 12247, 12248, 12249, 12250, 12251, 12252, 12253, 12254, 12255, 12256, 12257, 12258, 12259, 12260, 12261, 12262, 12263}\n",
            "The loss value is :  [0.7559961080551147] \n",
            "\n",
            "The loss value is :  [0.7559961080551147] \n",
            "\n",
            "This is group: 1603.0 and this is first subgraph \n",
            "Target node: 12259\n",
            "1-hop neighbors of A: {12264}\n",
            "2-hop neighbors of A: {12265}\n",
            "Out of neighborhood of A: {12261, 12262, 12263, 12266, 12267}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1604.0 and this is first subgraph \n",
            "Target node: 12267\n",
            "1-hop neighbors of A: {12272, 12268}\n",
            "2-hop neighbors of A: {12273, 12269}\n",
            "Out of neighborhood of A: {12270, 12271}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1605.0 and this is first subgraph \n",
            "Target node: 12268\n",
            "1-hop neighbors of A: {12273}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12270, 12271, 12272, 12274, 12275, 12276, 12277}\n",
            "The loss value is :  [0.47123411297798157] \n",
            "\n",
            "The loss value is :  [0.47123411297798157] \n",
            "\n",
            "This is group: 1606.0 and this is first subgraph \n",
            "Target node: 12260\n",
            "1-hop neighbors of A: {12261}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12271, 12272, 12277, 12278, 12279, 12280, 12281}\n",
            "The loss value is :  [0.7825357913970947] \n",
            "\n",
            "The loss value is :  [0.7825357913970947] \n",
            "\n",
            "This is group: 1607.0 and this is first subgraph \n",
            "Target node: 12279\n",
            "1-hop neighbors of A: {12281}\n",
            "2-hop neighbors of A: {12282}\n",
            "Out of neighborhood of A: {12280, 12283, 12284, 12285, 12286}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1608.0 and this is first subgraph \n",
            "Target node: 12279\n",
            "1-hop neighbors of A: {12297}\n",
            "2-hop neighbors of A: {12298}\n",
            "Out of neighborhood of A: {12288, 12289, 12290, 12291, 12292, 12293, 12294, 12295, 12296, 12299, 12284, 12285, 12286, 12287}\n",
            "The loss value is :  [0.36662349104881287] \n",
            "\n",
            "The loss value is :  [0.36662349104881287] \n",
            "\n",
            "This is group: 1609.0 and this is first subgraph \n",
            "Target node: 12280\n",
            "1-hop neighbors of A: {12302}\n",
            "2-hop neighbors of A: {12304, 12303}\n",
            "Out of neighborhood of A: {12297, 12299, 12300, 12301, 12305}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1610.0 and this is first subgraph \n",
            "Target node: 12283\n",
            "1-hop neighbors of A: {12284}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12288, 12290, 12291, 12297, 12300, 12301, 12306, 12287}\n",
            "The loss value is :  [0.5135181546211243] \n",
            "\n",
            "The loss value is :  [0.5135181546211243] \n",
            "\n",
            "This is group: 1611.0 and this is first subgraph \n",
            "Target node: 12307\n",
            "1-hop neighbors of A: {12308}\n",
            "2-hop neighbors of A: {12309, 12310}\n",
            "Out of neighborhood of A: {12312, 12313, 12311}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1612.0 and this is first subgraph \n",
            "Target node: 12312\n",
            "1-hop neighbors of A: {12313}\n",
            "2-hop neighbors of A: {12314}\n",
            "Out of neighborhood of A: {12315, 12316, 12317, 12318, 12319}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1613.0 and this is first subgraph \n",
            "Target node: 12317\n",
            "1-hop neighbors of A: {12320, 12319}\n",
            "2-hop neighbors of A: {12322, 12323}\n",
            "Out of neighborhood of A: {12321, 12324, 12318}\n",
            "The loss value is :  [0.29499351978302] \n",
            "\n",
            "The loss value is :  [0.29499351978302] \n",
            "\n",
            "This is group: 1614.0 and this is first subgraph \n",
            "Target node: 12319\n",
            "1-hop neighbors of A: {12321}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12323, 12324, 12325, 12326, 12327, 12328, 12329}\n",
            "The loss value is :  [0.47616440057754517] \n",
            "\n",
            "The loss value is :  [0.47616440057754517] \n",
            "\n",
            "This is group: 1615.0 and this is first subgraph \n",
            "Target node: 12325\n",
            "1-hop neighbors of A: {12330, 12332}\n",
            "2-hop neighbors of A: {12333, 12335}\n",
            "Out of neighborhood of A: {12328, 12331, 12334}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1616.0 and this is first subgraph \n",
            "Target node: 12329\n",
            "1-hop neighbors of A: {12330}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12353, 12354, 12355, 12356, 12357, 12358, 12359, 12360, 12361, 12362, 12335, 12336, 12337, 12338, 12339, 12340, 12341, 12342, 12343, 12344, 12345, 12346, 12347, 12348, 12349, 12350, 12351}\n",
            "The loss value is :  [0.6793315410614014] \n",
            "\n",
            "The loss value is :  [0.6793315410614014] \n",
            "\n",
            "This is group: 1617.0 and this is first subgraph \n",
            "Target node: 12362\n",
            "1-hop neighbors of A: {12363}\n",
            "2-hop neighbors of A: {12364}\n",
            "Out of neighborhood of A: {12368, 12365, 12366, 12367}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1618.0 and this is first subgraph \n",
            "Target node: 12368\n",
            "1-hop neighbors of A: {12369}\n",
            "2-hop neighbors of A: {12370}\n",
            "Out of neighborhood of A: {12371, 12372, 12373}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1619.0 and this is first subgraph \n",
            "Target node: 12341\n",
            "1-hop neighbors of A: {12342}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12369, 12372, 12373, 12374, 12349, 12350}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1620.0 and this is first subgraph \n",
            "Target node: 12352\n",
            "1-hop neighbors of A: {12353}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12373, 12374, 12375, 12376, 12377}\n",
            "The loss value is :  [0.6221948862075806] \n",
            "\n",
            "The loss value is :  [0.6221948862075806] \n",
            "\n",
            "This is group: 1621.0 and this is first subgraph \n",
            "Target node: 12375\n",
            "1-hop neighbors of A: {12378, 12379}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12376, 12377, 12380, 12381, 12382, 12383}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1622.0 and this is first subgraph \n",
            "Target node: 12377\n",
            "1-hop neighbors of A: {12383}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12384, 12385, 12386, 12387, 12388, 12389, 12378}\n",
            "The loss value is :  [0.6353381276130676] \n",
            "\n",
            "The loss value is :  [0.6353381276130676] \n",
            "\n",
            "This is group: 1623.0 and this is first subgraph \n",
            "Target node: 12388\n",
            "1-hop neighbors of A: {12390, 12391}\n",
            "2-hop neighbors of A: {12394, 12395}\n",
            "Out of neighborhood of A: {12392, 12393, 12389}\n",
            "The loss value is :  [0.1040063127875328] \n",
            "\n",
            "The loss value is :  [0.1040063127875328] \n",
            "\n",
            "This is group: 1624.0 and this is first subgraph \n",
            "Target node: 12391\n",
            "1-hop neighbors of A: {12395}\n",
            "2-hop neighbors of A: {12400}\n",
            "Out of neighborhood of A: {12392, 12393, 12394, 12396, 12397, 12398, 12399, 12401}\n",
            "The loss value is :  [0.5225290060043335] \n",
            "\n",
            "The loss value is :  [0.5225290060043335] \n",
            "\n",
            "This is group: 1625.0 and this is first subgraph \n",
            "Target node: 12396\n",
            "1-hop neighbors of A: {12401}\n",
            "2-hop neighbors of A: {12404}\n",
            "Out of neighborhood of A: {12398, 12402, 12403, 12405, 12406}\n",
            "The loss value is :  [0.44078388810157776] \n",
            "\n",
            "The loss value is :  [0.44078388810157776] \n",
            "\n",
            "This is group: 1626.0 and this is first subgraph \n",
            "Target node: 12404\n",
            "1-hop neighbors of A: {12407}\n",
            "2-hop neighbors of A: {12411}\n",
            "Out of neighborhood of A: {12405, 12406, 12408, 12409, 12410}\n",
            "The loss value is :  [0.22506040334701538] \n",
            "\n",
            "The loss value is :  [0.22506040334701538] \n",
            "\n",
            "This is group: 1627.0 and this is first subgraph \n",
            "Target node: 12395\n",
            "1-hop neighbors of A: {12399}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12416, 12417, 12418, 12419, 12420, 12421, 12422, 12423, 12424, 12425, 12426, 12427, 12428, 12397, 12401, 12408, 12409, 12410, 12411, 12412, 12413, 12414, 12415}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1628.0 and this is first subgraph \n",
            "Target node: 12427\n",
            "1-hop neighbors of A: {12428, 12429, 12430}\n",
            "2-hop neighbors of A: {12432, 12433, 12431}\n",
            "Out of neighborhood of A: {12434}\n",
            "The loss value is :  [0.15972697734832764] \n",
            "\n",
            "The loss value is :  [0.15972697734832764] \n",
            "\n",
            "This is group: 1629.0 and this is first subgraph \n",
            "Target node: 12431\n",
            "1-hop neighbors of A: {12434}\n",
            "2-hop neighbors of A: {12432, 12433}\n",
            "Out of neighborhood of A: {12435, 12436, 12437, 12438}\n",
            "The loss value is :  [0.4754524827003479] \n",
            "\n",
            "The loss value is :  [0.4754524827003479] \n",
            "\n",
            "This is group: 1630.0 and this is first subgraph \n",
            "Target node: 12435\n",
            "1-hop neighbors of A: {12438}\n",
            "2-hop neighbors of A: {12440, 12441, 12439}\n",
            "Out of neighborhood of A: {12442, 12437}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1631.0 and this is first subgraph \n",
            "Target node: 12437\n",
            "1-hop neighbors of A: {12442, 12443, 12444}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12448, 12445, 12446, 12447}\n",
            "The loss value is :  [0.4890812337398529] \n",
            "\n",
            "The loss value is :  [0.4890812337398529] \n",
            "\n",
            "This is group: 1632.0 and this is first subgraph \n",
            "Target node: 12447\n",
            "1-hop neighbors of A: {12449, 12450}\n",
            "2-hop neighbors of A: {12452}\n",
            "Out of neighborhood of A: {12448, 12451}\n",
            "The loss value is :  [0.05017991364002228] \n",
            "\n",
            "The loss value is :  [0.05017991364002228] \n",
            "\n",
            "This is group: 1633.0 and this is first subgraph \n",
            "Target node: 12450\n",
            "1-hop neighbors of A: {12453}\n",
            "2-hop neighbors of A: {12454}\n",
            "Out of neighborhood of A: {12451, 12452, 12455, 12456, 12457}\n",
            "The loss value is :  [0.6778870820999146] \n",
            "\n",
            "The loss value is :  [0.6778870820999146] \n",
            "\n",
            "This is group: 1634.0 and this is first subgraph \n",
            "Target node: 12455\n",
            "1-hop neighbors of A: {12461}\n",
            "2-hop neighbors of A: {12460}\n",
            "Out of neighborhood of A: {12457, 12458, 12459, 12462, 12463, 12464, 12465, 12466, 12467}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1635.0 and this is first subgraph \n",
            "Target node: 12466\n",
            "1-hop neighbors of A: {12468, 12469}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12480, 12481, 12482, 12483, 12484, 12485, 12486, 12487, 12488, 12489, 12490, 12516, 12471, 12472, 12473, 12474, 12475, 12476, 12477, 12478, 12479}\n",
            "The loss value is :  [0.47677284479141235] \n",
            "\n",
            "The loss value is :  [0.47677284479141235] \n",
            "\n",
            "This is group: 1636.0 and this is first subgraph \n",
            "Target node: 12488\n",
            "1-hop neighbors of A: {12489}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12490, 12491, 12492, 12493, 12494, 12495, 12505}\n",
            "The loss value is :  [0.6195895671844482] \n",
            "\n",
            "The loss value is :  [0.6195895671844482] \n",
            "\n",
            "This is group: 1637.0 and this is first subgraph \n",
            "Target node: 12494\n",
            "1-hop neighbors of A: {12498, 12495}\n",
            "2-hop neighbors of A: {12496}\n",
            "Out of neighborhood of A: {12497, 12500}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1638.0 and this is first subgraph \n",
            "Target node: 12498\n",
            "1-hop neighbors of A: {12499}\n",
            "2-hop neighbors of A: {12500, 12502}\n",
            "Out of neighborhood of A: {12501, 12503}\n",
            "The loss value is :  [0.28579723834991455] \n",
            "\n",
            "The loss value is :  [0.28579723834991455] \n",
            "\n",
            "This is group: 1639.0 and this is first subgraph \n",
            "Target node: 12502\n",
            "1-hop neighbors of A: {12504}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12505, 12506, 12507, 12508, 12510}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1640.0 and this is first subgraph \n",
            "Target node: 12507\n",
            "1-hop neighbors of A: {12511}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12514, 12515, 12508, 12509, 12510}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1641.0 and this is first subgraph \n",
            "Target node: 12509\n",
            "1-hop neighbors of A: {12510}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12512, 12513, 12516, 12517, 12518, 12519, 12523, 12511}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1642.0 and this is first subgraph \n",
            "Target node: 12518\n",
            "1-hop neighbors of A: {12519}\n",
            "2-hop neighbors of A: {12520}\n",
            "Out of neighborhood of A: {12521, 12522, 12524, 12525, 12526}\n",
            "The loss value is :  [0.4223172962665558] \n",
            "\n",
            "The loss value is :  [0.4223172962665558] \n",
            "\n",
            "This is group: 1643.0 and this is first subgraph \n",
            "Target node: 12525\n",
            "1-hop neighbors of A: {12527}\n",
            "2-hop neighbors of A: {12529}\n",
            "Out of neighborhood of A: {12528, 12530, 12531, 12526}\n",
            "The loss value is :  [0.19060438871383667] \n",
            "\n",
            "The loss value is :  [0.19060438871383667] \n",
            "\n",
            "This is group: 1644.0 and this is first subgraph \n",
            "Target node: 12529\n",
            "1-hop neighbors of A: {12530}\n",
            "2-hop neighbors of A: {12532}\n",
            "Out of neighborhood of A: {12544, 12545, 12546, 12547, 12531, 12533, 12534, 12535, 12536, 12537, 12538, 12539, 12540, 12541, 12542, 12543}\n",
            "The loss value is :  [0.4837157726287842] \n",
            "\n",
            "The loss value is :  [0.4837157726287842] \n",
            "\n",
            "This is group: 1645.0 and this is first subgraph \n",
            "Target node: 12546\n",
            "1-hop neighbors of A: {12548, 12549, 12550}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12552, 12553, 12547, 12551}\n",
            "The loss value is :  [0.5777862071990967] \n",
            "\n",
            "The loss value is :  [0.5777862071990967] \n",
            "\n",
            "This is group: 1646.0 and this is first subgraph \n",
            "Target node: 12552\n",
            "1-hop neighbors of A: {12554}\n",
            "2-hop neighbors of A: {12555}\n",
            "Out of neighborhood of A: {12553, 12556}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1647.0 and this is first subgraph \n",
            "Target node: 12557\n",
            "1-hop neighbors of A: {12562, 12558}\n",
            "2-hop neighbors of A: {12559}\n",
            "Out of neighborhood of A: {12560, 12561}\n",
            "The loss value is :  [0.27192485332489014] \n",
            "\n",
            "The loss value is :  [0.27192485332489014] \n",
            "\n",
            "This is group: 1648.0 and this is first subgraph \n",
            "Target node: 12557\n",
            "1-hop neighbors of A: {12568}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12560, 12562, 12563, 12564, 12565, 12566, 12567, 12569}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1649.0 and this is first subgraph \n",
            "Target node: 12559\n",
            "1-hop neighbors of A: {12570}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12561, 12562, 12563, 12564, 12565, 12566, 12567, 12569, 12571, 12572, 12573, 12574, 12575}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1650.0 and this is first subgraph \n",
            "Target node: 12566\n",
            "1-hop neighbors of A: {12567}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12576, 12577, 12578, 12579, 12580, 12581}\n",
            "The loss value is :  [0.40253591537475586] \n",
            "\n",
            "The loss value is :  [0.40253591537475586] \n",
            "\n",
            "This is group: 1651.0 and this is first subgraph \n",
            "Target node: 12576\n",
            "1-hop neighbors of A: {12582}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12580, 12581, 12583, 12584, 12585}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1652.0 and this is first subgraph \n",
            "Target node: 12578\n",
            "1-hop neighbors of A: {12586}\n",
            "2-hop neighbors of A: {12587}\n",
            "Out of neighborhood of A: {12584, 12585, 12580, 12581}\n",
            "The loss value is :  [0.3077230453491211] \n",
            "\n",
            "The loss value is :  [0.3077230453491211] \n",
            "\n",
            "This is group: 1653.0 and this is first subgraph \n",
            "Target node: 12588\n",
            "1-hop neighbors of A: {12592, 12589}\n",
            "2-hop neighbors of A: {12593, 12590, 12591}\n",
            "Out of neighborhood of A: {12594, 12595, 12596, 12597, 12598, 12599, 12600, 12601, 12602, 12603, 12604, 12605, 12606, 12607, 12608, 12609, 12610, 12611, 12614, 12615, 12618}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1654.0 and this is first subgraph \n",
            "Target node: 12610\n",
            "1-hop neighbors of A: {12619}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12611, 12612, 12613, 12615, 12616, 12617}\n",
            "The loss value is :  [0.25701308250427246] \n",
            "\n",
            "The loss value is :  [0.25701308250427246] \n",
            "\n",
            "This is group: 1655.0 and this is first subgraph \n",
            "Target node: 12618\n",
            "1-hop neighbors of A: {12626}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12619, 12620, 12621, 12622, 12623, 12625}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1656.0 and this is first subgraph \n",
            "Target node: 12623\n",
            "1-hop neighbors of A: {12624}\n",
            "2-hop neighbors of A: {12625}\n",
            "Out of neighborhood of A: {12626, 12627, 12628, 12629}\n",
            "The loss value is :  [0.3374776840209961] \n",
            "\n",
            "The loss value is :  [0.3374776840209961] \n",
            "\n",
            "This is group: 1657.0 and this is first subgraph \n",
            "Target node: 12629\n",
            "1-hop neighbors of A: {12630}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12683, 12631, 12632, 12633, 12634, 12635, 12636, 12637, 12638, 12639, 12640, 12641, 12642, 12643, 12644, 12646, 12648, 12663}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1658.0 and this is first subgraph \n",
            "Target node: 12642\n",
            "1-hop neighbors of A: {12644, 12645}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12646, 12647, 12648, 12649, 12650, 12661}\n",
            "The loss value is :  [0.5222201347351074] \n",
            "\n",
            "The loss value is :  [0.5222201347351074] \n",
            "\n",
            "This is group: 1659.0 and this is first subgraph \n",
            "Target node: 12649\n",
            "1-hop neighbors of A: {12650, 12654, 12662}\n",
            "2-hop neighbors of A: {12651, 12660}\n",
            "Out of neighborhood of A: {12652, 12655}\n",
            "The loss value is :  [0.5022913217544556] \n",
            "\n",
            "The loss value is :  [0.5022913217544556] \n",
            "\n",
            "This is group: 1660.0 and this is first subgraph \n",
            "Target node: 12651\n",
            "1-hop neighbors of A: {12655}\n",
            "2-hop neighbors of A: {12656}\n",
            "Out of neighborhood of A: {12652, 12653, 12654, 12657, 12658}\n",
            "The loss value is :  [0.21390756964683533] \n",
            "\n",
            "The loss value is :  [0.21390756964683533] \n",
            "\n",
            "This is group: 1661.0 and this is first subgraph \n",
            "Target node: 12656\n",
            "1-hop neighbors of A: {12678}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12657, 12658, 12659, 12661, 12662, 12663, 12664}\n",
            "The loss value is :  [0.4755218029022217] \n",
            "\n",
            "The loss value is :  [0.4755218029022217] \n",
            "\n",
            "This is group: 1662.0 and this is first subgraph \n",
            "Target node: 12664\n",
            "1-hop neighbors of A: {12665}\n",
            "2-hop neighbors of A: {12666, 12675}\n",
            "Out of neighborhood of A: {12672, 12673, 12674, 12676, 12677, 12678, 12679, 12681, 12682, 12667, 12668, 12669, 12670, 12671}\n",
            "The loss value is :  [1.3059335947036743] \n",
            "\n",
            "The loss value is :  [1.3059335947036743] \n",
            "\n",
            "This is group: 1663.0 and this is first subgraph \n",
            "Target node: 12678\n",
            "1-hop neighbors of A: {12680, 12679}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12683, 12684, 12685, 12686, 12688, 12689}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1664.0 and this is first subgraph \n",
            "Target node: 12688\n",
            "1-hop neighbors of A: {12693}\n",
            "2-hop neighbors of A: {12694}\n",
            "Out of neighborhood of A: {12689, 12690, 12691, 12692}\n",
            "The loss value is :  [1.5025906562805176] \n",
            "\n",
            "The loss value is :  [1.5025906562805176] \n",
            "\n",
            "This is group: 1665.0 and this is first subgraph \n",
            "Target node: 12690\n",
            "1-hop neighbors of A: {12695}\n",
            "2-hop neighbors of A: {12697}\n",
            "Out of neighborhood of A: {12691, 12692, 12693, 12696, 12698, 12699}\n",
            "The loss value is :  [0.9963334798812866] \n",
            "\n",
            "The loss value is :  [0.9963334798812866] \n",
            "\n",
            "This is group: 1666.0 and this is first subgraph \n",
            "Target node: 12692\n",
            "1-hop neighbors of A: {12693}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12704, 12705, 12696, 12697, 12700, 12701, 12702, 12703}\n",
            "The loss value is :  [0.5090025067329407] \n",
            "\n",
            "The loss value is :  [0.5090025067329407] \n",
            "\n",
            "This is group: 1667.0 and this is first subgraph \n",
            "Target node: 12701\n",
            "1-hop neighbors of A: {12707}\n",
            "2-hop neighbors of A: {12702}\n",
            "Out of neighborhood of A: {12705, 12706}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1668.0 and this is first subgraph \n",
            "Target node: 12709\n",
            "1-hop neighbors of A: {12711}\n",
            "2-hop neighbors of A: {12712}\n",
            "Out of neighborhood of A: set()\n",
            "The loss value is :  [0.16668537259101868] \n",
            "\n",
            "The loss value is :  [0.16668537259101868] \n",
            "\n",
            "This is group: 1669.0 and this is first subgraph \n",
            "Target node: 12708\n",
            "1-hop neighbors of A: {12714, 12715}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12710, 12712, 12713, 12716, 12717, 12718}\n",
            "The loss value is :  [0.687029242515564] \n",
            "\n",
            "The loss value is :  [0.687029242515564] \n",
            "\n",
            "This is group: 1670.0 and this is first subgraph \n",
            "Target node: 12711\n",
            "1-hop neighbors of A: {12724}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12716, 12718, 12719, 12720, 12721, 12722, 12723}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1671.0 and this is first subgraph \n",
            "Target node: 12713\n",
            "1-hop neighbors of A: {12714}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12723, 12725, 12726, 12727, 12728, 12729}\n",
            "The loss value is :  [0.7342569828033447] \n",
            "\n",
            "The loss value is :  [0.7342569828033447] \n",
            "\n",
            "This is group: 1672.0 and this is first subgraph \n",
            "Target node: 12712\n",
            "1-hop neighbors of A: {12715}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12713, 12714, 12717, 12718, 12719, 12720, 12721, 12724, 12730, 12731}\n",
            "The loss value is :  [0.6554219722747803] \n",
            "\n",
            "The loss value is :  [0.6554219722747803] \n",
            "\n",
            "This is group: 1673.0 and this is first subgraph \n",
            "Target node: 12730\n",
            "1-hop neighbors of A: {12731}\n",
            "2-hop neighbors of A: {12732, 12733}\n",
            "Out of neighborhood of A: {12736, 12737, 12738, 12734, 12735}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1674.0 and this is first subgraph \n",
            "Target node: 12735\n",
            "1-hop neighbors of A: {12736}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12737, 12738, 12739, 12740, 12741, 12742, 12743, 12744, 12745, 12746, 12747, 12748, 12749}\n",
            "The loss value is :  [0.3548816740512848] \n",
            "\n",
            "The loss value is :  [0.3548816740512848] \n",
            "\n",
            "This is group: 1675.0 and this is first subgraph \n",
            "Target node: 12744\n",
            "1-hop neighbors of A: {12752}\n",
            "2-hop neighbors of A: {12751}\n",
            "Out of neighborhood of A: {12745, 12746, 12749, 12750, 12753, 12754, 12755, 12756, 12757, 12758, 12759, 12760, 12761, 12762, 12763, 12764}\n",
            "The loss value is :  [0.47766241431236267] \n",
            "\n",
            "The loss value is :  [0.47766241431236267] \n",
            "\n",
            "This is group: 1676.0 and this is first subgraph \n",
            "Target node: 12762\n",
            "1-hop neighbors of A: {12763}\n",
            "2-hop neighbors of A: {12764}\n",
            "Out of neighborhood of A: {12768, 12769, 12765, 12766, 12767}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1677.0 and this is first subgraph \n",
            "Target node: 12765\n",
            "1-hop neighbors of A: {12770, 12771, 12772}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12768, 12773, 12774}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1678.0 and this is first subgraph \n",
            "Target node: 12768\n",
            "1-hop neighbors of A: {12769}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12773, 12774, 12775, 12777, 12778, 12779, 12780, 12781, 12782, 12783, 12784, 12785, 12786, 12787, 12788, 12789, 12790, 12791, 12792, 12793, 12794, 12795, 12796, 12797}\n",
            "The loss value is :  [0.7146183848381042] \n",
            "\n",
            "The loss value is :  [0.7146183848381042] \n",
            "\n",
            "This is group: 1679.0 and this is first subgraph \n",
            "Target node: 12795\n",
            "1-hop neighbors of A: {12796}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12800, 12801, 12798, 12799}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1680.0 and this is first subgraph \n",
            "Target node: 12801\n",
            "1-hop neighbors of A: {12802, 12803}\n",
            "2-hop neighbors of A: {12804}\n",
            "Out of neighborhood of A: {12813, 12805, 12814, 12815}\n",
            "The loss value is :  [0.7264307737350464] \n",
            "\n",
            "The loss value is :  [0.7264307737350464] \n",
            "\n",
            "This is group: 1681.0 and this is first subgraph \n",
            "Target node: 12807\n",
            "1-hop neighbors of A: {12831}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12814, 12815, 12816, 12817, 12818, 12819, 12820, 12821, 12822, 12823, 12824, 12825, 12826, 12827, 12828, 12829, 12830, 12832, 12833, 12834}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1682.0 and this is first subgraph \n",
            "Target node: 12806\n",
            "1-hop neighbors of A: {12838}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12833, 12834, 12835, 12836, 12837, 12811, 12828}\n",
            "The loss value is :  [0.5433147549629211] \n",
            "\n",
            "The loss value is :  [0.5433147549629211] \n",
            "\n",
            "This is group: 1683.0 and this is first subgraph \n",
            "Target node: 12807\n",
            "1-hop neighbors of A: {12839}\n",
            "2-hop neighbors of A: {12840}\n",
            "Out of neighborhood of A: {12808, 12841, 12842, 12843, 12809}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1684.0 and this is first subgraph \n",
            "Target node: 12809\n",
            "1-hop neighbors of A: {12849}\n",
            "2-hop neighbors of A: {12850}\n",
            "Out of neighborhood of A: {12810, 12812, 12822, 12829, 12830, 12832, 12844, 12845, 12846, 12847, 12848, 12851, 12852, 12853}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1685.0 and this is first subgraph \n",
            "Target node: 12813\n",
            "1-hop neighbors of A: {12856, 12855}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12841, 12817, 12853, 12854, 12822, 12857, 12858, 12859}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1686.0 and this is first subgraph \n",
            "Target node: 12821\n",
            "1-hop neighbors of A: {12822}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12826, 12825, 12858, 12859, 12830, 12831}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1687.0 and this is first subgraph \n",
            "Target node: 12807\n",
            "1-hop neighbors of A: {12838}\n",
            "2-hop neighbors of A: {12847}\n",
            "Out of neighborhood of A: {12808, 12810, 12811, 12832, 12834, 12835, 12836, 12837, 12839, 12841, 12842, 12843, 12844, 12845, 12849, 12860, 12861, 12862}\n",
            "The loss value is :  [0.49485939741134644] \n",
            "\n",
            "The loss value is :  [0.49485939741134644] \n",
            "\n",
            "This is group: 1688.0 and this is first subgraph \n",
            "Target node: 12862\n",
            "1-hop neighbors of A: {12863}\n",
            "2-hop neighbors of A: {12865}\n",
            "Out of neighborhood of A: {12864, 12866, 12867, 12868}\n",
            "The loss value is :  [0.8560314774513245] \n",
            "\n",
            "The loss value is :  [0.8560314774513245] \n",
            "\n",
            "This is group: 1689.0 and this is first subgraph \n",
            "Target node: 12864\n",
            "1-hop neighbors of A: {12865}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12867, 12868, 12869, 12870, 12871, 12872}\n",
            "The loss value is :  [0.7456157803535461] \n",
            "\n",
            "The loss value is :  [0.7456157803535461] \n",
            "\n",
            "This is group: 1690.0 and this is first subgraph \n",
            "Target node: 12871\n",
            "1-hop neighbors of A: {12876}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12872, 12873, 12874, 12875, 12877}\n",
            "The loss value is :  [1.1313467025756836] \n",
            "\n",
            "The loss value is :  [1.1313467025756836] \n",
            "\n",
            "This is group: 1691.0 and this is first subgraph \n",
            "Target node: 12875\n",
            "1-hop neighbors of A: {12876, 12878}\n",
            "2-hop neighbors of A: {12879}\n",
            "Out of neighborhood of A: {12880, 12881, 12882, 12883}\n",
            "The loss value is :  [0.3103446364402771] \n",
            "\n",
            "The loss value is :  [0.3103446364402771] \n",
            "\n",
            "This is group: 1692.0 and this is first subgraph \n",
            "Target node: 12882\n",
            "1-hop neighbors of A: {12884, 12885, 12886}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12896, 12883, 12887, 12888, 12889, 12890, 12891, 12892, 12893, 12894, 12895}\n",
            "The loss value is :  [0.4133668839931488] \n",
            "\n",
            "The loss value is :  [0.4133668839931488] \n",
            "\n",
            "This is group: 1693.0 and this is first subgraph \n",
            "Target node: 12894\n",
            "1-hop neighbors of A: {12896, 12897}\n",
            "2-hop neighbors of A: {12900, 12901, 12902, 12903}\n",
            "Out of neighborhood of A: {12898, 12899, 12904, 12905, 12906, 12907, 12908, 12909, 12910, 12911, 12912, 12913, 12914, 12915, 12916, 12917, 12918, 12895}\n",
            "The loss value is :  [0.6985280513763428] \n",
            "\n",
            "The loss value is :  [0.6985280513763428] \n",
            "\n",
            "This is group: 1694.0 and this is first subgraph \n",
            "Target node: 12915\n",
            "1-hop neighbors of A: {12918}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12916, 12917, 12919, 12920, 12921, 12922}\n",
            "The loss value is :  [0.5450726747512817] \n",
            "\n",
            "The loss value is :  [0.5450726747512817] \n",
            "\n",
            "This is group: 1695.0 and this is first subgraph \n",
            "Target node: 12919\n",
            "1-hop neighbors of A: {12922}\n",
            "2-hop neighbors of A: {12925}\n",
            "Out of neighborhood of A: {12920, 12921, 12923, 12924, 12926}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1696.0 and this is first subgraph \n",
            "Target node: 12925\n",
            "1-hop neighbors of A: {12927}\n",
            "2-hop neighbors of A: {12930, 12931, 12932}\n",
            "Out of neighborhood of A: {12928, 12929, 12926}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1697.0 and this is first subgraph \n",
            "Target node: 12927\n",
            "1-hop neighbors of A: {12932}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12928, 12929, 12933, 12934, 12935, 12936, 12937}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1698.0 and this is first subgraph \n",
            "Target node: 12937\n",
            "1-hop neighbors of A: {12938}\n",
            "2-hop neighbors of A: {12939, 12940}\n",
            "Out of neighborhood of A: {12941, 12942, 12943}\n",
            "The loss value is :  [0.2692687213420868] \n",
            "\n",
            "The loss value is :  [0.2692687213420868] \n",
            "\n",
            "This is group: 1699.0 and this is first subgraph \n",
            "Target node: 12903\n",
            "1-hop neighbors of A: {12907}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12934, 12935, 12941, 12942, 12943, 12944, 12945, 12946, 12947, 12905, 12909, 12918, 12921, 12923, 12925}\n",
            "The loss value is :  [0.4052242934703827] \n",
            "\n",
            "The loss value is :  [0.4052242934703827] \n",
            "\n",
            "This is group: 1700.0 and this is first subgraph \n",
            "Target node: 12948\n",
            "1-hop neighbors of A: {12952, 12953, 12949}\n",
            "2-hop neighbors of A: {12950}\n",
            "Out of neighborhood of A: {12951}\n",
            "The loss value is :  [0.6861652135848999] \n",
            "\n",
            "The loss value is :  [0.6861652135848999] \n",
            "\n",
            "This is group: 1701.0 and this is first subgraph \n",
            "Target node: 12950\n",
            "1-hop neighbors of A: {12957}\n",
            "2-hop neighbors of A: {12958}\n",
            "Out of neighborhood of A: {12951, 12952, 12953, 12954, 12955, 12956}\n",
            "The loss value is :  [0.45185568928718567] \n",
            "\n",
            "The loss value is :  [0.45185568928718567] \n",
            "\n",
            "This is group: 1702.0 and this is first subgraph \n",
            "Target node: 12951\n",
            "1-hop neighbors of A: {12952}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12960, 12961, 12962, 12963, 12964, 12965, 12966, 12967, 12968, 12969, 12954, 12955, 12959}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1703.0 and this is first subgraph \n",
            "Target node: 12965\n",
            "1-hop neighbors of A: {12966}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {12970, 12971, 12972, 12973, 12975, 13040}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1704.0 and this is first subgraph \n",
            "Target node: 12973\n",
            "1-hop neighbors of A: {12974}\n",
            "2-hop neighbors of A: {12976, 12975}\n",
            "Out of neighborhood of A: {12977, 12978, 12979}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1705.0 and this is first subgraph \n",
            "Target node: 12978\n",
            "1-hop neighbors of A: {12979, 12983}\n",
            "2-hop neighbors of A: {12980, 12982, 12984, 13037}\n",
            "Out of neighborhood of A: {12992, 12993, 12994, 12995, 12996, 13031, 13002, 12981, 12985, 12986, 12987, 12988, 12989, 12990, 12991}\n",
            "The loss value is :  [0.5998644828796387] \n",
            "\n",
            "The loss value is :  [0.5998644828796387] \n",
            "\n",
            "This is group: 1706.0 and this is first subgraph \n",
            "Target node: 12996\n",
            "1-hop neighbors of A: {13001, 12997}\n",
            "2-hop neighbors of A: {12998}\n",
            "Out of neighborhood of A: {13000, 12999}\n",
            "The loss value is :  [1.2313131093978882] \n",
            "\n",
            "The loss value is :  [1.2313131093978882] \n",
            "\n",
            "This is group: 1707.0 and this is first subgraph \n",
            "Target node: 12999\n",
            "1-hop neighbors of A: {13000, 13034}\n",
            "2-hop neighbors of A: {13001}\n",
            "Out of neighborhood of A: {13002, 13003, 13004, 13005}\n",
            "The loss value is :  [0.4869402050971985] \n",
            "\n",
            "The loss value is :  [0.4869402050971985] \n",
            "\n",
            "This is group: 1708.0 and this is first subgraph \n",
            "Target node: 13004\n",
            "1-hop neighbors of A: {13009, 13005}\n",
            "2-hop neighbors of A: {13006}\n",
            "Out of neighborhood of A: {13008, 13010, 13007}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1709.0 and this is first subgraph \n",
            "Target node: 13007\n",
            "1-hop neighbors of A: {13021, 13013}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13008, 13009, 13010, 13011, 13012}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1710.0 and this is first subgraph \n",
            "Target node: 13012\n",
            "1-hop neighbors of A: {13013, 13014}\n",
            "2-hop neighbors of A: {13015, 13017, 13018}\n",
            "Out of neighborhood of A: {13016}\n",
            "The loss value is :  [0.5601725578308105] \n",
            "\n",
            "The loss value is :  [0.5601725578308105] \n",
            "\n",
            "This is group: 1711.0 and this is first subgraph \n",
            "Target node: 13015\n",
            "1-hop neighbors of A: {13016, 13020, 13022}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: set()\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1712.0 and this is first subgraph \n",
            "Target node: 13015\n",
            "1-hop neighbors of A: {13022}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13029, 13030, 13016, 13017, 13018, 13019, 13020, 13023}\n",
            "The loss value is :  [0.49882763624191284] \n",
            "\n",
            "The loss value is :  [0.49882763624191284] \n",
            "\n",
            "This is group: 1713.0 and this is first subgraph \n",
            "Target node: 13023\n",
            "1-hop neighbors of A: {13030}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13024, 13025, 13026, 13027, 13028, 13029}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1714.0 and this is first subgraph \n",
            "Target node: 13026\n",
            "1-hop neighbors of A: {13028}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13031, 13032, 13033, 13034, 13035, 13036, 13037, 13038, 13039}\n",
            "The loss value is :  [0.367351233959198] \n",
            "\n",
            "The loss value is :  [0.367351233959198] \n",
            "\n",
            "This is group: 1715.0 and this is first subgraph \n",
            "Target node: 13037\n",
            "1-hop neighbors of A: {13039}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13040, 13041, 13042, 13043, 13044, 13045, 13046, 13047, 13048, 13049, 13050}\n",
            "The loss value is :  [0.32084324955940247] \n",
            "\n",
            "The loss value is :  [0.32084324955940247] \n",
            "\n",
            "This is group: 1716.0 and this is first subgraph \n",
            "Target node: 13047\n",
            "1-hop neighbors of A: {13051}\n",
            "2-hop neighbors of A: {13052}\n",
            "Out of neighborhood of A: {13048, 13049, 13053, 13054, 13055}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1717.0 and this is first subgraph \n",
            "Target node: 13048\n",
            "1-hop neighbors of A: {13049}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13056, 13057, 13058, 13059, 13060, 13061}\n",
            "The loss value is :  [0.4382416903972626] \n",
            "\n",
            "The loss value is :  [0.4382416903972626] \n",
            "\n",
            "This is group: 1718.0 and this is first subgraph \n",
            "Target node: 13059\n",
            "1-hop neighbors of A: {13062}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13060, 13063, 13064, 13065, 13066, 13067, 13068}\n",
            "The loss value is :  [0.7719566822052002] \n",
            "\n",
            "The loss value is :  [0.7719566822052002] \n",
            "\n",
            "This is group: 1719.0 and this is first subgraph \n",
            "Target node: 13066\n",
            "1-hop neighbors of A: {13068}\n",
            "2-hop neighbors of A: {13072}\n",
            "Out of neighborhood of A: {13067, 13069, 13070, 13071}\n",
            "The loss value is :  [0.5983602404594421] \n",
            "\n",
            "The loss value is :  [0.5983602404594421] \n",
            "\n",
            "This is group: 1720.0 and this is first subgraph \n",
            "Target node: 13073\n",
            "1-hop neighbors of A: {13074, 13075, 13076}\n",
            "2-hop neighbors of A: {13077, 13078}\n",
            "Out of neighborhood of A: set()\n",
            "The loss value is :  [0.3633899688720703] \n",
            "\n",
            "The loss value is :  [0.3633899688720703] \n",
            "\n",
            "This is group: 1721.0 and this is first subgraph \n",
            "Target node: 13074\n",
            "1-hop neighbors of A: {13078, 13079}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13075, 13076, 13080, 13081, 13082, 13083}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1722.0 and this is first subgraph \n",
            "Target node: 13084\n",
            "1-hop neighbors of A: {13085, 13102, 13087}\n",
            "2-hop neighbors of A: {13089, 13099, 13101, 13086}\n",
            "Out of neighborhood of A: {13088, 13090, 13091, 13092, 13093, 13094, 13095, 13096, 13097, 13098, 13100, 13103, 13104, 13105, 13106}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1723.0 and this is first subgraph \n",
            "Target node: 13104\n",
            "1-hop neighbors of A: {13106, 13107}\n",
            "2-hop neighbors of A: {13108, 13109}\n",
            "Out of neighborhood of A: {13110, 13111}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1724.0 and this is first subgraph \n",
            "Target node: 13109\n",
            "1-hop neighbors of A: {13111}\n",
            "2-hop neighbors of A: {13112}\n",
            "Out of neighborhood of A: {13113, 13114, 13110}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1725.0 and this is first subgraph \n",
            "Target node: 13115\n",
            "1-hop neighbors of A: {13116, 13117}\n",
            "2-hop neighbors of A: {13118}\n",
            "Out of neighborhood of A: {13120, 13119}\n",
            "The loss value is :  [0.32507503032684326] \n",
            "\n",
            "The loss value is :  [0.32507503032684326] \n",
            "\n",
            "This is group: 1726.0 and this is first subgraph \n",
            "Target node: 13115\n",
            "1-hop neighbors of A: {13121}\n",
            "2-hop neighbors of A: {13122}\n",
            "Out of neighborhood of A: {13120, 13123, 13124, 13119}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1727.0 and this is first subgraph \n",
            "Target node: 13120\n",
            "1-hop neighbors of A: {13121}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13123, 13124, 13125, 13126, 13127, 13128}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1728.0 and this is first subgraph \n",
            "Target node: 13126\n",
            "1-hop neighbors of A: {13128, 13129}\n",
            "2-hop neighbors of A: {13133}\n",
            "Out of neighborhood of A: {13130, 13131, 13132, 13127}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1729.0 and this is first subgraph \n",
            "Target node: 13134\n",
            "1-hop neighbors of A: {13135}\n",
            "2-hop neighbors of A: {13136, 13137}\n",
            "Out of neighborhood of A: {13138, 13139, 13140}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1730.0 and this is first subgraph \n",
            "Target node: 13139\n",
            "1-hop neighbors of A: {13140, 13143}\n",
            "2-hop neighbors of A: {13141, 13142}\n",
            "Out of neighborhood of A: set()\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1731.0 and this is first subgraph \n",
            "Target node: 13143\n",
            "1-hop neighbors of A: {13144, 13145}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13153, 13155, 13160, 13146, 13147, 13148}\n",
            "The loss value is :  [0.5873939990997314] \n",
            "\n",
            "The loss value is :  [0.5873939990997314] \n",
            "\n",
            "This is group: 1732.0 and this is first subgraph \n",
            "Target node: 13148\n",
            "1-hop neighbors of A: {13163}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13156, 13196, 13149, 13150, 13151}\n",
            "The loss value is :  [1.3490142822265625] \n",
            "\n",
            "The loss value is :  [1.3490142822265625] \n",
            "\n",
            "This is group: 1733.0 and this is first subgraph \n",
            "Target node: 13150\n",
            "1-hop neighbors of A: {13155}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13152, 13153, 13154, 13151}\n",
            "The loss value is :  [0.7192723155021667] \n",
            "\n",
            "The loss value is :  [0.7192723155021667] \n",
            "\n",
            "This is group: 1734.0 and this is first subgraph \n",
            "Target node: 13154\n",
            "1-hop neighbors of A: {13155, 13165}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13156, 13157, 13158, 13161, 13164, 13183}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1735.0 and this is first subgraph \n",
            "Target node: 13158\n",
            "1-hop neighbors of A: {13159}\n",
            "2-hop neighbors of A: {13160}\n",
            "Out of neighborhood of A: {13188, 13161, 13162, 13163, 13164}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1736.0 and this is first subgraph \n",
            "Target node: 13163\n",
            "1-hop neighbors of A: {13164}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13185, 13166, 13167, 13168, 13173, 13175, 13180}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1737.0 and this is first subgraph \n",
            "Target node: 13168\n",
            "1-hop neighbors of A: {13185}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13192, 13169, 13170, 13171, 13175, 13176}\n",
            "The loss value is :  [0.5751068592071533] \n",
            "\n",
            "The loss value is :  [0.5751068592071533] \n",
            "\n",
            "This is group: 1738.0 and this is first subgraph \n",
            "Target node: 13171\n",
            "1-hop neighbors of A: {13172}\n",
            "2-hop neighbors of A: {13173, 13174}\n",
            "Out of neighborhood of A: {13187, 13175}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1739.0 and this is first subgraph \n",
            "Target node: 13176\n",
            "1-hop neighbors of A: {13186}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13177, 13178, 13181, 13182}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1740.0 and this is first subgraph \n",
            "Target node: 13178\n",
            "1-hop neighbors of A: {13179}\n",
            "2-hop neighbors of A: {13180}\n",
            "Out of neighborhood of A: {13184, 13185, 13186, 13188, 13189, 13190, 13191, 13192, 13193, 13200, 13181, 13182, 13183}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1741.0 and this is first subgraph \n",
            "Target node: 13192\n",
            "1-hop neighbors of A: {13194, 13195}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13196, 13197, 13198, 13199, 13200, 13201, 13202, 13203, 13205, 13206, 13207, 13208, 13209, 13210, 13211, 13212, 13213, 13214}\n",
            "The loss value is :  [0.37541866302490234] \n",
            "\n",
            "The loss value is :  [0.37541866302490234] \n",
            "\n",
            "This is group: 1742.0 and this is first subgraph \n",
            "Target node: 13208\n",
            "1-hop neighbors of A: {13209}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13216, 13213, 13214, 13215}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1743.0 and this is first subgraph \n",
            "Target node: 13215\n",
            "1-hop neighbors of A: {13216, 13217}\n",
            "2-hop neighbors of A: {13218, 13219, 13220}\n",
            "Out of neighborhood of A: set()\n",
            "The loss value is :  [0.8449453711509705] \n",
            "\n",
            "The loss value is :  [0.8449453711509705] \n",
            "\n",
            "This is group: 1744.0 and this is first subgraph \n",
            "Target node: 13217\n",
            "1-hop neighbors of A: {13220}\n",
            "2-hop neighbors of A: {13223}\n",
            "Out of neighborhood of A: {13219, 13221, 13222, 13224, 13226, 13227}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1745.0 and this is first subgraph \n",
            "Target node: 13225\n",
            "1-hop neighbors of A: {13228, 13230}\n",
            "2-hop neighbors of A: {13227, 13229}\n",
            "Out of neighborhood of A: {13226, 13231, 13232, 13233, 13234, 13235, 13236, 13237, 13238, 13239, 13240, 13241, 13242}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1746.0 and this is first subgraph \n",
            "Target node: 13239\n",
            "1-hop neighbors of A: {13243}\n",
            "2-hop neighbors of A: {13244}\n",
            "Out of neighborhood of A: {13248, 13241, 13245, 13246, 13247}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1747.0 and this is first subgraph \n",
            "Target node: 13246\n",
            "1-hop neighbors of A: {13248}\n",
            "2-hop neighbors of A: {13252, 13253}\n",
            "Out of neighborhood of A: {13249, 13250, 13251, 13247}\n",
            "The loss value is :  [0.9919193983078003] \n",
            "\n",
            "The loss value is :  [0.9919193983078003] \n",
            "\n",
            "This is group: 1748.0 and this is first subgraph \n",
            "Target node: 13248\n",
            "1-hop neighbors of A: {13253}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13254, 13255, 13256, 13257, 13258}\n",
            "The loss value is :  [0.8420804738998413] \n",
            "\n",
            "The loss value is :  [0.8420804738998413] \n",
            "\n",
            "This is group: 1749.0 and this is first subgraph \n",
            "Target node: 13255\n",
            "1-hop neighbors of A: {13259}\n",
            "2-hop neighbors of A: {13260, 13261}\n",
            "Out of neighborhood of A: {13257, 13258, 13262, 13263, 13264, 13265, 13266, 13267, 13268, 13269, 13270, 13271, 13272, 13273, 13274}\n",
            "The loss value is :  [0.514738917350769] \n",
            "\n",
            "The loss value is :  [0.514738917350769] \n",
            "\n",
            "This is group: 1750.0 and this is first subgraph \n",
            "Target node: 13275\n",
            "1-hop neighbors of A: {13276, 13277}\n",
            "2-hop neighbors of A: {13278, 13279}\n",
            "Out of neighborhood of A: set()\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1751.0 and this is first subgraph \n",
            "Target node: 13277\n",
            "1-hop neighbors of A: {13279}\n",
            "2-hop neighbors of A: {13281, 13282, 13283}\n",
            "Out of neighborhood of A: {13280, 13284, 13285, 13286, 13287, 13288, 13289, 13290, 13291, 13292, 13293, 13294, 13295, 13278}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1752.0 and this is first subgraph \n",
            "Target node: 13286\n",
            "1-hop neighbors of A: {13296}\n",
            "2-hop neighbors of A: {13297}\n",
            "Out of neighborhood of A: {13293, 13295, 13298, 13299, 13300, 13301, 13302}\n",
            "The loss value is :  [0.6375793218612671] \n",
            "\n",
            "The loss value is :  [0.6375793218612671] \n",
            "\n",
            "This is group: 1753.0 and this is first subgraph \n",
            "Target node: 13300\n",
            "1-hop neighbors of A: {13302, 13303}\n",
            "2-hop neighbors of A: {13304, 13305, 13306}\n",
            "Out of neighborhood of A: set()\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1754.0 and this is first subgraph \n",
            "Target node: 13305\n",
            "1-hop neighbors of A: {13306}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13312, 13353, 13308, 13309, 13310}\n",
            "The loss value is :  [0.6568071842193604] \n",
            "\n",
            "The loss value is :  [0.6568071842193604] \n",
            "\n",
            "This is group: 1755.0 and this is first subgraph \n",
            "Target node: 13309\n",
            "1-hop neighbors of A: {13313, 13314, 13310}\n",
            "2-hop neighbors of A: {13315, 13317, 13311}\n",
            "Out of neighborhood of A: {13312}\n",
            "The loss value is :  [0.3300324082374573] \n",
            "\n",
            "The loss value is :  [0.3300324082374573] \n",
            "\n",
            "This is group: 1756.0 and this is first subgraph \n",
            "Target node: 13314\n",
            "1-hop neighbors of A: {13315}\n",
            "2-hop neighbors of A: {13316}\n",
            "Out of neighborhood of A: {13321, 13317, 13318, 13319}\n",
            "The loss value is :  [0.5600342750549316] \n",
            "\n",
            "The loss value is :  [0.5600342750549316] \n",
            "\n",
            "This is group: 1757.0 and this is first subgraph \n",
            "Target node: 13318\n",
            "1-hop neighbors of A: {13322, 13323}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13320, 13321, 13326, 13319}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1758.0 and this is first subgraph \n",
            "Target node: 13323\n",
            "1-hop neighbors of A: {13324}\n",
            "2-hop neighbors of A: {13325}\n",
            "Out of neighborhood of A: {13328, 13329, 13326, 13327}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1759.0 and this is first subgraph \n",
            "Target node: 13328\n",
            "1-hop neighbors of A: {13329}\n",
            "2-hop neighbors of A: {13330}\n",
            "Out of neighborhood of A: {13331, 13332, 13342, 13335}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1760.0 and this is first subgraph \n",
            "Target node: 13332\n",
            "1-hop neighbors of A: {13333}\n",
            "2-hop neighbors of A: {13337, 13334}\n",
            "Out of neighborhood of A: {13336, 13339, 13335}\n",
            "The loss value is :  [0.34701699018478394] \n",
            "\n",
            "The loss value is :  [0.34701699018478394] \n",
            "\n",
            "This is group: 1761.0 and this is first subgraph \n",
            "Target node: 13337\n",
            "1-hop neighbors of A: {13338}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13344, 13347, 13339, 13340, 13341, 13342, 13343}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1762.0 and this is first subgraph \n",
            "Target node: 13343\n",
            "1-hop neighbors of A: {13344}\n",
            "2-hop neighbors of A: {13345, 13348}\n",
            "Out of neighborhood of A: {13352, 13346, 13351}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1763.0 and this is first subgraph \n",
            "Target node: 13346\n",
            "1-hop neighbors of A: {13347}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13348, 13349, 13350, 13353, 13354, 13355, 13356, 13357, 13358, 13359, 13360}\n",
            "The loss value is :  [0.5822521448135376] \n",
            "\n",
            "The loss value is :  [0.5822521448135376] \n",
            "\n",
            "This is group: 1764.0 and this is first subgraph \n",
            "Target node: 13361\n",
            "1-hop neighbors of A: {13362, 13363}\n",
            "2-hop neighbors of A: {13364, 13365, 13366}\n",
            "Out of neighborhood of A: {13367}\n",
            "The loss value is :  [0.6100369095802307] \n",
            "\n",
            "The loss value is :  [0.6100369095802307] \n",
            "\n",
            "This is group: 1765.0 and this is first subgraph \n",
            "Target node: 13364\n",
            "1-hop neighbors of A: {13368}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13365, 13366, 13369, 13370, 13371}\n",
            "The loss value is :  [0.47604644298553467] \n",
            "\n",
            "The loss value is :  [0.47604644298553467] \n",
            "\n",
            "This is group: 1766.0 and this is first subgraph \n",
            "Target node: 13370\n",
            "1-hop neighbors of A: {13371}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13376, 13377, 13372, 13373, 13374, 13375}\n",
            "The loss value is :  [0.459816575050354] \n",
            "\n",
            "The loss value is :  [0.459816575050354] \n",
            "\n",
            "This is group: 1767.0 and this is first subgraph \n",
            "Target node: 13374\n",
            "1-hop neighbors of A: {13377}\n",
            "2-hop neighbors of A: {13380}\n",
            "Out of neighborhood of A: {13376, 13378, 13379, 13375}\n",
            "The loss value is :  [0.1551642119884491] \n",
            "\n",
            "The loss value is :  [0.1551642119884491] \n",
            "\n",
            "This is group: 1768.0 and this is first subgraph \n",
            "Target node: 13379\n",
            "1-hop neighbors of A: {13380}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13381, 13382, 13383, 13384, 13385, 13386}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1769.0 and this is first subgraph \n",
            "Target node: 13383\n",
            "1-hop neighbors of A: {13387, 13388}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13385, 13389, 13390}\n",
            "The loss value is :  [0.5933144092559814] \n",
            "\n",
            "The loss value is :  [0.5933144092559814] \n",
            "\n",
            "This is group: 1770.0 and this is first subgraph \n",
            "Target node: 13385\n",
            "1-hop neighbors of A: {13391}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13392, 13393, 13394, 13395, 13396}\n",
            "The loss value is :  [0.4429572820663452] \n",
            "\n",
            "The loss value is :  [0.4429572820663452] \n",
            "\n",
            "This is group: 1771.0 and this is first subgraph \n",
            "Target node: 13393\n",
            "1-hop neighbors of A: {13416}\n",
            "2-hop neighbors of A: {13417}\n",
            "Out of neighborhood of A: {13394, 13396, 13397, 13398, 13399, 13400, 13401, 13402, 13403, 13404, 13405, 13406, 13407, 13408, 13409, 13410, 13411, 13412, 13413, 13414, 13415, 13418, 13419, 13420, 13421, 13422, 13423, 13424, 13425}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1772.0 and this is first subgraph \n",
            "Target node: 13407\n",
            "1-hop neighbors of A: {13430}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13408, 13411, 13424, 13425, 13426, 13427, 13428}\n",
            "The loss value is :  [0.5523034334182739] \n",
            "\n",
            "The loss value is :  [0.5523034334182739] \n",
            "\n",
            "This is group: 1773.0 and this is first subgraph \n",
            "Target node: 13429\n",
            "1-hop neighbors of A: {13432, 13431}\n",
            "2-hop neighbors of A: {13433}\n",
            "Out of neighborhood of A: {13434, 13435, 13436}\n",
            "The loss value is :  [0.40977758169174194] \n",
            "\n",
            "The loss value is :  [0.40977758169174194] \n",
            "\n",
            "This is group: 1774.0 and this is first subgraph \n",
            "Target node: 13435\n",
            "1-hop neighbors of A: {13436}\n",
            "2-hop neighbors of A: {13437}\n",
            "Out of neighborhood of A: {13440, 13441, 13438, 13439}\n",
            "The loss value is :  [0.32846587896347046] \n",
            "\n",
            "The loss value is :  [0.32846587896347046] \n",
            "\n",
            "This is group: 1775.0 and this is first subgraph \n",
            "Target node: 13441\n",
            "1-hop neighbors of A: {13442}\n",
            "2-hop neighbors of A: {13443}\n",
            "Out of neighborhood of A: {13444, 13445, 13446}\n",
            "The loss value is :  [0.9076851606369019] \n",
            "\n",
            "The loss value is :  [0.9076851606369019] \n",
            "\n",
            "This is group: 1776.0 and this is first subgraph \n",
            "Target node: 13395\n",
            "1-hop neighbors of A: {13396}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13445, 13446, 13447, 13448, 13449, 13450, 13451, 13452, 13453, 13454, 13455, 13456, 13400, 13401, 13403, 13404, 13406, 13407, 13408, 13409, 13412, 13420, 13421, 13427, 13429, 13430}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1777.0 and this is first subgraph \n",
            "Target node: 13451\n",
            "1-hop neighbors of A: {13452}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13453, 13456, 13457, 13458, 13459, 13460}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1778.0 and this is first subgraph \n",
            "Target node: 13458\n",
            "1-hop neighbors of A: {13463}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13459, 13461, 13462, 13464, 13465, 13466, 13468}\n",
            "The loss value is :  [0.40765589475631714] \n",
            "\n",
            "The loss value is :  [0.40765589475631714] \n",
            "\n",
            "This is group: 1779.0 and this is first subgraph \n",
            "Target node: 13464\n",
            "1-hop neighbors of A: {13468}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13472, 13473, 13474, 13465, 13467, 13469, 13470, 13471}\n",
            "The loss value is :  [0.4363589286804199] \n",
            "\n",
            "The loss value is :  [0.4363589286804199] \n",
            "\n",
            "This is group: 1780.0 and this is first subgraph \n",
            "Target node: 13466\n",
            "1-hop neighbors of A: {13475, 13476}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13468, 13470, 13471, 13477, 13478, 13479, 13480, 13481, 13482, 13483, 13484, 13485, 13486, 13487, 13488, 13489, 13490, 13491, 13492, 13493, 13494, 13495}\n",
            "The loss value is :  [0.5991610884666443] \n",
            "\n",
            "The loss value is :  [0.5991610884666443] \n",
            "\n",
            "This is group: 1781.0 and this is first subgraph \n",
            "Target node: 13467\n",
            "1-hop neighbors of A: {13468}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13472, 13473, 13495, 13496, 13497, 13498}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1782.0 and this is first subgraph \n",
            "Target node: 13467\n",
            "1-hop neighbors of A: {13476}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13473, 13501, 13499, 13500, 13469}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1783.0 and this is first subgraph \n",
            "Target node: 13500\n",
            "1-hop neighbors of A: {13502}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13501, 13503, 13504, 13505, 13506, 13507, 13508, 13509, 13510, 13511, 13512, 13513, 13514, 13515, 13516, 13517, 13518, 13519, 13520, 13521, 13522, 13523, 13524, 13525, 13526, 13527, 13528, 13529, 13530, 13531, 13532, 13533, 13534, 13535, 13536, 13537, 13538, 13539, 13540}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1784.0 and this is first subgraph \n",
            "Target node: 13538\n",
            "1-hop neighbors of A: {13539}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13568, 13569, 13570, 13571, 13541, 13542, 13543, 13544, 13545, 13546, 13547, 13548, 13549, 13550, 13551, 13552, 13553, 13554, 13555, 13556, 13557, 13558, 13559, 13560, 13561, 13562, 13563, 13564, 13565, 13566, 13567}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1785.0 and this is first subgraph \n",
            "Target node: 13541\n",
            "1-hop neighbors of A: {13576}\n",
            "2-hop neighbors of A: {13577}\n",
            "Out of neighborhood of A: {13568, 13569, 13571, 13572, 13573, 13574, 13575, 13578, 13579, 13580, 13581, 13582, 13583, 13584, 13585, 13586, 13587, 13588, 13542, 13545, 13546, 13547, 13551, 13552, 13553}\n",
            "The loss value is :  [0.5043473243713379] \n",
            "\n",
            "The loss value is :  [0.5043473243713379] \n",
            "\n",
            "This is group: 1786.0 and this is first subgraph \n",
            "Target node: 13585\n",
            "1-hop neighbors of A: {13589}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13608, 13586, 13590, 13591, 13592, 13593, 13599}\n",
            "The loss value is :  [0.43301284313201904] \n",
            "\n",
            "The loss value is :  [0.43301284313201904] \n",
            "\n",
            "This is group: 1787.0 and this is first subgraph \n",
            "Target node: 13592\n",
            "1-hop neighbors of A: {13601}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13600, 13593, 13594, 13595, 13597}\n",
            "The loss value is :  [0.8558772802352905] \n",
            "\n",
            "The loss value is :  [0.8558772802352905] \n",
            "\n",
            "This is group: 1788.0 and this is first subgraph \n",
            "Target node: 13595\n",
            "1-hop neighbors of A: {13608, 13596}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13600, 13602, 13597, 13598}\n",
            "The loss value is :  [0.7075609564781189] \n",
            "\n",
            "The loss value is :  [0.7075609564781189] \n",
            "\n",
            "This is group: 1789.0 and this is first subgraph \n",
            "Target node: 13601\n",
            "1-hop neighbors of A: {13609}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13602, 13603, 13604, 13605, 13607}\n",
            "The loss value is :  [0.5881454944610596] \n",
            "\n",
            "The loss value is :  [0.5881454944610596] \n",
            "\n",
            "This is group: 1790.0 and this is first subgraph \n",
            "Target node: 13605\n",
            "1-hop neighbors of A: {13606}\n",
            "2-hop neighbors of A: {13607}\n",
            "Out of neighborhood of A: {13609, 13610, 13611, 13612, 13613}\n",
            "The loss value is :  [1.342825174331665] \n",
            "\n",
            "The loss value is :  [1.342825174331665] \n",
            "\n",
            "This is group: 1791.0 and this is first subgraph \n",
            "Target node: 13613\n",
            "1-hop neighbors of A: {13618, 13614}\n",
            "2-hop neighbors of A: {13617, 13615}\n",
            "Out of neighborhood of A: {13616}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1792.0 and this is first subgraph \n",
            "Target node: 13617\n",
            "1-hop neighbors of A: {13618}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13619, 13620, 13621, 13622, 13623, 13624}\n",
            "The loss value is :  [0.461683452129364] \n",
            "\n",
            "The loss value is :  [0.461683452129364] \n",
            "\n",
            "This is group: 1793.0 and this is first subgraph \n",
            "Target node: 13623\n",
            "1-hop neighbors of A: {13624, 13625}\n",
            "2-hop neighbors of A: {13626}\n",
            "Out of neighborhood of A: {13632, 13633, 13634, 13635, 13636, 13627, 13628, 13629, 13630, 13631}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1794.0 and this is first subgraph \n",
            "Target node: 13631\n",
            "1-hop neighbors of A: {13636}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13632, 13633, 13637, 13638, 13639, 13640, 13641, 13642, 13643}\n",
            "The loss value is :  [0.5574827194213867] \n",
            "\n",
            "The loss value is :  [0.5574827194213867] \n",
            "\n",
            "This is group: 1795.0 and this is first subgraph \n",
            "Target node: 13642\n",
            "1-hop neighbors of A: {13643}\n",
            "2-hop neighbors of A: {13644}\n",
            "Out of neighborhood of A: {13645}\n",
            "The loss value is :  [0.052409637719392776] \n",
            "\n",
            "The loss value is :  [0.052409637719392776] \n",
            "\n",
            "This is group: 1796.0 and this is first subgraph \n",
            "Target node: 13641\n",
            "1-hop neighbors of A: {13646, 13647}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13643, 13644, 13645, 13648, 13649, 13650}\n",
            "The loss value is :  [0.574062705039978] \n",
            "\n",
            "The loss value is :  [0.574062705039978] \n",
            "\n",
            "This is group: 1797.0 and this is first subgraph \n",
            "Target node: 13645\n",
            "1-hop neighbors of A: {13646}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13649, 13651, 13652, 13653, 13654, 13655, 13656, 13657, 13658, 13659, 13660, 13661, 13662, 13663, 13664, 13665, 13666, 13667, 13668, 13669, 13670, 13671, 13672, 13673, 13674, 13675, 13676, 13677, 13678, 13679, 13680}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1798.0 and this is first subgraph \n",
            "Target node: 13678\n",
            "1-hop neighbors of A: {13681}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13682, 13679}\n",
            "The loss value is :  [1.1921358108520508] \n",
            "\n",
            "The loss value is :  [1.1921358108520508] \n",
            "\n",
            "This is group: 1799.0 and this is first subgraph \n",
            "Target node: 13679\n",
            "1-hop neighbors of A: {13682, 13683}\n",
            "2-hop neighbors of A: {13687, 13686}\n",
            "Out of neighborhood of A: {13680, 13681, 13684, 13685}\n",
            "The loss value is :  [0.5680406093597412] \n",
            "\n",
            "The loss value is :  [0.5680406093597412] \n",
            "\n",
            "This is group: 1800.0 and this is first subgraph \n",
            "Target node: 13684\n",
            "1-hop neighbors of A: {13688}\n",
            "2-hop neighbors of A: {13690}\n",
            "Out of neighborhood of A: {13689, 13691, 13692, 13686}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1801.0 and this is first subgraph \n",
            "Target node: 13662\n",
            "1-hop neighbors of A: {13669}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13691, 13692, 13693, 13694, 13695}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1802.0 and this is first subgraph \n",
            "Target node: 13662\n",
            "1-hop neighbors of A: {13669}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13665, 13673, 13685, 13687, 13688, 13689}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1803.0 and this is first subgraph \n",
            "Target node: 13687\n",
            "1-hop neighbors of A: {13689}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13696, 13697, 13698, 13699, 13700, 13701, 13702}\n",
            "The loss value is :  [0.7138785123825073] \n",
            "\n",
            "The loss value is :  [0.7138785123825073] \n",
            "\n",
            "This is group: 1804.0 and this is first subgraph \n",
            "Target node: 13700\n",
            "1-hop neighbors of A: {13701, 13703}\n",
            "2-hop neighbors of A: {13702, 13704, 13705}\n",
            "Out of neighborhood of A: {13706, 13707, 13708, 13709, 13710, 13711, 13712, 13713, 13714, 13715, 13716, 13717, 13718, 13719, 13720, 13721, 13722, 13723, 13724, 13725}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1805.0 and this is first subgraph \n",
            "Target node: 13712\n",
            "1-hop neighbors of A: {13713}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13728, 13729, 13719, 13720, 13724, 13725, 13726, 13727}\n",
            "The loss value is :  [0.253366619348526] \n",
            "\n",
            "The loss value is :  [0.253366619348526] \n",
            "\n",
            "This is group: 1806.0 and this is first subgraph \n",
            "Target node: 13719\n",
            "1-hop neighbors of A: {13720}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13730, 13731, 13732, 13733, 13734, 13735}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1807.0 and this is first subgraph \n",
            "Target node: 13733\n",
            "1-hop neighbors of A: {13735}\n",
            "2-hop neighbors of A: {13734}\n",
            "Out of neighborhood of A: {13736, 13737, 13738, 13739, 13740}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1808.0 and this is first subgraph \n",
            "Target node: 13736\n",
            "1-hop neighbors of A: {13742}\n",
            "2-hop neighbors of A: {13744}\n",
            "Out of neighborhood of A: {13737, 13740, 13741, 13743, 13745, 13746}\n",
            "The loss value is :  [0.7557320594787598] \n",
            "\n",
            "The loss value is :  [0.7557320594787598] \n",
            "\n",
            "This is group: 1809.0 and this is first subgraph \n",
            "Target node: 13743\n",
            "1-hop neighbors of A: {13746, 13747}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13748, 13749, 13750, 13751, 13752}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1810.0 and this is first subgraph \n",
            "Target node: 13750\n",
            "1-hop neighbors of A: {13752}\n",
            "2-hop neighbors of A: {13753}\n",
            "Out of neighborhood of A: {13760, 13761, 13762, 13763, 13764, 13765, 13751, 13754, 13755, 13756, 13757, 13758, 13759}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1811.0 and this is first subgraph \n",
            "Target node: 13764\n",
            "1-hop neighbors of A: {13767}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13765, 13766, 13768, 13769, 13770, 13771}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1812.0 and this is first subgraph \n",
            "Target node: 13769\n",
            "1-hop neighbors of A: {13771}\n",
            "2-hop neighbors of A: {13772}\n",
            "Out of neighborhood of A: {13773, 13774, 13775, 13776, 13777}\n",
            "The loss value is :  [0.37200045585632324] \n",
            "\n",
            "The loss value is :  [0.37200045585632324] \n",
            "\n",
            "This is group: 1813.0 and this is first subgraph \n",
            "Target node: 13775\n",
            "1-hop neighbors of A: {13777, 13779}\n",
            "2-hop neighbors of A: {13778, 13780, 13781}\n",
            "Out of neighborhood of A: {13782, 13783}\n",
            "The loss value is :  [0.7579895257949829] \n",
            "\n",
            "The loss value is :  [0.7579895257949829] \n",
            "\n",
            "This is group: 1814.0 and this is first subgraph \n",
            "Target node: 13766\n",
            "1-hop neighbors of A: {13767}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13776, 13777, 13782, 13783, 13784, 13785, 13786}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1815.0 and this is first subgraph \n",
            "Target node: 13785\n",
            "1-hop neighbors of A: {13786}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13787, 13788, 13789, 13790, 13791}\n",
            "The loss value is :  [0.44241631031036377] \n",
            "\n",
            "The loss value is :  [0.44241631031036377] \n",
            "\n",
            "This is group: 1816.0 and this is first subgraph \n",
            "Target node: 13789\n",
            "1-hop neighbors of A: {13792}\n",
            "2-hop neighbors of A: {13796}\n",
            "Out of neighborhood of A: {13793, 13794, 13795, 13790, 13791}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1817.0 and this is first subgraph \n",
            "Target node: 13793\n",
            "1-hop neighbors of A: {13794, 13797}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13795, 13796, 13798, 13799, 13800, 13801, 13802, 13803, 13804, 13805, 13806, 13807, 13808, 13809, 13810, 13811, 13812, 13813, 13814}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1818.0 and this is first subgraph \n",
            "Target node: 13812\n",
            "1-hop neighbors of A: {13815}\n",
            "2-hop neighbors of A: {13816, 13821, 13822}\n",
            "Out of neighborhood of A: {13824, 13826, 13813, 13817, 13818, 13819, 13820, 13823}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1819.0 and this is first subgraph \n",
            "Target node: 13823\n",
            "1-hop neighbors of A: {13831}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13824, 13826, 13827, 13828, 13829, 13830}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1820.0 and this is first subgraph \n",
            "Target node: 13825\n",
            "1-hop neighbors of A: {13826, 13828}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13831, 13832, 13833, 13834, 13835, 13836}\n",
            "The loss value is :  [0.20408722758293152] \n",
            "\n",
            "The loss value is :  [0.20408722758293152] \n",
            "\n",
            "This is group: 1821.0 and this is first subgraph \n",
            "Target node: 13834\n",
            "1-hop neighbors of A: {13837, 13838}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13835, 13839, 13840, 13841, 13845, 13846}\n",
            "The loss value is :  [0.48849958181381226] \n",
            "\n",
            "The loss value is :  [0.48849958181381226] \n",
            "\n",
            "This is group: 1822.0 and this is first subgraph \n",
            "Target node: 13840\n",
            "1-hop neighbors of A: {13846}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13841, 13842, 13843, 13844, 13845, 13854}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1823.0 and this is first subgraph \n",
            "Target node: 13845\n",
            "1-hop neighbors of A: {13898}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13846, 13847, 13848, 13849, 13850, 13853}\n",
            "The loss value is :  [0.4737640619277954] \n",
            "\n",
            "The loss value is :  [0.4737640619277954] \n",
            "\n",
            "This is group: 1824.0 and this is first subgraph \n",
            "Target node: 13849\n",
            "1-hop neighbors of A: {13850, 13894}\n",
            "2-hop neighbors of A: {13851}\n",
            "Out of neighborhood of A: {13856, 13857, 13890, 13858, 13859, 13860, 13861, 13863, 13852, 13853, 13854, 13855}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1825.0 and this is first subgraph \n",
            "Target node: 13859\n",
            "1-hop neighbors of A: {13860, 13862}\n",
            "2-hop neighbors of A: {13881, 13861}\n",
            "Out of neighborhood of A: set()\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1826.0 and this is first subgraph \n",
            "Target node: 13861\n",
            "1-hop neighbors of A: {13885}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13863, 13864, 13865, 13866, 13867, 13868, 13869, 13870, 13871, 13872, 13875}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1827.0 and this is first subgraph \n",
            "Target node: 13869\n",
            "1-hop neighbors of A: {13870}\n",
            "2-hop neighbors of A: {13902}\n",
            "Out of neighborhood of A: {13872, 13873, 13874, 13875, 13876, 13880}\n",
            "The loss value is :  [0.45508554577827454] \n",
            "\n",
            "The loss value is :  [0.45508554577827454] \n",
            "\n",
            "This is group: 1828.0 and this is first subgraph \n",
            "Target node: 13875\n",
            "1-hop neighbors of A: {13880}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13888, 13889, 13890, 13891, 13892, 13893, 13894, 13895, 13896, 13897, 13898, 13899, 13876, 13877, 13878, 13879, 13881, 13882, 13883, 13884, 13885, 13886, 13887}\n",
            "The loss value is :  [1.1936722993850708] \n",
            "\n",
            "The loss value is :  [1.1936722993850708] \n",
            "\n",
            "This is group: 1829.0 and this is first subgraph \n",
            "Target node: 13898\n",
            "1-hop neighbors of A: {13899, 13900, 13901}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13904, 13905, 13902, 13903}\n",
            "The loss value is :  [0.7440568208694458] \n",
            "\n",
            "The loss value is :  [0.7440568208694458] \n",
            "\n",
            "This is group: 1830.0 and this is first subgraph \n",
            "Target node: 13907\n",
            "1-hop neighbors of A: {13908, 13909, 13910}\n",
            "2-hop neighbors of A: {13912, 13911}\n",
            "Out of neighborhood of A: set()\n",
            "The loss value is :  [0.4867444634437561] \n",
            "\n",
            "The loss value is :  [0.4867444634437561] \n",
            "\n",
            "This is group: 1831.0 and this is first subgraph \n",
            "Target node: 13911\n",
            "1-hop neighbors of A: {13912, 13913, 13914}\n",
            "2-hop neighbors of A: {13915, 13916}\n",
            "Out of neighborhood of A: {13917}\n",
            "The loss value is :  [0.7240493893623352] \n",
            "\n",
            "The loss value is :  [0.7240493893623352] \n",
            "\n",
            "This is group: 1832.0 and this is first subgraph \n",
            "Target node: 13916\n",
            "1-hop neighbors of A: {13917}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13920, 13921, 13918, 13919}\n",
            "The loss value is :  [0.3669039309024811] \n",
            "\n",
            "The loss value is :  [0.3669039309024811] \n",
            "\n",
            "This is group: 1833.0 and this is first subgraph \n",
            "Target node: 13918\n",
            "1-hop neighbors of A: {13924, 13925}\n",
            "2-hop neighbors of A: {13921, 13923}\n",
            "Out of neighborhood of A: {13952, 13919, 13920, 13922, 13926, 13927, 13928, 13929, 13930, 13931, 13932, 13933, 13934, 13935, 13936, 13937, 13938, 13939, 13940, 13941, 13942, 13943, 13944, 13945, 13946, 13947, 13948, 13949, 13950, 13951}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1834.0 and this is first subgraph \n",
            "Target node: 13942\n",
            "1-hop neighbors of A: {13955}\n",
            "2-hop neighbors of A: {13956}\n",
            "Out of neighborhood of A: {13952, 13953, 13954, 13948, 13949, 13951}\n",
            "The loss value is :  [0.5479711890220642] \n",
            "\n",
            "The loss value is :  [0.5479711890220642] \n",
            "\n",
            "This is group: 1835.0 and this is first subgraph \n",
            "Target node: 13955\n",
            "1-hop neighbors of A: {13956}\n",
            "2-hop neighbors of A: {13957}\n",
            "Out of neighborhood of A: {13958, 13959, 13960, 13961, 13962, 13963}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1836.0 and this is first subgraph \n",
            "Target node: 13945\n",
            "1-hop neighbors of A: {13970}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13962, 13963, 13964, 13965, 13966, 13967, 13968, 13969, 13971, 13972, 13973}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1837.0 and this is first subgraph \n",
            "Target node: 13943\n",
            "1-hop neighbors of A: {13944}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13958, 13970, 13972, 13973, 13974, 13975, 13976, 13977, 13950, 13951}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1838.0 and this is first subgraph \n",
            "Target node: 13974\n",
            "1-hop neighbors of A: {13977}\n",
            "2-hop neighbors of A: {13980, 13981}\n",
            "Out of neighborhood of A: {13984, 13985, 13986, 13975, 13976, 13978, 13979, 13982, 13983}\n",
            "The loss value is :  [0.4040273427963257] \n",
            "\n",
            "The loss value is :  [0.4040273427963257] \n",
            "\n",
            "This is group: 1839.0 and this is first subgraph \n",
            "Target node: 13984\n",
            "1-hop neighbors of A: {13986}\n",
            "2-hop neighbors of A: {13987, 13988}\n",
            "Out of neighborhood of A: {13985, 13989, 13990}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1840.0 and this is first subgraph \n",
            "Target node: 13989\n",
            "1-hop neighbors of A: {13990, 13991}\n",
            "2-hop neighbors of A: {13992}\n",
            "Out of neighborhood of A: {13993, 13994, 13995, 13996}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1841.0 and this is first subgraph \n",
            "Target node: 13995\n",
            "1-hop neighbors of A: {13996, 13998}\n",
            "2-hop neighbors of A: {13999}\n",
            "Out of neighborhood of A: {14000, 14001, 14002, 13997}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1842.0 and this is first subgraph \n",
            "Target node: 13996\n",
            "1-hop neighbors of A: {14006}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13997, 13999, 14001, 14002, 14003, 14004, 14005}\n",
            "The loss value is :  [0.9051263332366943] \n",
            "\n",
            "The loss value is :  [0.9051263332366943] \n",
            "\n",
            "This is group: 1843.0 and this is first subgraph \n",
            "Target node: 13997\n",
            "1-hop neighbors of A: {14006}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {13998, 14000, 14001, 14002, 14007, 14008, 14009}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1844.0 and this is first subgraph \n",
            "Target node: 14008\n",
            "1-hop neighbors of A: {14009, 14010}\n",
            "2-hop neighbors of A: {14011, 14012}\n",
            "Out of neighborhood of A: {14013, 14014}\n",
            "The loss value is :  [0.28766492009162903] \n",
            "\n",
            "The loss value is :  [0.28766492009162903] \n",
            "\n",
            "This is group: 1845.0 and this is first subgraph \n",
            "Target node: 14014\n",
            "1-hop neighbors of A: {14015}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14016, 14017, 14018, 14019, 14020, 14021}\n",
            "The loss value is :  [0.473442405462265] \n",
            "\n",
            "The loss value is :  [0.473442405462265] \n",
            "\n",
            "This is group: 1846.0 and this is first subgraph \n",
            "Target node: 14018\n",
            "1-hop neighbors of A: {14021}\n",
            "2-hop neighbors of A: {14024}\n",
            "Out of neighborhood of A: {14019, 14020, 14022, 14023, 14025}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1847.0 and this is first subgraph \n",
            "Target node: 14024\n",
            "1-hop neighbors of A: {14025}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14026, 14027, 14028, 14029, 14030, 14031}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1848.0 and this is first subgraph \n",
            "Target node: 14031\n",
            "1-hop neighbors of A: {14032, 14033, 14034}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14035, 14036, 14037, 14038, 14039, 14040, 14041, 14042, 14043, 14044, 14045, 14046, 14047, 14048, 14049, 14050}\n",
            "The loss value is :  [0.8406578302383423] \n",
            "\n",
            "The loss value is :  [0.8406578302383423] \n",
            "\n",
            "This is group: 1849.0 and this is first subgraph \n",
            "Target node: 14046\n",
            "1-hop neighbors of A: {14052}\n",
            "2-hop neighbors of A: {14050}\n",
            "Out of neighborhood of A: {14048, 14049, 14051, 14053, 14054, 14055, 14056, 14057, 14047}\n",
            "The loss value is :  [0.30489280819892883] \n",
            "\n",
            "The loss value is :  [0.30489280819892883] \n",
            "\n",
            "This is group: 1850.0 and this is first subgraph \n",
            "Target node: 14056\n",
            "1-hop neighbors of A: {14057, 14058, 14059, 14060}\n",
            "2-hop neighbors of A: {14061, 14062}\n",
            "Out of neighborhood of A: {14063}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1851.0 and this is first subgraph \n",
            "Target node: 14061\n",
            "1-hop neighbors of A: {14064, 14063}\n",
            "2-hop neighbors of A: {14066, 14067, 14068}\n",
            "Out of neighborhood of A: {14065, 14069, 14062}\n",
            "The loss value is :  [0.4919353723526001] \n",
            "\n",
            "The loss value is :  [0.4919353723526001] \n",
            "\n",
            "This is group: 1852.0 and this is first subgraph \n",
            "Target node: 14065\n",
            "1-hop neighbors of A: {14069}\n",
            "2-hop neighbors of A: {14072, 14073}\n",
            "Out of neighborhood of A: {14067, 14068, 14070, 14071}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1853.0 and this is first subgraph \n",
            "Target node: 14069\n",
            "1-hop neighbors of A: {14074}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14071, 14075, 14076, 14077, 14078, 14079}\n",
            "The loss value is :  [0.2381487637758255] \n",
            "\n",
            "The loss value is :  [0.2381487637758255] \n",
            "\n",
            "This is group: 1854.0 and this is first subgraph \n",
            "Target node: 14077\n",
            "1-hop neighbors of A: {14080}\n",
            "2-hop neighbors of A: {14082}\n",
            "Out of neighborhood of A: {14081, 14083, 14084, 14085, 14078}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1855.0 and this is first subgraph \n",
            "Target node: 14084\n",
            "1-hop neighbors of A: {14085}\n",
            "2-hop neighbors of A: {14086, 14087}\n",
            "Out of neighborhood of A: {14088, 14089, 14090}\n",
            "The loss value is :  [0.7195755243301392] \n",
            "\n",
            "The loss value is :  [0.7195755243301392] \n",
            "\n",
            "This is group: 1856.0 and this is first subgraph \n",
            "Target node: 14088\n",
            "1-hop neighbors of A: {14091}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14089, 14090, 14092, 14093, 14094, 14095, 14096}\n",
            "The loss value is :  [0.32109907269477844] \n",
            "\n",
            "The loss value is :  [0.32109907269477844] \n",
            "\n",
            "This is group: 1857.0 and this is first subgraph \n",
            "Target node: 14096\n",
            "1-hop neighbors of A: {14097, 14098}\n",
            "2-hop neighbors of A: {14099, 14100}\n",
            "Out of neighborhood of A: {14101}\n",
            "The loss value is :  [0.4838162064552307] \n",
            "\n",
            "The loss value is :  [0.4838162064552307] \n",
            "\n",
            "This is group: 1858.0 and this is first subgraph \n",
            "Target node: 14099\n",
            "1-hop neighbors of A: {14101}\n",
            "2-hop neighbors of A: {14103}\n",
            "Out of neighborhood of A: {14100, 14102, 14104, 14105, 14106}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1859.0 and this is first subgraph \n",
            "Target node: 14065\n",
            "1-hop neighbors of A: {14066}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14081, 14082, 14090, 14092, 14102, 14103, 14105, 14107, 14108, 14109, 14110, 14111, 14112, 14113, 14114, 14115, 14116, 14117, 14118, 14119, 14120, 14068, 14070}\n",
            "The loss value is :  [0.7213890552520752] \n",
            "\n",
            "The loss value is :  [0.7213890552520752] \n",
            "\n",
            "This is group: 1860.0 and this is first subgraph \n",
            "Target node: 14115\n",
            "1-hop neighbors of A: {14122}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14116, 14117, 14120, 14121, 14123, 14124, 14125, 14126}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1861.0 and this is first subgraph \n",
            "Target node: 14125\n",
            "1-hop neighbors of A: {14126, 14127}\n",
            "2-hop neighbors of A: {14128, 14129}\n",
            "Out of neighborhood of A: {14130}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1862.0 and this is first subgraph \n",
            "Target node: 14131\n",
            "1-hop neighbors of A: {14132}\n",
            "2-hop neighbors of A: {14133, 14134}\n",
            "Out of neighborhood of A: {14136, 14137, 14135}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1863.0 and this is first subgraph \n",
            "Target node: 14134\n",
            "1-hop neighbors of A: {14137}\n",
            "2-hop neighbors of A: {14138}\n",
            "Out of neighborhood of A: {14144, 14145, 14146, 14147, 14148, 14136, 14139, 14140, 14141, 14142, 14143}\n",
            "The loss value is :  [0.7029678821563721] \n",
            "\n",
            "The loss value is :  [0.7029678821563721] \n",
            "\n",
            "This is group: 1864.0 and this is first subgraph \n",
            "Target node: 14147\n",
            "1-hop neighbors of A: {14151}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14148, 14149, 14150, 14152, 14153}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1865.0 and this is first subgraph \n",
            "Target node: 14147\n",
            "1-hop neighbors of A: {14155}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14149, 14150, 14151, 14152, 14153, 14154, 14156, 14157, 14158}\n",
            "The loss value is :  [1.10374116897583] \n",
            "\n",
            "The loss value is :  [1.10374116897583] \n",
            "\n",
            "This is group: 1866.0 and this is first subgraph \n",
            "Target node: 14157\n",
            "1-hop neighbors of A: {14162, 14158}\n",
            "2-hop neighbors of A: {14161, 14163, 14168, 14159}\n",
            "Out of neighborhood of A: {14160, 14164, 14165, 14166, 14167, 14169, 14170}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1867.0 and this is first subgraph \n",
            "Target node: 14169\n",
            "1-hop neighbors of A: {14171}\n",
            "2-hop neighbors of A: {14173, 14174}\n",
            "Out of neighborhood of A: {14176, 14170, 14172, 14175}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1868.0 and this is first subgraph \n",
            "Target node: 14172\n",
            "1-hop neighbors of A: {14176}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14177, 14178, 14179, 14180, 14181, 14173, 14174, 14175}\n",
            "The loss value is :  [0.663500189781189] \n",
            "\n",
            "The loss value is :  [0.663500189781189] \n",
            "\n",
            "This is group: 1869.0 and this is first subgraph \n",
            "Target node: 14176\n",
            "1-hop neighbors of A: {14182}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14177, 14178, 14180, 14181, 14183, 14184, 14185, 14186, 14187}\n",
            "The loss value is :  [0.6515889167785645] \n",
            "\n",
            "The loss value is :  [0.6515889167785645] \n",
            "\n",
            "This is group: 1870.0 and this is first subgraph \n",
            "Target node: 14181\n",
            "1-hop neighbors of A: {14187, 14188}\n",
            "2-hop neighbors of A: {14192, 14193, 14194, 14195}\n",
            "Out of neighborhood of A: {14208, 14209, 14210, 14211, 14212, 14213, 14214, 14215, 14216, 14184, 14186, 14189, 14190, 14191, 14196, 14197, 14198, 14199, 14200, 14201, 14202, 14203, 14204, 14205, 14206, 14207}\n",
            "The loss value is :  [0.7373954653739929] \n",
            "\n",
            "The loss value is :  [0.7373954653739929] \n",
            "\n",
            "This is group: 1871.0 and this is first subgraph \n",
            "Target node: 14212\n",
            "1-hop neighbors of A: {14217}\n",
            "2-hop neighbors of A: {14220}\n",
            "Out of neighborhood of A: {14213, 14215, 14218, 14219, 14221, 14222}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1872.0 and this is first subgraph \n",
            "Target node: 14180\n",
            "1-hop neighbors of A: {14183}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14182, 14185, 14219, 14188, 14189, 14222, 14202, 14206}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1873.0 and this is first subgraph \n",
            "Target node: 14202\n",
            "1-hop neighbors of A: {14206}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14215, 14216, 14218, 14220, 14223, 14224, 14225, 14226, 14227, 14228, 14229, 14230, 14231, 14232, 14233, 14234, 14235, 14236, 14237, 14238, 14239, 14240, 14241, 14242, 14243, 14244, 14245, 14246, 14247, 14248, 14249, 14250}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1874.0 and this is first subgraph \n",
            "Target node: 14248\n",
            "1-hop neighbors of A: {14251, 14252}\n",
            "2-hop neighbors of A: {14256}\n",
            "Out of neighborhood of A: {14249, 14250, 14253, 14254, 14255}\n",
            "The loss value is :  [0.575752317905426] \n",
            "\n",
            "The loss value is :  [0.575752317905426] \n",
            "\n",
            "This is group: 1875.0 and this is first subgraph \n",
            "Target node: 14252\n",
            "1-hop neighbors of A: {14256}\n",
            "2-hop neighbors of A: {14261}\n",
            "Out of neighborhood of A: {14253, 14254, 14255, 14257, 14258, 14259, 14260}\n",
            "The loss value is :  [0.6909217834472656] \n",
            "\n",
            "The loss value is :  [0.6909217834472656] \n",
            "\n",
            "This is group: 1876.0 and this is first subgraph \n",
            "Target node: 14256\n",
            "1-hop neighbors of A: {14261, 14262}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14264, 14258, 14263}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1877.0 and this is first subgraph \n",
            "Target node: 14259\n",
            "1-hop neighbors of A: {14265}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14272, 14273, 14260, 14262, 14264, 14266, 14267, 14268, 14269, 14270, 14271}\n",
            "The loss value is :  [0.6309705376625061] \n",
            "\n",
            "The loss value is :  [0.6309705376625061] \n",
            "\n",
            "This is group: 1878.0 and this is first subgraph \n",
            "Target node: 14271\n",
            "1-hop neighbors of A: {14273}\n",
            "2-hop neighbors of A: {14275, 14276}\n",
            "Out of neighborhood of A: {14272, 14274, 14277, 14278}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1879.0 and this is first subgraph \n",
            "Target node: 14277\n",
            "1-hop neighbors of A: {14279}\n",
            "2-hop neighbors of A: {14282}\n",
            "Out of neighborhood of A: {14278, 14280, 14281, 14283, 14284}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1880.0 and this is first subgraph \n",
            "Target node: 14282\n",
            "1-hop neighbors of A: {14284}\n",
            "2-hop neighbors of A: {14287}\n",
            "Out of neighborhood of A: {14283, 14285, 14286, 14288, 14289, 14290, 14291, 14292, 14293, 14294}\n",
            "The loss value is :  [0.7926251888275146] \n",
            "\n",
            "The loss value is :  [0.7926251888275146] \n",
            "\n",
            "This is group: 1881.0 and this is first subgraph \n",
            "Target node: 14241\n",
            "1-hop neighbors of A: {14242}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14259, 14292, 14291, 14294, 14295, 14296, 14264}\n",
            "The loss value is :  [0.6332432627677917] \n",
            "\n",
            "The loss value is :  [0.6332432627677917] \n",
            "\n",
            "This is group: 1882.0 and this is first subgraph \n",
            "Target node: 14297\n",
            "1-hop neighbors of A: {14298, 14302, 14303}\n",
            "2-hop neighbors of A: {14299, 14307}\n",
            "Out of neighborhood of A: {14300}\n",
            "The loss value is :  [0.8475449681282043] \n",
            "\n",
            "The loss value is :  [0.8475449681282043] \n",
            "\n",
            "This is group: 1883.0 and this is first subgraph \n",
            "Target node: 14299\n",
            "1-hop neighbors of A: {14312}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14306, 14308, 14300, 14301, 14302}\n",
            "The loss value is :  [0.37637802958488464] \n",
            "\n",
            "The loss value is :  [0.37637802958488464] \n",
            "\n",
            "This is group: 1884.0 and this is first subgraph \n",
            "Target node: 14302\n",
            "1-hop neighbors of A: {14306}\n",
            "2-hop neighbors of A: {14305}\n",
            "Out of neighborhood of A: {14304, 14353, 14307, 14303}\n",
            "The loss value is :  [0.3321506381034851] \n",
            "\n",
            "The loss value is :  [0.3321506381034851] \n",
            "\n",
            "This is group: 1885.0 and this is first subgraph \n",
            "Target node: 14308\n",
            "1-hop neighbors of A: {14309, 14310, 14311}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14312, 14313, 14314, 14315, 14319}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1886.0 and this is first subgraph \n",
            "Target node: 14314\n",
            "1-hop neighbors of A: {14315, 14319}\n",
            "2-hop neighbors of A: {14320, 14316}\n",
            "Out of neighborhood of A: {14324, 14317}\n",
            "The loss value is :  [0.5055360794067383] \n",
            "\n",
            "The loss value is :  [0.5055360794067383] \n",
            "\n",
            "This is group: 1887.0 and this is first subgraph \n",
            "Target node: 14317\n",
            "1-hop neighbors of A: {14318}\n",
            "2-hop neighbors of A: {14319}\n",
            "Out of neighborhood of A: {14320, 14321, 14322}\n",
            "The loss value is :  [0.4078342914581299] \n",
            "\n",
            "The loss value is :  [0.4078342914581299] \n",
            "\n",
            "This is group: 1888.0 and this is first subgraph \n",
            "Target node: 14320\n",
            "1-hop neighbors of A: {14323}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14352, 14324, 14325, 14326, 14330, 14331}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1889.0 and this is first subgraph \n",
            "Target node: 14326\n",
            "1-hop neighbors of A: {14335, 14327}\n",
            "2-hop neighbors of A: {14355, 14340, 14328}\n",
            "Out of neighborhood of A: {14336, 14337, 14338, 14339, 14341, 14342, 14343, 14344, 14345, 14346, 14347, 14348, 14349, 14350, 14351, 14352, 14353, 14354, 14356, 14357, 14358, 14329, 14330, 14331, 14332, 14333, 14334}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1890.0 and this is first subgraph \n",
            "Target node: 14358\n",
            "1-hop neighbors of A: {14360, 14361, 14359}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14362, 14363, 14364, 14365}\n",
            "The loss value is :  [0.693584680557251] \n",
            "\n",
            "The loss value is :  [0.693584680557251] \n",
            "\n",
            "This is group: 1891.0 and this is first subgraph \n",
            "Target node: 14363\n",
            "1-hop neighbors of A: {14365}\n",
            "2-hop neighbors of A: {14368, 14367}\n",
            "Out of neighborhood of A: {14369, 14370, 14371, 14372, 14373, 14374, 14364, 14366}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1892.0 and this is first subgraph \n",
            "Target node: 14373\n",
            "1-hop neighbors of A: {14375}\n",
            "2-hop neighbors of A: {14377, 14378}\n",
            "Out of neighborhood of A: {14376, 14379, 14380, 14374}\n",
            "The loss value is :  [0.6183843612670898] \n",
            "\n",
            "The loss value is :  [0.6183843612670898] \n",
            "\n",
            "This is group: 1893.0 and this is first subgraph \n",
            "Target node: 14377\n",
            "1-hop neighbors of A: {14381, 14382}\n",
            "2-hop neighbors of A: {14386}\n",
            "Out of neighborhood of A: {14378, 14379, 14380, 14383, 14384, 14385}\n",
            "The loss value is :  [0.32231760025024414] \n",
            "\n",
            "The loss value is :  [0.32231760025024414] \n",
            "\n",
            "This is group: 1894.0 and this is first subgraph \n",
            "Target node: 14381\n",
            "1-hop neighbors of A: {14386}\n",
            "2-hop neighbors of A: {14390}\n",
            "Out of neighborhood of A: {14382, 14383, 14384, 14387, 14388, 14389}\n",
            "The loss value is :  [0.733017086982727] \n",
            "\n",
            "The loss value is :  [0.733017086982727] \n",
            "\n",
            "This is group: 1895.0 and this is first subgraph \n",
            "Target node: 14386\n",
            "1-hop neighbors of A: {14391}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14387, 14388, 14389, 14392, 14393, 14394, 14395, 14396}\n",
            "The loss value is :  [0.6211241483688354] \n",
            "\n",
            "The loss value is :  [0.6211241483688354] \n",
            "\n",
            "This is group: 1896.0 and this is first subgraph \n",
            "Target node: 14389\n",
            "1-hop neighbors of A: {14396}\n",
            "2-hop neighbors of A: {14400, 14398, 14399}\n",
            "Out of neighborhood of A: {14392, 14397}\n",
            "The loss value is :  [0.5576292276382446] \n",
            "\n",
            "The loss value is :  [0.5576292276382446] \n",
            "\n",
            "This is group: 1897.0 and this is first subgraph \n",
            "Target node: 14397\n",
            "1-hop neighbors of A: {14401}\n",
            "2-hop neighbors of A: {14402, 14403}\n",
            "Out of neighborhood of A: {14404, 14405, 14406, 14407, 14408, 14409, 14410, 14411, 14412, 14413, 14414, 14415, 14416, 14417, 14418, 14419}\n",
            "The loss value is :  [0.8244434595108032] \n",
            "\n",
            "The loss value is :  [0.8244434595108032] \n",
            "\n",
            "This is group: 1898.0 and this is first subgraph \n",
            "Target node: 14383\n",
            "1-hop neighbors of A: {14387}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14406, 14407, 14415, 14417, 14419, 14420, 14421, 14422, 14423, 14424, 14425, 14426, 14427, 14428, 14429, 14430, 14431, 14432, 14433, 14434, 14435, 14436, 14437, 14438, 14439, 14385, 14389}\n",
            "The loss value is :  [0.6779258251190186] \n",
            "\n",
            "The loss value is :  [0.6779258251190186] \n",
            "\n",
            "This is group: 1899.0 and this is first subgraph \n",
            "Target node: 14438\n",
            "1-hop neighbors of A: {14440}\n",
            "2-hop neighbors of A: {14444}\n",
            "Out of neighborhood of A: {14439, 14441, 14442, 14443, 14445}\n",
            "The loss value is :  [1.1001287698745728] \n",
            "\n",
            "The loss value is :  [1.1001287698745728] \n",
            "\n",
            "This is group: 1900.0 and this is first subgraph \n",
            "Target node: 14444\n",
            "1-hop neighbors of A: {14445, 14446}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14447, 14448, 14449, 14450, 14451}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1901.0 and this is first subgraph \n",
            "Target node: 14450\n",
            "1-hop neighbors of A: {14451}\n",
            "2-hop neighbors of A: {14452}\n",
            "Out of neighborhood of A: {14456, 14453, 14454, 14455}\n",
            "The loss value is :  [0.1365864872932434] \n",
            "\n",
            "The loss value is :  [0.1365864872932434] \n",
            "\n",
            "This is group: 1902.0 and this is first subgraph \n",
            "Target node: 14455\n",
            "1-hop neighbors of A: {14457}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14464, 14465, 14466, 14467, 14468, 14469, 14458, 14459, 14460, 14461, 14462, 14463}\n",
            "The loss value is :  [0.8220918774604797] \n",
            "\n",
            "The loss value is :  [0.8220918774604797] \n",
            "\n",
            "This is group: 1903.0 and this is first subgraph \n",
            "Target node: 14466\n",
            "1-hop neighbors of A: {14469}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14470, 14471, 14472, 14473, 14474}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1904.0 and this is first subgraph \n",
            "Target node: 14472\n",
            "1-hop neighbors of A: {14475}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14473, 14476, 14477, 14478, 14479}\n",
            "The loss value is :  [0.7173205018043518] \n",
            "\n",
            "The loss value is :  [0.7173205018043518] \n",
            "\n",
            "This is group: 1905.0 and this is first subgraph \n",
            "Target node: 14479\n",
            "1-hop neighbors of A: {14480, 14481}\n",
            "2-hop neighbors of A: {14482, 14483}\n",
            "Out of neighborhood of A: set()\n",
            "The loss value is :  [0.5516634583473206] \n",
            "\n",
            "The loss value is :  [0.5516634583473206] \n",
            "\n",
            "This is group: 1906.0 and this is first subgraph \n",
            "Target node: 14481\n",
            "1-hop neighbors of A: {14484}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14482, 14483, 14485, 14486, 14487, 14488, 14489}\n",
            "The loss value is :  [0.41654378175735474] \n",
            "\n",
            "The loss value is :  [0.41654378175735474] \n",
            "\n",
            "This is group: 1907.0 and this is first subgraph \n",
            "Target node: 14484\n",
            "1-hop neighbors of A: {14486}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14485, 14487, 14488, 14489, 14490, 14491, 14492, 14493, 14494}\n",
            "The loss value is :  [0.3537750840187073] \n",
            "\n",
            "The loss value is :  [0.3537750840187073] \n",
            "\n",
            "This is group: 1908.0 and this is first subgraph \n",
            "Target node: 14491\n",
            "1-hop neighbors of A: {14494}\n",
            "2-hop neighbors of A: {14496}\n",
            "Out of neighborhood of A: {14497, 14498, 14499, 14493, 14495}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1909.0 and this is first subgraph \n",
            "Target node: 14496\n",
            "1-hop neighbors of A: {14499, 14500}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14501, 14502, 14503, 14504, 14505}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1910.0 and this is first subgraph \n",
            "Target node: 14503\n",
            "1-hop neighbors of A: {14506}\n",
            "2-hop neighbors of A: {14511}\n",
            "Out of neighborhood of A: {14504, 14505, 14507, 14508, 14509, 14510}\n",
            "The loss value is :  [0.3260893225669861] \n",
            "\n",
            "The loss value is :  [0.3260893225669861] \n",
            "\n",
            "This is group: 1911.0 and this is first subgraph \n",
            "Target node: 14506\n",
            "1-hop neighbors of A: {14511}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14507, 14508, 14510, 14512, 14513, 14514, 14515}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1912.0 and this is first subgraph \n",
            "Target node: 14513\n",
            "1-hop neighbors of A: {14516, 14517, 14518}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14514, 14515, 14519, 14520, 14521}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1913.0 and this is first subgraph \n",
            "Target node: 14515\n",
            "1-hop neighbors of A: {14521}\n",
            "2-hop neighbors of A: {14524}\n",
            "Out of neighborhood of A: {14519, 14520, 14522, 14523, 14525, 14526, 14527, 14528, 14529, 14530, 14531, 14532, 14533, 14534, 14535, 14536, 14537, 14538, 14539, 14540, 14541, 14542, 14543, 14544, 14545, 14546, 14547, 14548, 14549, 14550}\n",
            "The loss value is :  [0.6299476027488708] \n",
            "\n",
            "The loss value is :  [0.6299476027488708] \n",
            "\n",
            "This is group: 1914.0 and this is first subgraph \n",
            "Target node: 14508\n",
            "1-hop neighbors of A: {14509}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14528, 14533, 14534, 14511, 14514, 14549, 14551, 14552, 14524}\n",
            "The loss value is :  [0.8256597518920898] \n",
            "\n",
            "The loss value is :  [0.8256597518920898] \n",
            "\n",
            "This is group: 1915.0 and this is first subgraph \n",
            "Target node: 14533\n",
            "1-hop neighbors of A: {14534}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14537, 14539, 14550, 14552, 14553, 14554, 14555, 14556, 14557, 14558, 14559, 14560, 14561, 14562, 14563, 14564, 14565, 14566, 14567, 14568, 14569, 14570, 14571, 14572, 14573}\n",
            "The loss value is :  [0.5605485439300537] \n",
            "\n",
            "The loss value is :  [0.5605485439300537] \n",
            "\n",
            "This is group: 1916.0 and this is first subgraph \n",
            "Target node: 14574\n",
            "1-hop neighbors of A: {14575}\n",
            "2-hop neighbors of A: {14576, 14577, 14578}\n",
            "Out of neighborhood of A: {14579, 14580}\n",
            "The loss value is :  [0.24470341205596924] \n",
            "\n",
            "The loss value is :  [0.24470341205596924] \n",
            "\n",
            "This is group: 1917.0 and this is first subgraph \n",
            "Target node: 14581\n",
            "1-hop neighbors of A: {14582, 14583}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14584, 14585, 14586, 14587, 14588}\n",
            "The loss value is :  [0.2598658800125122] \n",
            "\n",
            "The loss value is :  [0.2598658800125122] \n",
            "\n",
            "This is group: 1918.0 and this is first subgraph \n",
            "Target node: 14585\n",
            "1-hop neighbors of A: {14588, 14589}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14592, 14593, 14594, 14595, 14586, 14590, 14591}\n",
            "The loss value is :  [0.419518381357193] \n",
            "\n",
            "The loss value is :  [0.419518381357193] \n",
            "\n",
            "This is group: 1919.0 and this is first subgraph \n",
            "Target node: 14593\n",
            "1-hop neighbors of A: {14603, 14604}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14594, 14595, 14596, 14597, 14598, 14599, 14600, 14601, 14602, 14605, 14606, 14607, 14608, 14609, 14610, 14611, 14612, 14613, 14614, 14615, 14616, 14617, 14618, 14619, 14620, 14621, 14622, 14623, 14624, 14625}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1920.0 and this is first subgraph \n",
            "Target node: 14624\n",
            "1-hop neighbors of A: {14625}\n",
            "2-hop neighbors of A: {14626, 14630}\n",
            "Out of neighborhood of A: {14627, 14628, 14629}\n",
            "The loss value is :  [0.7099826335906982] \n",
            "\n",
            "The loss value is :  [0.7099826335906982] \n",
            "\n",
            "This is group: 1921.0 and this is first subgraph \n",
            "Target node: 14627\n",
            "1-hop neighbors of A: {14631}\n",
            "2-hop neighbors of A: {14632, 14633}\n",
            "Out of neighborhood of A: {14634, 14635, 14628, 14636}\n",
            "The loss value is :  [0.48535680770874023] \n",
            "\n",
            "The loss value is :  [0.48535680770874023] \n",
            "\n",
            "This is group: 1922.0 and this is first subgraph \n",
            "Target node: 14619\n",
            "1-hop neighbors of A: {14620}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14629, 14630, 14635, 14637, 14638, 14639, 14640, 14641, 14642, 14643, 14644, 14645, 14646, 14647}\n",
            "The loss value is :  [0.3213866949081421] \n",
            "\n",
            "The loss value is :  [0.3213866949081421] \n",
            "\n",
            "This is group: 1923.0 and this is first subgraph \n",
            "Target node: 14641\n",
            "1-hop neighbors of A: {14648}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14642, 14643, 14646, 14647, 14649, 14650}\n",
            "The loss value is :  [0.5138757228851318] \n",
            "\n",
            "The loss value is :  [0.5138757228851318] \n",
            "\n",
            "This is group: 1924.0 and this is first subgraph \n",
            "Target node: 14649\n",
            "1-hop neighbors of A: {14651}\n",
            "2-hop neighbors of A: {14652}\n",
            "Out of neighborhood of A: {14656, 14657, 14650, 14653, 14654, 14655}\n",
            "The loss value is :  [0.5276581048965454] \n",
            "\n",
            "The loss value is :  [0.5276581048965454] \n",
            "\n",
            "This is group: 1925.0 and this is first subgraph \n",
            "Target node: 14651\n",
            "1-hop neighbors of A: {14661}\n",
            "2-hop neighbors of A: {14662, 14663}\n",
            "Out of neighborhood of A: {14657, 14658, 14659, 14660, 14655}\n",
            "The loss value is :  [0.7567590475082397] \n",
            "\n",
            "The loss value is :  [0.7567590475082397] \n",
            "\n",
            "This is group: 1926.0 and this is first subgraph \n",
            "Target node: 14649\n",
            "1-hop neighbors of A: {14667}\n",
            "2-hop neighbors of A: {14668}\n",
            "Out of neighborhood of A: {14658, 14664, 14665, 14666, 14669, 14653}\n",
            "The loss value is :  [0.7760609984397888] \n",
            "\n",
            "The loss value is :  [0.7760609984397888] \n",
            "\n",
            "This is group: 1927.0 and this is first subgraph \n",
            "Target node: 14653\n",
            "1-hop neighbors of A: {14669, 14654}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14659, 14660, 14661, 14670, 14671, 14672}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1928.0 and this is first subgraph \n",
            "Target node: 14670\n",
            "1-hop neighbors of A: {14675}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14671, 14672, 14673, 14674, 14679, 14680}\n",
            "The loss value is :  [0.41874051094055176] \n",
            "\n",
            "The loss value is :  [0.41874051094055176] \n",
            "\n",
            "This is group: 1929.0 and this is first subgraph \n",
            "Target node: 14674\n",
            "1-hop neighbors of A: {14680}\n",
            "2-hop neighbors of A: {14681}\n",
            "Out of neighborhood of A: {14675, 14676, 14677, 14678, 14682, 14683}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1930.0 and this is first subgraph \n",
            "Target node: 14682\n",
            "1-hop neighbors of A: {14683}\n",
            "2-hop neighbors of A: {14684}\n",
            "Out of neighborhood of A: {14685, 14686}\n",
            "The loss value is :  [1.2298705577850342] \n",
            "\n",
            "The loss value is :  [1.2298705577850342] \n",
            "\n",
            "This is group: 1931.0 and this is first subgraph \n",
            "Target node: 14682\n",
            "1-hop neighbors of A: {14687}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14688, 14689, 14690, 14691, 14692, 14685, 14686}\n",
            "The loss value is :  [0.39161521196365356] \n",
            "\n",
            "The loss value is :  [0.39161521196365356] \n",
            "\n",
            "This is group: 1932.0 and this is first subgraph \n",
            "Target node: 14690\n",
            "1-hop neighbors of A: {14693}\n",
            "2-hop neighbors of A: {14695}\n",
            "Out of neighborhood of A: {14691, 14692, 14694, 14696, 14697}\n",
            "The loss value is :  [0.6260797381401062] \n",
            "\n",
            "The loss value is :  [0.6260797381401062] \n",
            "\n",
            "This is group: 1933.0 and this is first subgraph \n",
            "Target node: 14695\n",
            "1-hop neighbors of A: {14700}\n",
            "2-hop neighbors of A: {14701}\n",
            "Out of neighborhood of A: {14696, 14697, 14698, 14699}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1934.0 and this is first subgraph \n",
            "Target node: 14686\n",
            "1-hop neighbors of A: {14687}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14689, 14690, 14692, 14693, 14696, 14699, 14700, 14701, 14702, 14703, 14704}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1935.0 and this is first subgraph \n",
            "Target node: 14703\n",
            "1-hop neighbors of A: {14704, 14705}\n",
            "2-hop neighbors of A: {14706}\n",
            "Out of neighborhood of A: {14707}\n",
            "The loss value is :  [0.3531181812286377] \n",
            "\n",
            "The loss value is :  [0.3531181812286377] \n",
            "\n",
            "This is group: 1936.0 and this is first subgraph \n",
            "Target node: 14704\n",
            "1-hop neighbors of A: {14708}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14705, 14706, 14709, 14710, 14711, 14712}\n",
            "The loss value is :  [0.5187622308731079] \n",
            "\n",
            "The loss value is :  [0.5187622308731079] \n",
            "\n",
            "This is group: 1937.0 and this is first subgraph \n",
            "Target node: 14707\n",
            "1-hop neighbors of A: {14708}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14710, 14711, 14712, 14713, 14714, 14715, 14716, 14717}\n",
            "The loss value is :  [0.5346116423606873] \n",
            "\n",
            "The loss value is :  [0.5346116423606873] \n",
            "\n",
            "This is group: 1938.0 and this is first subgraph \n",
            "Target node: 14714\n",
            "1-hop neighbors of A: {14717}\n",
            "2-hop neighbors of A: {14721}\n",
            "Out of neighborhood of A: {14720, 14722, 14723, 14715, 14716, 14718, 14719}\n",
            "The loss value is :  [0.4188256859779358] \n",
            "\n",
            "The loss value is :  [0.4188256859779358] \n",
            "\n",
            "This is group: 1939.0 and this is first subgraph \n",
            "Target node: 14719\n",
            "1-hop neighbors of A: {14721}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14720, 14722, 14723, 14724, 14725}\n",
            "The loss value is :  [0.41650742292404175] \n",
            "\n",
            "The loss value is :  [0.41650742292404175] \n",
            "\n",
            "This is group: 1940.0 and this is first subgraph \n",
            "Target node: 14726\n",
            "1-hop neighbors of A: {14728, 14727}\n",
            "2-hop neighbors of A: {14729, 14730}\n",
            "Out of neighborhood of A: set()\n",
            "The loss value is :  [0.6695467829704285] \n",
            "\n",
            "The loss value is :  [0.6695467829704285] \n",
            "\n",
            "This is group: 1941.0 and this is first subgraph \n",
            "Target node: 14728\n",
            "1-hop neighbors of A: {14730}\n",
            "2-hop neighbors of A: {14733}\n",
            "Out of neighborhood of A: {14729, 14731, 14732, 14734, 14735}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1942.0 and this is first subgraph \n",
            "Target node: 14731\n",
            "1-hop neighbors of A: {14732, 14733}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14736, 14737, 14738, 14739, 14740}\n",
            "The loss value is :  [0.7181321382522583] \n",
            "\n",
            "The loss value is :  [0.7181321382522583] \n",
            "\n",
            "This is group: 1943.0 and this is first subgraph \n",
            "Target node: 14736\n",
            "1-hop neighbors of A: {14745, 14746}\n",
            "2-hop neighbors of A: {14744}\n",
            "Out of neighborhood of A: {14740, 14741, 14742, 14743, 14747, 14748, 14749, 14750, 14751, 14752, 14753, 14754, 14755, 14756, 14757, 14758, 14759, 14760, 14761, 14762, 14763, 14764, 14765}\n",
            "The loss value is :  [0.6655707359313965] \n",
            "\n",
            "The loss value is :  [0.6655707359313965] \n",
            "\n",
            "This is group: 1944.0 and this is first subgraph \n",
            "Target node: 14759\n",
            "1-hop neighbors of A: {14760}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14766, 14767, 14768, 14769, 14770, 14771}\n",
            "The loss value is :  [0.7125619649887085] \n",
            "\n",
            "The loss value is :  [0.7125619649887085] \n",
            "\n",
            "This is group: 1945.0 and this is first subgraph \n",
            "Target node: 14769\n",
            "1-hop neighbors of A: {14772, 14773}\n",
            "2-hop neighbors of A: {14770}\n",
            "Out of neighborhood of A: {14771, 14774, 14775}\n",
            "The loss value is :  [0.7273187637329102] \n",
            "\n",
            "The loss value is :  [0.7273187637329102] \n",
            "\n",
            "This is group: 1946.0 and this is first subgraph \n",
            "Target node: 14776\n",
            "1-hop neighbors of A: {14777}\n",
            "2-hop neighbors of A: {14778, 14779}\n",
            "Out of neighborhood of A: {14780, 14781}\n",
            "The loss value is :  [0.3552640974521637] \n",
            "\n",
            "The loss value is :  [0.3552640974521637] \n",
            "\n",
            "This is group: 1947.0 and this is first subgraph \n",
            "Target node: 14779\n",
            "1-hop neighbors of A: {14782}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14784, 14785, 14786, 14780, 14781, 14783}\n",
            "The loss value is :  [0.6882644891738892] \n",
            "\n",
            "The loss value is :  [0.6882644891738892] \n",
            "\n",
            "This is group: 1948.0 and this is first subgraph \n",
            "Target node: 14780\n",
            "1-hop neighbors of A: {14782}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14784, 14785, 14787, 14788, 14789, 14790, 14791, 14792, 14793, 14794, 14795, 14796, 14797}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1949.0 and this is first subgraph \n",
            "Target node: 14796\n",
            "1-hop neighbors of A: {14797, 14798, 14799}\n",
            "2-hop neighbors of A: {14800, 14801, 14802}\n",
            "Out of neighborhood of A: {14803, 14804, 14805, 14806, 14807, 14808, 14809, 14810, 14811, 14812, 14813, 14814, 14815, 14816, 14817, 14818, 14819, 14820, 14821, 14822, 14823, 14824, 14825, 14826}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1950.0 and this is first subgraph \n",
            "Target node: 14821\n",
            "1-hop neighbors of A: {14828}\n",
            "2-hop neighbors of A: {14831}\n",
            "Out of neighborhood of A: {14826, 14827, 14829, 14830}\n",
            "The loss value is :  [0.32263392210006714] \n",
            "\n",
            "The loss value is :  [0.32263392210006714] \n",
            "\n",
            "This is group: 1951.0 and this is first subgraph \n",
            "Target node: 14829\n",
            "1-hop neighbors of A: {14832}\n",
            "2-hop neighbors of A: {14833}\n",
            "Out of neighborhood of A: {14834, 14835, 14836, 14830}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1952.0 and this is first subgraph \n",
            "Target node: 14836\n",
            "1-hop neighbors of A: {14837}\n",
            "2-hop neighbors of A: {14838}\n",
            "Out of neighborhood of A: {14840, 14841, 14839}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1953.0 and this is first subgraph \n",
            "Target node: 14821\n",
            "1-hop neighbors of A: {14822}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14827, 14828, 14833, 14839, 14841, 14842, 14843, 14844, 14845}\n",
            "The loss value is :  [0.34097978472709656] \n",
            "\n",
            "The loss value is :  [0.34097978472709656] \n",
            "\n",
            "This is group: 1954.0 and this is first subgraph \n",
            "Target node: 14827\n",
            "1-hop neighbors of A: {14828}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14830, 14831, 14834, 14835, 14842, 14843, 14846, 14847}\n",
            "The loss value is :  [0.27867504954338074] \n",
            "\n",
            "The loss value is :  [0.27867504954338074] \n",
            "\n",
            "This is group: 1955.0 and this is first subgraph \n",
            "Target node: 14846\n",
            "1-hop neighbors of A: {14850, 14853}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14848, 14851, 14852, 14847}\n",
            "The loss value is :  [0.9772560596466064] \n",
            "\n",
            "The loss value is :  [0.9772560596466064] \n",
            "\n",
            "This is group: 1956.0 and this is first subgraph \n",
            "Target node: 14848\n",
            "1-hop neighbors of A: {14856}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14849, 14850, 14851, 14854, 14855}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1957.0 and this is first subgraph \n",
            "Target node: 14849\n",
            "1-hop neighbors of A: {14852}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14857, 14858, 14859, 14860}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1958.0 and this is first subgraph \n",
            "Target node: 14860\n",
            "1-hop neighbors of A: {14865, 14861}\n",
            "2-hop neighbors of A: {14862}\n",
            "Out of neighborhood of A: {14864, 14866, 14867, 14863}\n",
            "The loss value is :  [0.45991575717926025] \n",
            "\n",
            "The loss value is :  [0.45991575717926025] \n",
            "\n",
            "This is group: 1959.0 and this is first subgraph \n",
            "Target node: 14866\n",
            "1-hop neighbors of A: {14867, 14868, 14869}\n",
            "2-hop neighbors of A: {14870, 14871, 14872}\n",
            "Out of neighborhood of A: {14873, 14874, 14875, 14876, 14877, 14878, 14879, 14880, 14881, 14882, 14883, 14884, 14885, 14886, 14887, 14888, 14889, 14890, 14891, 14892, 14893, 14894, 14895, 14896}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1960.0 and this is first subgraph \n",
            "Target node: 14876\n",
            "1-hop neighbors of A: {14879}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14881, 14895, 14896, 14897, 14898, 14899, 14878}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1961.0 and this is first subgraph \n",
            "Target node: 14888\n",
            "1-hop neighbors of A: {14889}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14893, 14894, 14898, 14899, 14900, 14901}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1962.0 and this is first subgraph \n",
            "Target node: 14900\n",
            "1-hop neighbors of A: {14905, 14901}\n",
            "2-hop neighbors of A: {14902}\n",
            "Out of neighborhood of A: {14904, 14903}\n",
            "The loss value is :  [0.8966230154037476] \n",
            "\n",
            "The loss value is :  [0.8966230154037476] \n",
            "\n",
            "This is group: 1963.0 and this is first subgraph \n",
            "Target node: 14900\n",
            "1-hop neighbors of A: {14905}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14902, 14903, 14906, 14907, 14908}\n",
            "The loss value is :  [0.6052781343460083] \n",
            "\n",
            "The loss value is :  [0.6052781343460083] \n",
            "\n",
            "This is group: 1964.0 and this is first subgraph \n",
            "Target node: 14900\n",
            "1-hop neighbors of A: {14909}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14912, 14913, 14904, 14905, 14907, 14908, 14910, 14911}\n",
            "The loss value is :  [0.6741799116134644] \n",
            "\n",
            "The loss value is :  [0.6741799116134644] \n",
            "\n",
            "This is group: 1965.0 and this is first subgraph \n",
            "Target node: 14911\n",
            "1-hop neighbors of A: {14914}\n",
            "2-hop neighbors of A: {14915, 14916}\n",
            "Out of neighborhood of A: {14917, 14918, 14919, 14920, 14921, 14922, 14923, 14924, 14925, 14926, 14927}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1966.0 and this is first subgraph \n",
            "Target node: 14924\n",
            "1-hop neighbors of A: {14927}\n",
            "2-hop neighbors of A: {14931}\n",
            "Out of neighborhood of A: {14925, 14926, 14928, 14929, 14930, 14932}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1967.0 and this is first subgraph \n",
            "Target node: 14928\n",
            "1-hop neighbors of A: {14929}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14930, 14931}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1968.0 and this is first subgraph \n",
            "Target node: 14933\n",
            "1-hop neighbors of A: {14934, 14935}\n",
            "2-hop neighbors of A: {14936, 14937, 14938, 14939}\n",
            "Out of neighborhood of A: set()\n",
            "The loss value is :  [0.3434996008872986] \n",
            "\n",
            "The loss value is :  [0.3434996008872986] \n",
            "\n",
            "This is group: 1969.0 and this is first subgraph \n",
            "Target node: 14937\n",
            "1-hop neighbors of A: {14940, 14941}\n",
            "2-hop neighbors of A: {14944}\n",
            "Out of neighborhood of A: {14938, 14939, 14942, 14943}\n",
            "The loss value is :  [0.5371577739715576] \n",
            "\n",
            "The loss value is :  [0.5371577739715576] \n",
            "\n",
            "This is group: 1970.0 and this is first subgraph \n",
            "Target node: 14941\n",
            "1-hop neighbors of A: {14945}\n",
            "2-hop neighbors of A: {14947}\n",
            "Out of neighborhood of A: {14942, 14943, 14944, 14946, 14948, 14949, 14950, 14951, 14952, 14953, 14954, 14955, 14956, 14957, 14958, 14959, 14960, 14961, 14962, 14963, 14964, 14965, 14966, 14967, 14968, 14969, 14970}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1971.0 and this is first subgraph \n",
            "Target node: 14969\n",
            "1-hop neighbors of A: {14970, 14971, 14972}\n",
            "2-hop neighbors of A: {14973, 14974}\n",
            "Out of neighborhood of A: {14975}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1972.0 and this is first subgraph \n",
            "Target node: 14975\n",
            "1-hop neighbors of A: {14976, 14978}\n",
            "2-hop neighbors of A: {14977, 14979, 14980}\n",
            "Out of neighborhood of A: set()\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1973.0 and this is first subgraph \n",
            "Target node: 14981\n",
            "1-hop neighbors of A: {14986, 14987, 14982}\n",
            "2-hop neighbors of A: {14988}\n",
            "Out of neighborhood of A: {14984, 14985, 14983}\n",
            "The loss value is :  [0.5699244141578674] \n",
            "\n",
            "The loss value is :  [0.5699244141578674] \n",
            "\n",
            "This is group: 1974.0 and this is first subgraph \n",
            "Target node: 14982\n",
            "1-hop neighbors of A: {14992, 14988}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {14983, 14984, 14985, 14986, 14989, 14990, 14991}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1975.0 and this is first subgraph \n",
            "Target node: 14990\n",
            "1-hop neighbors of A: {14991}\n",
            "2-hop neighbors of A: {14992}\n",
            "Out of neighborhood of A: {14993, 14994, 14995, 14996, 14997}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1976.0 and this is first subgraph \n",
            "Target node: 14996\n",
            "1-hop neighbors of A: {14998, 14999}\n",
            "2-hop neighbors of A: {15001, 15002}\n",
            "Out of neighborhood of A: {15000, 14997}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1977.0 and this is first subgraph \n",
            "Target node: 14999\n",
            "1-hop neighbors of A: {15001}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {15008, 15003, 15004, 15005, 15006, 15007}\n",
            "The loss value is :  [0.38170066475868225] \n",
            "\n",
            "The loss value is :  [0.38170066475868225] \n",
            "\n",
            "This is group: 1978.0 and this is first subgraph \n",
            "Target node: 15006\n",
            "1-hop neighbors of A: {15007}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {15008, 15009, 15010, 15011, 15012, 15013, 15014, 15015, 15016, 15017, 15018, 15019, 15020, 15021, 15022, 15023, 15024}\n",
            "The loss value is :  [0.5009887218475342] \n",
            "\n",
            "The loss value is :  [0.5009887218475342] \n",
            "\n",
            "This is group: 1979.0 and this is first subgraph \n",
            "Target node: 15022\n",
            "1-hop neighbors of A: {15025, 15026, 15029}\n",
            "2-hop neighbors of A: {15024, 15027, 15028}\n",
            "Out of neighborhood of A: {15023, 15030, 15031, 15032, 15033, 15034, 15035, 15036, 15037, 15038}\n",
            "The loss value is :  [0.645368754863739] \n",
            "\n",
            "The loss value is :  [0.645368754863739] \n",
            "\n",
            "This is group: 1980.0 and this is first subgraph \n",
            "Target node: 15034\n",
            "1-hop neighbors of A: {15041}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {15040, 15042, 15043, 15044, 15036, 15037, 15039}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1981.0 and this is first subgraph \n",
            "Target node: 15043\n",
            "1-hop neighbors of A: {15048}\n",
            "2-hop neighbors of A: {15051, 15047}\n",
            "Out of neighborhood of A: {15044, 15045, 15046, 15049, 15050, 15052, 15053}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1982.0 and this is first subgraph \n",
            "Target node: 15054\n",
            "1-hop neighbors of A: {15055}\n",
            "2-hop neighbors of A: {15056, 15057}\n",
            "Out of neighborhood of A: {15058}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1983.0 and this is first subgraph \n",
            "Target node: 15056\n",
            "1-hop neighbors of A: {15058}\n",
            "2-hop neighbors of A: {15060, 15061}\n",
            "Out of neighborhood of A: {15057, 15059, 15062}\n",
            "The loss value is :  [0.4772564172744751] \n",
            "\n",
            "The loss value is :  [0.4772564172744751] \n",
            "\n",
            "This is group: 1984.0 and this is first subgraph \n",
            "Target node: 15063\n",
            "1-hop neighbors of A: {15064}\n",
            "2-hop neighbors of A: {15065, 15066}\n",
            "Out of neighborhood of A: {15067, 15068, 15069}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1985.0 and this is first subgraph \n",
            "Target node: 15067\n",
            "1-hop neighbors of A: {15069}\n",
            "2-hop neighbors of A: {15071}\n",
            "Out of neighborhood of A: {15068, 15070, 15072, 15073, 15074, 15075, 15076, 15077, 15078, 15079, 15080, 15081, 15082, 15083, 15084, 15085, 15086, 15087, 15088, 15089, 15090, 15091, 15092, 15093, 15094, 15095, 15096, 15097, 15098}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1986.0 and this is first subgraph \n",
            "Target node: 15094\n",
            "1-hop neighbors of A: {15099}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {15104, 15095, 15100, 15101, 15102, 15103}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1987.0 and this is first subgraph \n",
            "Target node: 15102\n",
            "1-hop neighbors of A: {15104}\n",
            "2-hop neighbors of A: {15105}\n",
            "Out of neighborhood of A: {15106, 15107, 15108, 15109}\n",
            "The loss value is :  [0.75069260597229] \n",
            "\n",
            "The loss value is :  [0.75069260597229] \n",
            "\n",
            "This is group: 1988.0 and this is first subgraph \n",
            "Target node: 15097\n",
            "1-hop neighbors of A: {15113}\n",
            "2-hop neighbors of A: {15114}\n",
            "Out of neighborhood of A: {15104, 15108, 15109, 15110, 15111, 15112}\n",
            "The loss value is :  [0.4947390854358673] \n",
            "\n",
            "The loss value is :  [0.4947390854358673] \n",
            "\n",
            "This is group: 1989.0 and this is first subgraph \n",
            "Target node: 15113\n",
            "1-hop neighbors of A: {15114}\n",
            "2-hop neighbors of A: {15115, 15116}\n",
            "Out of neighborhood of A: {15120, 15117, 15118, 15119}\n",
            "The loss value is :  [0.6530095338821411] \n",
            "\n",
            "The loss value is :  [0.6530095338821411] \n",
            "\n",
            "This is group: 1990.0 and this is first subgraph \n",
            "Target node: 15098\n",
            "1-hop neighbors of A: {15125}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {15116, 15118, 15120, 15121, 15122, 15123, 15124}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1991.0 and this is first subgraph \n",
            "Target node: 15098\n",
            "1-hop neighbors of A: {15125}\n",
            "2-hop neighbors of A: {15126}\n",
            "Out of neighborhood of A: {15128, 15129, 15127}\n",
            "The loss value is :  [0.48344728350639343] \n",
            "\n",
            "The loss value is :  [0.48344728350639343] \n",
            "\n",
            "This is group: 1992.0 and this is first subgraph \n",
            "Target node: 15094\n",
            "1-hop neighbors of A: {15138}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {15127, 15128, 15129, 15130, 15131, 15132, 15133, 15134, 15135, 15136, 15137, 15139, 15140, 15141, 15142}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1993.0 and this is first subgraph \n",
            "Target node: 15134\n",
            "1-hop neighbors of A: {15146}\n",
            "2-hop neighbors of A: {15147}\n",
            "Out of neighborhood of A: {15136, 15141, 15143, 15144, 15145}\n",
            "The loss value is :  [0.3213787376880646] \n",
            "\n",
            "The loss value is :  [0.3213787376880646] \n",
            "\n",
            "This is group: 1994.0 and this is first subgraph \n",
            "Target node: 15099\n",
            "1-hop neighbors of A: {15148}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {15137, 15146, 15147, 15149, 15150, 15126}\n",
            "The loss value is :  [0.5122633576393127] \n",
            "\n",
            "The loss value is :  [0.5122633576393127] \n",
            "\n",
            "This is group: 1995.0 and this is first subgraph \n",
            "Target node: 15098\n",
            "1-hop neighbors of A: {15099}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {15110, 15112, 15122, 15124, 15132, 15135, 15138, 15139, 15144, 15146, 15148, 15151, 15152, 15153, 15154, 15155, 15156, 15157, 15158, 15159, 15160, 15161}\n",
            "The loss value is :  [0.38646137714385986] \n",
            "\n",
            "The loss value is :  [0.38646137714385986] \n",
            "\n",
            "This is group: 1996.0 and this is first subgraph \n",
            "Target node: 15141\n",
            "1-hop neighbors of A: {15150}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {15159, 15161, 15162, 15163, 15164, 15165, 15166}\n",
            "The loss value is :  [0.9346916675567627] \n",
            "\n",
            "The loss value is :  [0.9346916675567627] \n",
            "\n",
            "This is group: 1997.0 and this is first subgraph \n",
            "Target node: 15162\n",
            "1-hop neighbors of A: {15167}\n",
            "2-hop neighbors of A: {15168}\n",
            "Out of neighborhood of A: {15169, 15170, 15171, 15172, 15163, 15165}\n",
            "The loss value is :  [0.5669385194778442] \n",
            "\n",
            "The loss value is :  [0.5669385194778442] \n",
            "\n",
            "This is group: 1998.0 and this is first subgraph \n",
            "Target node: 15165\n",
            "1-hop neighbors of A: {15166}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {15170, 15172, 15173, 15174, 15175, 15176, 15177, 15178, 15179, 15180, 15181, 15182, 15183, 15184, 15185, 15186, 15187, 15188, 15189, 15190, 15191}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "This is group: 1999.0 and this is first subgraph \n",
            "Target node: 15188\n",
            "1-hop neighbors of A: {15192}\n",
            "2-hop neighbors of A: {15195}\n",
            "Out of neighborhood of A: {15189, 15190, 15191, 15193, 15194, 15196, 15197, 15198, 15199, 15200, 15201, 15202, 15203, 15204, 15205, 15206, 15207, 15208, 15209, 15210, 15211, 15212, 15213, 15214, 15215, 15216, 15217, 15218, 15219, 15220, 15221, 15222, 15223, 15224, 15225, 15226, 15227}\n",
            "The loss value is :  [0.7012285590171814] \n",
            "\n",
            "The loss value is :  [0.7012285590171814] \n",
            "\n",
            "This is group: 2000.0 and this is first subgraph \n",
            "Target node: 15211\n",
            "1-hop neighbors of A: {15212}\n",
            "2-hop neighbors of A: set()\n",
            "Out of neighborhood of A: {15213, 15217, 15218, 15219, 15228, 15229, 15230, 15231}\n",
            "The loss value is :  [nan] \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "[[0.4384535849094391], [0.24970783293247223], [0.5426312685012817], [0.3930266201496124], [0.45893755555152893], [1.0822346210479736], [0.5760793685913086], [0.3095865845680237], [0.22271445393562317], [0.3782143294811249], [0.6818577647209167], [0.2659226953983307], [0.6266592741012573], [1.4143539667129517], [0.39636462926864624], [0.5736991167068481], [0.29669758677482605], [0.9485558271408081], [0.6360437870025635], [0.6284437775611877], [1.0107964277267456], [0.5043456554412842], [0.3253849446773529], [0.5761686563491821], [0.38670963048934937], [0.4774159789085388], [0.5979189276695251], [0.616706132888794], [0.5968492031097412], [0.4828335642814636], [0.5964392423629761], [0.24879160523414612], [0.4723295271396637], [0.5820190906524658], [0.9535142779350281], [0.5575777888298035], [0.40721410512924194], [0.23048502206802368], [0.5837128162384033], [0.1442083716392517], [0.1090461015701294], [0.5373488664627075], [0.7656862139701843], [0.3905046582221985], [0.6676030158996582], [0.42060190439224243], [0.3701157569885254], [0.5480270385742188], [0.4710827171802521], [0.6830068826675415], [0.9682801365852356], [0.5213372111320496], [0.3927154541015625], [0.5340337157249451], [0.5484161376953125], [0.6587920188903809], [0.966952919960022], [0.6494289040565491], [0.4028679132461548], [0.3474275767803192], [0.15669605135917664], [0.86107337474823], [1.0497857332229614], [0.3004809021949768], [1.1556150913238525], [0.6133192181587219], [1.1752521991729736], [0.4096938371658325], [0.5594226121902466], [0.2167670875787735], [0.8291085362434387], [0.478127658367157], [0.8522660732269287], [0.6182512044906616], [1.0278116464614868], [0.8385053277015686], [0.601555347442627], [0.34210535883903503], [0.4341171979904175], [0.5501145720481873], [0.4568154215812683], [0.7411467432975769], [0.5097537636756897], [0.3508399426937103], [0.6211392879486084], [0.38625168800354004], [0.4516974687576294], [0.46585750579833984], [0.3896123766899109], [0.39857423305511475], [0.4028679132461548], [0.8221941590309143], [0.2948707342147827], [0.27453166246414185], [0.1808137744665146], [0.6685837507247925], [0.49889281392097473], [0.6655746698379517], [0.37911203503608704], [0.4542900025844574], [0.5544005632400513], [0.7264772057533264], [0.9333204030990601], [0.5840427875518799], [0.43103837966918945], [0.7389087677001953], [0.2990688681602478], [0.8230258822441101], [0.44185322523117065], [0.5070579051971436], [0.3803054988384247], [1.4994171857833862], [0.2267197221517563], [0.5291156768798828], [0.6592744588851929], [0.34097546339035034], [0.7710525989532471], [0.46588051319122314], [0.45426082611083984], [0.35068005323410034], [0.3951975405216217], [0.5913504958152771], [0.32039716839790344], [0.3944147825241089], [1.068487286567688], [0.47889673709869385], [0.3447225093841553], [0.6316583156585693], [0.31228184700012207], [0.4279576241970062], [0.46870535612106323], [0.5486738085746765], [0.36071285605430603], [0.1577516347169876], [0.6699838638305664], [0.4187954068183899], [0.2733186185359955], [0.798857569694519], [0.49645060300827026], [0.3229026794433594], [0.50029456615448], [0.287250280380249], [0.9071444272994995], [0.6518148183822632], [0.1308695524930954], [1.070968747138977], [0.48140275478363037], [0.4731717109680176], [0.5439803600311279], [0.5881837010383606], [0.5691127777099609], [0.32403427362442017], [0.24396997690200806], [0.42818379402160645], [0.2602739632129669], [1.1160322427749634], [0.9891633987426758], [0.23866157233715057], [0.30310550332069397], [0.7157953977584839], [0.06771008670330048], [0.8288221955299377], [0.22469767928123474], [0.7529869079589844], [0.27465713024139404], [0.3077930212020874], [0.1915874034166336], [0.41138291358947754], [0.5046725273132324], [0.3963252305984497], [0.9138500690460205], [0.6669307947158813], [0.37336742877960205], [0.2745964527130127], [0.528727650642395], [0.47158780694007874], [0.44706568121910095], [0.4235137701034546], [0.3124275803565979], [0.5680983066558838], [0.2921227216720581], [0.45887821912765503], [0.618288516998291], [0.32940784096717834], [1.0261646509170532], [0.2110300064086914], [0.9910597801208496], [0.5337339043617249], [0.8658998012542725], [0.4972708821296692], [0.3014248013496399], [0.9181009531021118], [0.43523797392845154], [0.6977680325508118], [0.5364083647727966], [0.49003806710243225], [0.200677752494812], [0.5667840838432312], [0.3328368663787842], [0.5089501738548279], [0.3292334973812103], [0.8861428499221802], [0.34454023838043213], [0.5418884754180908], [0.41466665267944336], [0.7304484844207764], [0.7225749492645264], [0.5916739106178284], [0.4194357097148895], [0.9648271799087524], [0.5032205581665039], [0.20341818034648895], [0.47776657342910767], [0.8937350511550903], [0.3112618029117584], [1.2759923934936523], [0.3075171709060669], [0.4133530259132385], [0.5623539686203003], [0.5166750550270081], [0.4643828272819519], [0.3450331389904022], [0.4578993320465088], [1.2739217281341553], [0.5625712871551514], [0.5702372193336487], [0.23972104489803314], [0.2458958923816681], [0.29448580741882324], [0.4720555543899536], [0.9034355878829956], [0.7007167339324951], [0.44005876779556274], [0.8604772090911865], [0.7159523963928223], [0.4336576759815216], [0.6481952667236328], [0.5525033473968506], [0.35421597957611084], [0.31171107292175293], [0.6933221817016602], [0.3524523079395294], [1.0131679773330688], [0.548701286315918], [0.8676619529724121], [0.4507221579551697], [0.9070408344268799], [0.6861251592636108], [0.49087879061698914], [0.4889935255050659], [0.5607627034187317], [0.6833341121673584], [0.934404730796814], [0.5388073921203613], [0.4803503751754761], [0.32825037837028503], [0.5365636944770813], [0.34811028838157654], [0.5310106873512268], [0.8417433500289917], [0.2594056725502014], [0.31828874349594116], [0.2903873026371002], [0.5268597602844238], [0.33638739585876465], [0.8091163039207458], [0.0014444217085838318], [0.2561635971069336], [0.47739675641059875], [0.4101099967956543], [0.9386788606643677], [0.3614999055862427], [0.5678824186325073], [0.17582765221595764], [0.5663394331932068], [0.5943782925605774], [0.20411784946918488], [0.4978140592575073], [0.7495623826980591], [1.0566109418869019], [1.3940966129302979], [0.7327179908752441], [0.43009963631629944], [0.3396075963973999], [0.12280939519405365], [0.3825416564941406], [0.23951934278011322], [0.5374385118484497], [0.4534893333911896], [0.7865549325942993], [0.29619330167770386], [0.5074079632759094], [0.3963330388069153], [0.770553708076477], [0.5467973947525024], [0.6009748578071594], [1.0253890752792358], [0.7585893869400024], [0.860407829284668], [0.2611989378929138], [1.314563512802124], [0.8840924501419067], [0.6947281360626221], [0.6570240259170532], [0.2780366837978363], [0.32443320751190186], [0.4930882453918457], [0.30211883783340454], [0.20618312060832977], [0.5342190265655518], [0.5696083307266235], [0.3143575191497803], [0.3986220955848694], [0.6169620752334595], [0.4794485867023468], [0.2950347661972046], [0.49117010831832886], [0.5027768611907959], [0.8638225197792053], [0.28994667530059814], [0.36474600434303284], [0.9828429222106934], [0.2653715908527374], [0.37130165100097656], [0.516244649887085], [0.43009716272354126], [0.45836830139160156], [0.3801147937774658], [0.5212481617927551], [1.2376644611358643], [0.6805543899536133], [0.41402122378349304], [0.521987795829773], [0.45631134510040283], [0.7267130613327026], [0.6311735510826111], [0.6353773474693298], [0.3764301836490631], [0.7233906984329224], [0.5233292579650879], [0.35157546401023865], [0.60652095079422], [0.5914269685745239], [1.2253317832946777], [0.4810733199119568], [0.7135330438613892], [0.6370072364807129], [0.39417511224746704], [0.8083889484405518], [0.5064123868942261], [0.3379001319408417], [0.7891113758087158], [0.7453295588493347], [0.2992270588874817], [0.6991276741027832], [0.4072911739349365], [0.28953999280929565], [0.6075649857521057], [0.8253684043884277], [1.168992519378662], [0.824408769607544], [0.40222272276878357], [1.0424439907073975], [0.2126224935054779], [0.7017216682434082], [0.42169952392578125], [0.6355385780334473], [0.6588618755340576], [0.2796918451786041], [0.6944619417190552], [0.28614410758018494], [0.9279891848564148], [0.5069709420204163], [0.4738636016845703], [0.38657957315444946], [0.6017070412635803], [0.5694313049316406], [0.144365131855011], [0.5312255024909973], [0.48657822608947754], [0.33430248498916626], [0.3499097228050232], [0.5529224276542664], [0.37777841091156006], [0.8987295627593994], [0.45654693245887756], [0.5204247832298279], [0.663098931312561], [0.382600873708725], [0.6431545615196228], [0.37672895193099976], [1.1052744388580322], [0.9769680500030518], [0.524432897567749], [0.5030989050865173], [0.3341373801231384], [0.20322735607624054], [0.3272130489349365], [0.6693006753921509], [0.6686714291572571], [0.29863858222961426], [0.42262160778045654], [1.1707547903060913], [0.26102587580680847], [1.1786777973175049], [0.5358407497406006], [0.27175211906433105], [0.4578031003475189], [0.3083931803703308], [0.29995617270469666], [0.5886929035186768], [0.4023496210575104], [0.7843219041824341], [0.4288427531719208], [1.0282334089279175], [0.4927859902381897], [0.557105302810669], [0.40050581097602844], [0.43493330478668213], [0.4635505676269531], [0.3606805205345154], [0.5839760899543762], [1.0287976264953613], [0.2848820090293884], [0.37341660261154175], [0.5361652374267578], [0.3178038001060486], [0.707565188407898], [0.7350284457206726], [1.440328598022461], [0.23391877114772797], [1.0239672660827637], [0.3149101138114929], [0.6011778116226196], [0.7872260808944702], [0.7272112369537354], [0.6229832172393799], [0.28593766689300537], [0.604735255241394], [0.8226524591445923], [0.927848219871521], [0.44255056977272034], [0.7139859199523926], [0.38696616888046265], [0.7537761330604553], [0.8400344252586365], [0.4612647294998169], [0.18137028813362122], [0.6753072738647461], [0.7799314260482788], [0.32544663548469543], [0.6772905588150024], [0.4056990444660187], [0.42588740587234497], [0.4361908733844757], [0.4550257921218872], [0.2356720268726349], [0.5787734985351562], [0.35754936933517456], [1.0714540481567383], [0.46585026383399963], [0.4960765242576599], [0.33517932891845703], [0.6342803239822388], [0.63438481092453], [0.4577384293079376], [0.7338851094245911], [0.7648123502731323], [0.2393145114183426], [0.40613555908203125], [0.43390172719955444], [0.24001352488994598], [0.6088351011276245], [1.6907145977020264], [0.7420448660850525], [0.515927791595459], [0.7516411542892456], [0.35139885544776917], [0.5235472917556763], [0.5249966382980347], [0.46482324600219727], [0.21534448862075806], [0.4530446529388428], [0.38500523567199707], [0.398063987493515], [0.6059709787368774], [0.3320026695728302], [0.5041232705116272], [0.5172056555747986], [0.7362527847290039], [0.49327200651168823], [0.5641981363296509], [0.20493504405021667], [0.1528397500514984], [0.4067363739013672], [0.21369986236095428], [0.4050869941711426], [0.804029107093811], [0.4917586147785187], [0.977077066898346], [0.3084677457809448], [0.552020788192749], [0.28786274790763855], [0.4480136036872864], [0.32974451780319214], [0.5009015202522278], [0.6089780926704407], [0.3873324990272522], [0.6267269849777222], [0.5969115495681763], [0.700882077217102], [0.12920211255550385], [0.5886243581771851], [0.48467186093330383], [0.685467004776001], [0.6863088607788086], [0.32124263048171997], [0.20299799740314484], [0.36337780952453613], [0.9952717423439026], [0.5702483654022217], [0.3597140312194824], [0.19312547147274017], [0.39343491196632385], [1.3027713298797607], [0.19297660887241364], [0.5398603081703186], [0.9125727415084839], [0.3089604377746582], [0.9373282194137573], [0.30118197202682495], [0.2588925063610077], [0.7270883917808533], [0.41825756430625916], [0.5285339951515198], [0.41741105914115906], [0.2477697730064392], [0.8033663034439087], [0.5620554685592651], [0.7692161798477173], [0.47136518359184265], [0.46390822529792786], [0.5407832264900208], [0.48132559657096863], [0.4381904602050781], [0.3603020906448364], [0.38566911220550537], [0.8841297626495361], [0.6551858186721802], [0.23226535320281982], [0.35729336738586426], [0.3521420359611511], [0.5062595009803772], [0.5347841382026672], [0.5078549385070801], [0.37281063199043274], [0.31314384937286377], [0.3083353042602539], [0.44403135776519775], [0.4150998592376709], [0.6972856521606445], [0.6063781976699829], [0.38143232464790344], [0.6330263018608093], [0.6125364303588867], [0.5230268239974976], [0.48130083084106445], [0.6113561987876892], [0.6016860604286194], [0.44588619470596313], [0.34260109066963196], [0.6570607423782349], [0.5231240391731262], [1.3468976020812988], [0.40541955828666687], [0.6878694295883179], [0.709545373916626], [0.9293473362922668], [0.18707722425460815], [0.3871971368789673], [0.5686781406402588], [1.2669299840927124], [0.356513112783432], [0.4759463667869568], [0.4176676869392395], [0.7691589593887329], [0.3642209470272064], [0.4977405071258545], [0.41638725996017456], [0.33260786533355713], [0.11379464715719223], [0.6398848295211792], [0.6805907487869263], [0.4150589108467102], [0.610366702079773], [0.464435338973999], [0.41401565074920654], [0.3106294274330139], [0.7582007050514221], [0.43206366896629333], [0.3522481918334961], [0.4533202648162842], [0.4022040367126465], [0.5150436758995056], [0.5256489515304565], [0.2995849549770355], [0.17161591351032257], [0.21915394067764282], [0.8921216726303101], [0.4620647430419922], [0.4662458896636963], [0.4095383882522583], [0.21608658134937286], [0.30602753162384033], [0.6446623802185059], [0.6222742199897766], [0.8080060482025146], [0.5390739440917969], [1.5890392065048218], [0.14201484620571136], [0.7328300476074219], [0.8425027132034302], [0.882285475730896], [0.29785892367362976], [0.6054278016090393], [0.45225387811660767], [0.5261384844779968], [0.40132325887680054], [0.8576124906539917], [1.1175262928009033], [0.41623568534851074], [0.5686450600624084], [0.48249176144599915], [0.6862502098083496], [1.430558443069458], [0.5435199737548828], [0.4332836866378784], [0.7825769782066345], [0.6960026025772095], [0.40097635984420776], [0.38557711243629456], [0.8545907735824585], [0.2909150719642639], [0.36801934242248535], [0.71479332447052], [0.7940598130226135], [0.45240768790245056], [0.5406437516212463], [0.8009606599807739], [0.44264382123947144], [0.3907293975353241], [0.6144784688949585], [0.8005239963531494], [0.3540853261947632], [1.1745500564575195], [0.38595953583717346], [0.5633116960525513], [0.4729000926017761], [0.46554234623908997], [0.7328455448150635], [0.9535478353500366], [0.35955506563186646], [0.7255133986473083], [0.22155773639678955], [0.3365226984024048], [0.4923698902130127], [0.45123291015625], [0.3782508969306946], [0.6895468235015869], [0.3191841244697571], [1.26212739944458], [0.6801114082336426], [0.6071106195449829], [0.7570124864578247], [0.5335813164710999], [0.9948017597198486], [0.3883901834487915], [1.0122853517532349], [0.6393302083015442], [0.4877414107322693], [0.6046231389045715], [0.9833778142929077], [0.3927050232887268], [0.3985166549682617], [0.5756821632385254], [0.6938536167144775], [0.6022270321846008], [0.6292910575866699], [0.4211144745349884], [0.3739054501056671], [0.7042469382286072], [0.29419079422950745], [0.5869759917259216], [0.3795557916164398], [0.371296226978302], [0.3518812656402588], [0.47033604979515076], [0.42280933260917664], [0.2757154107093811], [0.5156993269920349], [0.28048503398895264], [0.833888828754425], [0.17880068719387054], [0.6956726908683777], [0.6176653504371643], [0.4678066670894623], [0.5924223065376282], [1.2464032173156738], [0.6844038963317871], [0.2842240631580353], [0.5162292718887329], [0.7115461826324463], [0.6632147431373596], [0.8492746353149414], [0.32091519236564636], [0.5716951489448547], [0.41664522886276245], [0.74985271692276], [0.4284542202949524], [1.1705670356750488], [0.9655100107192993], [0.7250915169715881], [0.34970802068710327], [1.2022862434387207], [0.3657470941543579], [0.33310022950172424], [0.4111230969429016], [0.9997555017471313], [0.5394589900970459], [0.9705801010131836], [0.6942435503005981], [0.7430675029754639], [0.5394458174705505], [0.2807956337928772], [0.9211868047714233], [0.37042707204818726], [0.8534949421882629], [0.38853245973587036], [0.2972869873046875], [0.5961714386940002], [1.016589641571045], [0.6360808610916138], [0.4237819314002991], [0.35331785678863525], [0.7572596073150635], [0.3121485114097595], [0.7645725011825562], [0.69935142993927], [0.9566778540611267], [0.9170649647712708], [0.6957478523254395], [0.5273069143295288], [0.298979252576828], [0.47877237200737], [0.4531092047691345], [0.2901590168476105], [0.5474973320960999], [0.7595887184143066], [0.782617449760437], [0.4483141005039215], [1.0485247373580933], [1.2816004753112793], [0.5488731861114502], [0.22515304386615753], [0.2819942235946655], [0.48615285754203796], [0.26342055201530457], [0.3878481686115265], [0.4989898204803467], [0.5768148899078369], [0.2892671823501587], [0.2640497386455536], [0.5825565457344055], [0.781345009803772], [0.5046219825744629], [0.20804496109485626], [0.28609949350357056], [0.5704163908958435], [1.0569618940353394], [0.5463386178016663], [0.5802364349365234], [0.5496959090232849], [0.597266674041748], [1.4731383323669434], [0.33102118968963623], [0.7165900468826294], [0.559980571269989], [0.3493131697177887], [0.5353091955184937], [0.5654123425483704], [0.6678757071495056], [0.41733038425445557], [0.5635221004486084], [0.6146576404571533], [0.46057048439979553], [0.874178946018219], [0.4987049698829651], [0.47399988770484924], [0.5560839176177979], [0.4189555048942566], [0.49682506918907166], [0.45674896240234375], [0.4443758726119995], [0.4658746123313904], [0.5230194330215454], [0.35553890466690063], [0.4510223865509033], [0.4443405270576477], [0.66070556640625], [0.4231348931789398], [0.5231825709342957], [0.39900144934654236], [0.6222559213638306], [0.4320249557495117], [0.7371317744255066], [0.40542837977409363], [1.0545767545700073], [0.7248049974441528], [1.0445870161056519], [0.33126360177993774], [0.9000295996665955], [0.6206889748573303], [0.2215808629989624], [0.39169344305992126], [0.49335381388664246], [0.4457487463951111], [0.4379982352256775], [0.8134589195251465], [0.4272876977920532], [0.4274950921535492], [0.692564070224762], [0.660405158996582], [0.638478696346283], [0.7378987073898315], [0.5103705525398254], [0.7424955368041992], [0.4852176904678345], [0.5262861251831055], [0.7287817597389221], [0.45752573013305664], [1.1462711095809937], [0.3553928732872009], [0.5419064164161682], [0.7830026149749756], [0.1877630054950714], [0.27376675605773926], [0.34554797410964966], [0.5214986205101013], [0.7951940298080444], [0.7238110899925232], [0.3748164176940918], [0.31959351897239685], [0.3967057466506958], [0.6515189409255981], [1.253796935081482], [0.25961434841156006], [0.518990159034729], [0.4021719694137573], [0.46945616602897644], [0.7559961080551147], [0.47123411297798157], [0.7825357913970947], [0.36662349104881287], [0.5135181546211243], [0.29499351978302], [0.47616440057754517], [0.6793315410614014], [0.6221948862075806], [0.6353381276130676], [0.1040063127875328], [0.5225290060043335], [0.44078388810157776], [0.22506040334701538], [0.15972697734832764], [0.4754524827003479], [0.4890812337398529], [0.05017991364002228], [0.6778870820999146], [0.47677284479141235], [0.6195895671844482], [0.28579723834991455], [0.4223172962665558], [0.19060438871383667], [0.4837157726287842], [0.5777862071990967], [0.27192485332489014], [0.40253591537475586], [0.3077230453491211], [0.25701308250427246], [0.3374776840209961], [0.5222201347351074], [0.5022913217544556], [0.21390756964683533], [0.4755218029022217], [1.3059335947036743], [1.5025906562805176], [0.9963334798812866], [0.5090025067329407], [0.16668537259101868], [0.687029242515564], [0.7342569828033447], [0.6554219722747803], [0.3548816740512848], [0.47766241431236267], [0.7146183848381042], [0.7264307737350464], [0.5433147549629211], [0.49485939741134644], [0.8560314774513245], [0.7456157803535461], [1.1313467025756836], [0.3103446364402771], [0.4133668839931488], [0.6985280513763428], [0.5450726747512817], [0.2692687213420868], [0.4052242934703827], [0.6861652135848999], [0.45185568928718567], [0.5998644828796387], [1.2313131093978882], [0.4869402050971985], [0.5601725578308105], [0.49882763624191284], [0.367351233959198], [0.32084324955940247], [0.4382416903972626], [0.7719566822052002], [0.5983602404594421], [0.3633899688720703], [0.32507503032684326], [0.5873939990997314], [1.3490142822265625], [0.7192723155021667], [0.5751068592071533], [0.37541866302490234], [0.8449453711509705], [0.9919193983078003], [0.8420804738998413], [0.514738917350769], [0.6375793218612671], [0.6568071842193604], [0.3300324082374573], [0.5600342750549316], [0.34701699018478394], [0.5822521448135376], [0.6100369095802307], [0.47604644298553467], [0.459816575050354], [0.1551642119884491], [0.5933144092559814], [0.4429572820663452], [0.5523034334182739], [0.40977758169174194], [0.32846587896347046], [0.9076851606369019], [0.40765589475631714], [0.4363589286804199], [0.5991610884666443], [0.5043473243713379], [0.43301284313201904], [0.8558772802352905], [0.7075609564781189], [0.5881454944610596], [1.342825174331665], [0.461683452129364], [0.5574827194213867], [0.052409637719392776], [0.574062705039978], [1.1921358108520508], [0.5680406093597412], [0.7138785123825073], [0.253366619348526], [0.7557320594787598], [0.37200045585632324], [0.7579895257949829], [0.44241631031036377], [0.20408722758293152], [0.48849958181381226], [0.4737640619277954], [0.45508554577827454], [1.1936722993850708], [0.7440568208694458], [0.4867444634437561], [0.7240493893623352], [0.3669039309024811], [0.5479711890220642], [0.4040273427963257], [0.9051263332366943], [0.28766492009162903], [0.473442405462265], [0.8406578302383423], [0.30489280819892883], [0.4919353723526001], [0.2381487637758255], [0.7195755243301392], [0.32109907269477844], [0.4838162064552307], [0.7213890552520752], [0.7029678821563721], [1.10374116897583], [0.663500189781189], [0.6515889167785645], [0.7373954653739929], [0.575752317905426], [0.6909217834472656], [0.6309705376625061], [0.7926251888275146], [0.6332432627677917], [0.8475449681282043], [0.37637802958488464], [0.3321506381034851], [0.5055360794067383], [0.4078342914581299], [0.693584680557251], [0.6183843612670898], [0.32231760025024414], [0.733017086982727], [0.6211241483688354], [0.5576292276382446], [0.8244434595108032], [0.6779258251190186], [1.1001287698745728], [0.1365864872932434], [0.8220918774604797], [0.7173205018043518], [0.5516634583473206], [0.41654378175735474], [0.3537750840187073], [0.3260893225669861], [0.6299476027488708], [0.8256597518920898], [0.5605485439300537], [0.24470341205596924], [0.2598658800125122], [0.419518381357193], [0.7099826335906982], [0.48535680770874023], [0.3213866949081421], [0.5138757228851318], [0.5276581048965454], [0.7567590475082397], [0.7760609984397888], [0.41874051094055176], [1.2298705577850342], [0.39161521196365356], [0.6260797381401062], [0.3531181812286377], [0.5187622308731079], [0.5346116423606873], [0.4188256859779358], [0.41650742292404175], [0.6695467829704285], [0.7181321382522583], [0.6655707359313965], [0.7125619649887085], [0.7273187637329102], [0.3552640974521637], [0.6882644891738892], [0.32263392210006714], [0.34097978472709656], [0.27867504954338074], [0.9772560596466064], [0.45991575717926025], [0.8966230154037476], [0.6052781343460083], [0.6741799116134644], [0.3434996008872986], [0.5371577739715576], [0.5699244141578674], [0.38170066475868225], [0.5009887218475342], [0.645368754863739], [0.4772564172744751], [0.75069260597229], [0.4947390854358673], [0.6530095338821411], [0.48344728350639343], [0.3213787376880646], [0.5122633576393127], [0.38646137714385986], [0.9346916675567627], [0.5669385194778442], [0.7012285590171814]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time of the calculation was :  226.50465965270996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AIDS first graph results"
      ],
      "metadata": {
        "id": "LOy1osWboYkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import torch\n",
        "from itertools import product\n",
        "\n",
        "AIDS = \"/content/drive/MyDrive/data/AIDS/\"\n",
        "AIDS_df = pd.read_csv(AIDS + 'AIDS_A.txt', sep=',', header=None, names=['from', 'to'])\n",
        "AIDS_graph_indicator = pd.read_csv(AIDS + 'AIDS_graph_indicator.txt', header=None, names=['graph_id'])\n",
        "AIDS_node_labels = pd.read_csv(AIDS + 'AIDS_node_labels.txt', header=None, names=['node_label'])\n",
        "AIDS_df['graph_id'] = AIDS_graph_indicator\n",
        "AIDS_df['node_label'] = AIDS_node_labels\n",
        "\n",
        "grouped = AIDS_df.groupby('graph_id')\n",
        "AIDS_causal_graphs = {}\n",
        "for graph_id, group in grouped:\n",
        "    V = set(group['from']).union(set(group['to']))\n",
        "    edges = list(zip(group['from'], group['to'])) + list(zip(group['to'], group['from']))\n",
        "    AIDS_causal_graphs[graph_id] = CausalGraph(V=V, path=edges)\n",
        "\n",
        "cg = AIDS_causal_graphs[1.0]\n",
        "v_star, one_hop_neighbors, two_hop_neighbors, out_of_neighborhood = cg.categorize_neighbors(target_node=cg.sort()[0])\n",
        "print(f\"Target node: {v_star}\")\n",
        "print(f\"1-hop neighbors of A: {one_hop_neighbors}\")\n",
        "print(f\"2-hop neighbors of A: {two_hop_neighbors}\")\n",
        "print(f\"Out of neighborhood of A: {out_of_neighborhood}\")\n",
        "cg.plot()\n",
        "\n",
        "# hyperparameters\n",
        "num_epochs = 2\n",
        "learning_rates = [0.001, 0.002, 0.005,0.01]\n",
        "hidden_sizes = [32, 64, 128, 256]\n",
        "num_layers = [1, 2, 3, 4]\n",
        "lambdas = [0.01, 0.05, .1,.2,.3]\n",
        "hyperparameters = product(learning_rates, hidden_sizes, num_layers, lambdas)\n",
        "total_loss = []\n",
        "for i, hyperparams in enumerate(hyperparameters):\n",
        "    learning_rate, h_size, h_layers, lambdas = hyperparams\n",
        "    print(f'Training with learning rate: {learning_rate}, h_size: {h_size}, h_layers: {h_layers}, lambdas: {lambdas}','\\n')\n",
        "    causal_loss = train(cg, lambdas, learning_rate, h_size, h_layers, num_epochs)\n",
        "    total_loss.append(causal_loss)\n",
        "total_loss = [x for x in total_loss if not math.isnan(x[0])]\n",
        "print(total_loss)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(total_loss)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss over epochs')\n",
        "plt.savefig('AIDS NCM Loss over epochs.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PFXlaRxcom2t",
        "outputId": "c6f4e1d0-88e8-40db-ae88-721bdb4e42d7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target node: 1\n",
            "1-hop neighbors of A: {2, 6}\n",
            "2-hop neighbors of A: {3, 5, 12}\n",
            "Out of neighborhood of A: {4, 38, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 46, 21, 25, 27, 28}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with learning rate: 0.001, h_size: 32, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.31120193004608154] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.33159366250038147] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.5925123691558838] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.27462852001190186] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [0.8942347168922424] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.5320172905921936] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.22706890106201172] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.2684314548969269] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.6401252746582031] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.15568381547927856] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.2276994287967682] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.611995279788971] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.8238033056259155] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.2369401454925537] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.18101954460144043] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [1.9394466876983643] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.391545057296753] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.06479258835315704] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.4975491762161255] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.15149369835853577] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.5900501608848572] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [1.2668105363845825] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.327505350112915] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.3578571677207947] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [1.2696247100830078] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.2107308954000473] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [0.9549912214279175] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.1593656539916992] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.4731045663356781] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [0.8779774904251099] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.7838681936264038] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.10240457952022552] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.38332390785217285] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.5626495480537415] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.14158572256565094] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.24754685163497925] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.6212546229362488] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.8190393447875977] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.12610329687595367] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.8127274513244629] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.4319498836994171] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.3664930760860443] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [0.8141123652458191] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.2802839279174805] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.0998225212097168] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.26148808002471924] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.6064948439598083] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.605658769607544] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.26895859837532043] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.22999048233032227] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.5126909613609314] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.4315811395645142] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.9569873809814453] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.290626049041748] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.11851337552070618] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.5019563436508179] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.6987439393997192] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [0.892208993434906] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.38247644901275635] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.08894362300634384] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.30017751455307007] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.5732712149620056] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.08668702095746994] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.3003220856189728] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [1.2729132175445557] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [3.2621846199035645] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.7817749977111816] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.5328422784805298] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.09834708273410797] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.3229025602340698] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.10209701955318451] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [1.4499714374542236] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [2.1357102394104004] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.6788051128387451] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.7561041116714478] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.1375586986541748] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.40788567066192627] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [1.2482542991638184] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [1.1297118663787842] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.11308296769857407] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.25511759519577026] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [1.1427483558654785] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.26393160223960876] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [1.2194401025772095] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.5317251086235046] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [1.1191126108169556] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.5487572550773621] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.958268642425537] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.14377011358737946] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.30023279786109924] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.6350131034851074] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.0497409105300903] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.7302278280258179] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [1.1886252164840698] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.200649619102478] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.062242940068244934] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.12735658884048462] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.24817515909671783] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.9391337633132935] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [1.0666840076446533] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.39376065135002136] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.37849947810173035] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.921565592288971] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.540378451347351] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.19622549414634705] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.5431766510009766] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.0821208953857422] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.3670611381530762] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.6733746528625488] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [1.0604441165924072] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.10563461482524872] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.10914739966392517] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.27600836753845215] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.6045361757278442] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [0.9355518221855164] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.16620782017707825] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.29468387365341187] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [2.099379301071167] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.25739264488220215] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.0980382114648819] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.27161163091659546] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.5197159647941589] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [2.115373373031616] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.5499542951583862] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [0.8875570297241211] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.6593812704086304] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.18645772337913513] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [1.0306308269500732] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.5358461737632751] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [1.7545950412750244] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.649078607559204] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.41176244616508484] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.7226173281669617] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [0.9218497276306152] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [2.4544174671173096] \n",
            "\n",
            "[[0.31120193004608154], [0.33159366250038147], [1.5925123691558838], [0.27462852001190186], [0.8942347168922424], [0.5320172905921936], [0.22706890106201172], [0.2684314548969269], [1.6401252746582031], [0.15568381547927856], [0.2276994287967682], [0.611995279788971], [1.8238033056259155], [1.2369401454925537], [0.18101954460144043], [1.9394466876983643], [1.391545057296753], [0.06479258835315704], [1.4975491762161255], [0.15149369835853577], [0.5900501608848572], [1.2668105363845825], [1.327505350112915], [0.3578571677207947], [1.2696247100830078], [0.2107308954000473], [0.9549912214279175], [1.1593656539916992], [0.4731045663356781], [0.8779774904251099], [1.7838681936264038], [0.10240457952022552], [0.38332390785217285], [0.5626495480537415], [0.14158572256565094], [0.24754685163497925], [0.6212546229362488], [1.8190393447875977], [0.12610329687595367], [0.8127274513244629], [0.4319498836994171], [0.3664930760860443], [0.8141123652458191], [1.2802839279174805], [0.0998225212097168], [0.26148808002471924], [0.6064948439598083], [1.605658769607544], [0.26895859837532043], [0.22999048233032227], [0.5126909613609314], [1.4315811395645142], [1.9569873809814453], [1.290626049041748], [0.11851337552070618], [0.5019563436508179], [0.6987439393997192], [0.892208993434906], [0.38247644901275635], [0.08894362300634384], [0.30017751455307007], [0.5732712149620056], [0.08668702095746994], [0.3003220856189728], [1.2729132175445557], [3.2621846199035645], [0.7817749977111816], [1.5328422784805298], [0.09834708273410797], [1.3229025602340698], [0.10209701955318451], [1.4499714374542236], [2.1357102394104004], [1.6788051128387451], [1.7561041116714478], [0.1375586986541748], [0.40788567066192627], [1.2482542991638184], [1.1297118663787842], [0.11308296769857407], [0.25511759519577026], [1.1427483558654785], [0.26393160223960876], [1.2194401025772095], [0.5317251086235046], [1.1191126108169556], [0.5487572550773621], [1.958268642425537], [0.14377011358737946], [0.30023279786109924], [0.6350131034851074], [1.0497409105300903], [0.7302278280258179], [1.1886252164840698], [1.200649619102478], [0.062242940068244934], [0.12735658884048462], [0.24817515909671783], [0.9391337633132935], [1.0666840076446533], [0.39376065135002136], [0.37849947810173035], [0.921565592288971], [1.540378451347351], [0.19622549414634705], [1.5431766510009766], [1.0821208953857422], [1.3670611381530762], [0.6733746528625488], [1.0604441165924072], [0.10563461482524872], [0.10914739966392517], [0.27600836753845215], [0.6045361757278442], [0.9355518221855164], [0.16620782017707825], [0.29468387365341187], [2.099379301071167], [0.25739264488220215], [0.0980382114648819], [0.27161163091659546], [0.5197159647941589], [2.115373373031616], [0.5499542951583862], [0.8875570297241211], [1.6593812704086304], [0.18645772337913513], [1.0306308269500732], [0.5358461737632751], [1.7545950412750244], [1.649078607559204], [0.41176244616508484], [0.7226173281669617], [0.9218497276306152], [2.4544174671173096]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MUTAG first graph results"
      ],
      "metadata": {
        "id": "1AMoRKQkoket"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "import pandas as pd\n",
        "\n",
        "Mutagenicity = \"/content/drive/MyDrive/data/Mutagenicity/\"\n",
        "# Load the data\n",
        "Mutagenicity_df = pd.read_csv(Mutagenicity + 'Mutagenicity_A.txt', sep=',', header=None, names=['from', 'to'])\n",
        "Mutagenicity_graph_indicator = pd.read_csv(Mutagenicity + 'Mutagenicity_graph_indicator.txt', header=None, names=['graph_id'])\n",
        "Mutagenicity_node_labels = pd.read_csv(Mutagenicity + 'Mutagenicity_node_labels.txt', header=None, names=['node_label'])\n",
        "# print(Mutagenicity_edges_df,Mutagenicity_graph_indicator,Mutagenicity_node_labels)\n",
        "Mutagenicity_df['graph_id'] = Mutagenicity_graph_indicator\n",
        "Mutagenicity_df['node_label'] = Mutagenicity_node_labels\n",
        "\n",
        "# Group edges by graph id\n",
        "grouped = Mutagenicity_df.groupby('graph_id')\n",
        "# Dictionary to hold each graph\n",
        "Mutagenicity_causal_graphs = {}\n",
        "for graph_id, group in grouped:\n",
        "    # Create a set of vertices for each group\n",
        "    V = set(group['from']).union(set(group['to']))\n",
        "    # Create a list of edges for each group\n",
        "    edges = list(zip(group['from'], group['to'])) + list(zip(group['to'], group['from']))\n",
        "    # Create a CausalGraph for each group\n",
        "    Mutagenicity_causal_graphs[graph_id] = CausalGraph(V=V, path=edges)\n",
        "    # Mutagenicity_causal_graphs[graph_id].plot()\n",
        "\n",
        "cg = Mutagenicity_causal_graphs[1.0]\n",
        "v_star, one_hop_neighbors, two_hop_neighbors, out_of_neighborhood = cg.categorize_neighbors(target_node=cg.sort()[0])\n",
        "print(f\"Target node: {v_star}\")\n",
        "print(f\"1-hop neighbors of A: {one_hop_neighbors}\")\n",
        "print(f\"2-hop neighbors of A: {two_hop_neighbors}\")\n",
        "print(f\"Out of neighborhood of A: {out_of_neighborhood}\")\n",
        "cg.plot()\n",
        "\n",
        "# hyperparameters\n",
        "num_epochs = 2\n",
        "learning_rates = [0.001, 0.002, 0.005,0.01]\n",
        "hidden_sizes = [32, 64, 128, 256]\n",
        "num_layers = [1, 2, 3, 4]\n",
        "lambdas = [0.01, 0.05, .1,.2,.3]\n",
        "hyperparameters = product(learning_rates, hidden_sizes, num_layers, lambdas)\n",
        "total_loss = []\n",
        "for i, hyperparams in enumerate(hyperparameters):\n",
        "    learning_rate, h_size, h_layers, lambdas = hyperparams\n",
        "    print(f'Training with learning rate: {learning_rate}, h_size: {h_size}, h_layers: {h_layers}, lambdas: {lambdas}','\\n')\n",
        "    causal_loss = train(cg, lambdas, learning_rate, h_size, h_layers, num_epochs)\n",
        "    total_loss.append(causal_loss)\n",
        "total_loss = [x for x in total_loss if not math.isnan(x[0])]\n",
        "print(total_loss)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(total_loss)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss over epochs')\n",
        "plt.savefig('Mutag NCM Loss over epochs.png')\n",
        "plt.show()\n",
        "\n",
        "# GNNCausalExplanation(dataset, num_subgraph_limit, num_nodes_density, delta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "obGG43xgodQk",
        "outputId": "b031a31d-533b-413f-8552-651e205ea387"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target node: 1\n",
            "1-hop neighbors of A: {2, 3, 4}\n",
            "2-hop neighbors of A: {5, 6, 7, 8}\n",
            "Out of neighborhood of A: {9}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with learning rate: 0.001, h_size: 32, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [-0.00021845847368240356] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.4095853567123413] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [0.5391976237297058] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.4531581997871399] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.15607568621635437] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.348425030708313] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [1.2273609638214111] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.1781262755393982] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.13256236910820007] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.576390266418457] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 32, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.21859599649906158] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.6816893815994263] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [0.4898217022418976] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.517722249031067] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.5416715741157532] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.294535756111145] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [0.8453755974769592] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.9583600759506226] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.6598857641220093] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.33957141637802124] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [1.118286371231079] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.51220703125] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.44898390769958496] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [0.5057395696640015] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 64, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [0.6699783205986023] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.5415997505187988] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.28818511962890625] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.27082473039627075] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.402402400970459] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.3234034478664398] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [0.9548887014389038] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.2473804950714111] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.5817443132400513] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.3548800945281982] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.14328207075595856] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.5545912384986877] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [0.9295819401741028] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 128, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.20530807971954346] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.6694188117980957] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.16509076952934265] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [1.1984786987304688] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.0280414819717407] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.09333564341068268] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.12715815007686615] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.5009667873382568] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.001, h_size: 256, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.2897951900959015] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.0808466672897339] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.40442678332328796] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [0.8717116713523865] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.07805001735687256] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.1312658190727234] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.357768714427948] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [1.1378581523895264] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 32, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.30277565121650696] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.24627992510795593] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.17550179362297058] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [2.2298812866210938] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.0042307376861572] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.22785237431526184] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [1.5332618951797485] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [0.8113269209861755] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.3390934467315674] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.7337213754653931] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [0.9236950278282166] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 64, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.11891532689332962] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.835361123085022] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [1.0467934608459473] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [1.323952078819275] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.4020747244358063] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [0.72756028175354] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.22766859829425812] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.37929773330688477] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 128, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.11076430976390839] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.4672936499118805] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [0.8513698577880859] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.3745659589767456] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.38396552205085754] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.44413506984710693] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.4097929000854492] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [0.5847126245498657] \n",
            "\n",
            "Training with learning rate: 0.002, h_size: 256, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.6056861877441406] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.24250911176204681] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [0.9223860502243042] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.2736260890960693] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.13698618113994598] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.3038511574268341] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.5309299230575562] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.19435852766036987] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.38016727566719055] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [0.6289911270141602] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 32, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.07041206955909729] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.37614554166793823] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [0.6405632495880127] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.677186131477356] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.07992193847894669] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.4401857554912567] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [0.6034087538719177] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.24032315611839294] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.27263858914375305] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [0.3208535313606262] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 64, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [2.0182316303253174] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.15598756074905396] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.47054123878479004] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.3454194664955139] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.26428261399269104] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.5926182270050049] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [2.2103111743927] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.5589685440063477] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.5941084027290344] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.5638453960418701] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.1787610799074173] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.4416470527648926] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [1.54030442237854] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 128, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.7919105291366577] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.08272353559732437] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.5257183313369751] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [1.4550564289093018] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.30602627992630005] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.1654912531375885] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.29069605469703674] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [1.3384363651275635] \n",
            "\n",
            "Training with learning rate: 0.005, h_size: 256, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.22149208188056946] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.19686056673526764] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [0.8379508852958679] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [2.005474805831909] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.07215817272663116] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.5157337784767151] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.0566657781600952] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.20569147169589996] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.24256598949432373] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [1.4008821249008179] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [0.7842707633972168] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.2629070281982422] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.1780424416065216] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.8114761114120483] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [0.4112965166568756] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 32, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.0476951599121094] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [1.2298383712768555] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.4343150854110718] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.2643914818763733] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.48601096868515015] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [0.9462849497795105] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.0912243127822876] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [0.812583863735199] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [0.5750370621681213] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.4836401045322418] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [1.0610677003860474] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 64, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.21357020735740662] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.5417309403419495] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.1396719217300415] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.696755051612854] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [1.3431622982025146] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.0143787860870361] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.30739396810531616] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [1.4614256620407104] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [0.6570371985435486] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 128, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.1263325214385986] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 1, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.20107609033584595] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 1, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 1, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.36499080061912537] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 1, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [0.5707852244377136] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 1, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.6729305982589722] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 2, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.03056405484676361] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 2, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 2, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [0.9006016850471497] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 2, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 2, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [1.8988430500030518] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 3, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 3, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [0.2900259494781494] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 3, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 3, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [0.4965161085128784] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 3, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 4, lambdas: 0.01 \n",
            "\n",
            "The loss value is :  [0.40490078926086426] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 4, lambdas: 0.05 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 4, lambdas: 0.1 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 4, lambdas: 0.2 \n",
            "\n",
            "The loss value is :  [1.4463889598846436] \n",
            "\n",
            "Training with learning rate: 0.01, h_size: 256, h_layers: 4, lambdas: 0.3 \n",
            "\n",
            "The loss value is :  [nan] \n",
            "\n",
            "[[-0.00021845847368240356], [0.4095853567123413], [0.5391976237297058], [0.4531581997871399], [0.15607568621635437], [1.348425030708313], [1.2273609638214111], [0.1781262755393982], [0.13256236910820007], [0.576390266418457], [0.21859599649906158], [0.6816893815994263], [0.4898217022418976], [1.517722249031067], [0.5416715741157532], [0.294535756111145], [0.8453755974769592], [1.9583600759506226], [0.6598857641220093], [0.33957141637802124], [1.118286371231079], [1.51220703125], [0.44898390769958496], [0.5057395696640015], [0.6699783205986023], [0.5415997505187988], [0.28818511962890625], [0.27082473039627075], [1.402402400970459], [0.3234034478664398], [0.9548887014389038], [1.2473804950714111], [0.5817443132400513], [1.3548800945281982], [0.14328207075595856], [0.5545912384986877], [0.9295819401741028], [0.20530807971954346], [0.6694188117980957], [0.16509076952934265], [1.1984786987304688], [1.0280414819717407], [0.09333564341068268], [0.12715815007686615], [0.5009667873382568], [0.2897951900959015], [1.0808466672897339], [0.40442678332328796], [0.8717116713523865], [0.07805001735687256], [0.1312658190727234], [0.357768714427948], [1.1378581523895264], [0.30277565121650696], [0.24627992510795593], [0.17550179362297058], [2.2298812866210938], [1.0042307376861572], [0.22785237431526184], [1.5332618951797485], [0.8113269209861755], [0.3390934467315674], [0.7337213754653931], [0.9236950278282166], [0.11891532689332962], [0.835361123085022], [1.0467934608459473], [1.323952078819275], [0.4020747244358063], [0.72756028175354], [0.22766859829425812], [0.37929773330688477], [0.11076430976390839], [0.4672936499118805], [0.8513698577880859], [0.3745659589767456], [0.38396552205085754], [0.44413506984710693], [0.4097929000854492], [0.5847126245498657], [1.6056861877441406], [0.24250911176204681], [0.9223860502243042], [1.2736260890960693], [0.13698618113994598], [0.3038511574268341], [0.5309299230575562], [0.19435852766036987], [0.38016727566719055], [0.6289911270141602], [0.07041206955909729], [0.37614554166793823], [0.6405632495880127], [1.677186131477356], [0.07992193847894669], [0.4401857554912567], [0.6034087538719177], [0.24032315611839294], [0.27263858914375305], [0.3208535313606262], [2.0182316303253174], [0.15598756074905396], [0.47054123878479004], [0.3454194664955139], [0.26428261399269104], [0.5926182270050049], [2.2103111743927], [0.5589685440063477], [0.5941084027290344], [1.5638453960418701], [0.1787610799074173], [0.4416470527648926], [1.54030442237854], [1.7919105291366577], [0.08272353559732437], [0.5257183313369751], [1.4550564289093018], [0.30602627992630005], [0.1654912531375885], [0.29069605469703674], [1.3384363651275635], [0.22149208188056946], [0.19686056673526764], [0.8379508852958679], [2.005474805831909], [0.07215817272663116], [0.5157337784767151], [1.0566657781600952], [0.20569147169589996], [0.24256598949432373], [1.4008821249008179], [0.7842707633972168], [1.2629070281982422], [0.1780424416065216], [0.8114761114120483], [0.4112965166568756], [1.0476951599121094], [1.2298383712768555], [1.4343150854110718], [0.2643914818763733], [0.48601096868515015], [0.9462849497795105], [1.0912243127822876], [0.812583863735199], [0.5750370621681213], [0.4836401045322418], [1.0610677003860474], [0.21357020735740662], [0.5417309403419495], [0.1396719217300415], [0.696755051612854], [1.3431622982025146], [1.0143787860870361], [0.30739396810531616], [1.4614256620407104], [0.6570371985435486], [1.1263325214385986], [0.20107609033584595], [0.36499080061912537], [0.5707852244377136], [1.6729305982589722], [0.03056405484676361], [0.9006016850471497], [1.8988430500030518], [0.2900259494781494], [0.4965161085128784], [0.40490078926086426], [1.4463889598846436]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}